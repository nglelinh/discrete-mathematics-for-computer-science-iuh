[
  {
    "id": "/contents/vi/chapter16/16_duality_revisited",
    "title": "16_duality_revisited",
    "chapter": "",
    "order": 0,
    "owner": "",
    "lesson_type": "",
    "content": "",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter16/16_duality_revisited/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_Introduction",
    "title": "00 Introduction",
    "chapter": "00",
    "order": 1,
    "owner": "GitHub Copilot",
    "lesson_type": "",
    "content": "Optimization is at the heart of data science. Whether you're training a neural network, minimizing errors in regression models, or efficiently allocating resources in recommendation systems, you're essentially solving problems that involve finding the \"best\" solution from a vast set of possibilities. But to do this effectively, you need to speak the language of mathematics. We'll revisit key ideas from linear algebra, set theory, and calculus, ensuring you're equipped to handle gradients, matrices, constraints, and uncertainties that arise in optimization tasks.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_Introduction/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_01_Calculus",
    "title": "00-01 Calculus",
    "chapter": "00",
    "order": 2,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers essential calculus concepts needed for optimization, organized into four main sections for better understanding.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_01_Calculus/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_01_01_Continuity_and_Uniform_Continuity",
    "title": "00-01-01 Continuity and Uniform Continuity",
    "chapter": "00",
    "order": 3,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson introduces the fundamental concepts of continuity and uniform continuity, which are essential for understanding the behavior of functions in optimization. --- Continuity and Uniform Continuity Continuity and Uniform Continuity are fundamental concepts that describe the behavior of functions, particularly concerning their \"smoothness\" or \"predictability.\" While closely related, they capture distinct properties, with uniform continuity being a stronger condition than mere continuity. Definition of Continuity A function MATH is said to be continuous at a point MATH if, for every positive real number MATH , there exists a positive real number MATH such that for all MATH , if MATH , there exists a positive real number MATH such that for all MATH , if MATH . 3. Almost Everywhere Differentiability : Lipschitz continuous functions are differentiable almost everywhere, and where the derivative exists, MATH . Examples: - MATH is 1-Lipschitz on MATH - MATH is 1-Lipschitz on MATH since MATH - MATH is not Lipschitz on MATH but is Lipschitz on any bounded interval Key Differences and Hierarchy The three types of continuity form a hierarchy of increasingly strong conditions: Continuity ⊆ Uniform Continuity ⊆ Lipschitz Continuity 1. Point-wise vs Global : - Continuity : Local property checked at each point - Uniform Continuity : Global property of the entire function - Lipschitz Continuity : Global property with quantitative bounds 2. Choice of MATH : - Continuity : MATH can depend on both MATH and the specific point MATH - Uniform Continuity : MATH depends only on MATH , working for all points simultaneously - Lipschitz Continuity : MATH provides explicit relationship 3. Rate of Change Control : - Continuity : No control over rate of change - Uniform Continuity : Ensures bounded variation over small intervals - Lipschitz Continuity : Provides explicit linear bound on rate of change 4. Strength Relationships : - Every Lipschitz continuous function is uniformly continuous - Every uniformly continuous function is continuous - The converses are not generally true Detailed Examples and Comparisons Example 1: MATH - On MATH : Continuous but not uniformly continuous rate of change MATH is unbounded - On MATH : Continuous, uniformly continuous, and Lipschitz with MATH Example 2: MATH - On MATH : Continuous, uniformly continuous, and 1-Lipschitz since MATH Example 3: MATH - On MATH : Continuous, uniformly continuous, and 1-Lipschitz - Note : Not differentiable at MATH , but still Lipschitz Example 4: MATH - On MATH : Continuous and uniformly continuous, but not Lipschitz derivative unbounded near MATH - On MATH for MATH : Lipschitz with MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_01_01_Continuity_and_Uniform_Continuity/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_01_01_newton_method_interpretation",
    "title": "14-01-01 Newton's method interpretation",
    "chapter": "",
    "order": 3,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config ; This page examines how the update step discussed earlier is derived from the quadratic approximation of the original function MATH . We also compare it with the gradient descent update step covered in Chapter 6 % multilang post url contents/chapter06/21-03-20-06 00 gradient descent % . Newton's method update step The second-order Taylor approximation quadratic approximation of function MATH is as follows: > MATH >\\begin align >f y \\approx f x + \\nabla f x ^ T y-x +\\frac 1 2 y-x ^ T \\nabla^ 2 f x y-x ,\\\\\\\\ >f approx y = f x + \\nabla f x ^ T y-x +\\frac 1 2 y-x ^ T \\nabla^ 2 f x y-x . >\\end align > MATH Here, MATH is the next step's MATH value, which is MATH . We also define the quadratic approximation as MATH . We want to find the input MATH that minimizes this MATH , i.e., the quadratic approximation. Since MATH is convex, the input MATH that makes the gradient of the above equation equal to zero will minimize MATH . This result becomes the step update formula in Newton's method. Remember that the differentiation in the equation below is with respect to y. > MATH >\\begin align >\\nabla f approx y &= \\nabla f x +\\frac 1 2 \\Big \\nabla^ 2 f x ^ T y-x + y-x ^ T \\nabla^ 2 f x \\Big \\\\\\\\ >&=\\nabla f x +\\nabla^ 2 f x y-x \\\\\\\\ >& = 0,\\\\\\\\ >\\Leftrightarrow y &= x- \\nabla^ 2 f x ^ -1 \\nabla f x . >\\end align > MATH Gradient descent update step In gradient descent, we use the second-order Taylor approximation terms of function MATH , but for the second-order term, we assume it as the identity matrix divided by MATH , rather than the actual second derivative result. > MATH >\\begin align >f y \\approx f x + \\nabla f x ^ T y-x +\\frac 1 2t \\| y-x \\| 2 ^ 2 ,\\\\\\\\ >f approx y = f x + \\nabla f x ^ T y-x +\\frac 1 2t \\| y-x \\| 2 ^ 2 .\\\\\\\\ >\\end align > MATH Similar to Newton's method, we can determine the MATH value where the gradient of the above approximation is zero, i.e., MATH . > MATH >\\begin align >\\nabla f y &= \\nabla f x + \\frac 1 t y-x , \\\\\\\\ > &= 0,\\\\\\\\ >y &= x-t\\nabla f x . >\\end align > MATH This result is identical to the step update of gradient descent. For detailed information about gradient descent, refer to the gradient descent chapter % multilang post url contents/chapter06/21-03-20-06 00 gradient descent % . Example As an example, for the function MATH , we assume taking steps of approximately equal length. That is, we set the step size in gradient descent to match the update magnitude of Newton's method at each iteration, and compare the convergence directions of gradient descent black and Newton's method blue according to their steps. Fig 1 Comparison between gradient descent black and Newton's method blue 3 As can be seen in Fig 1, gradient descent assumes the second derivative term as a constant multiplied by the identity matrix when calculating the gradient, so it converges perpendicularly to the tangent direction of the contour lines, and shows slower convergence speed compared to Newton's method. The remaining chapters will cover the properties, characteristics, convergence, examples, etc. of Newton's method.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_01_01_newton_method_interpretation/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_01_02_Derivatives_and_Multivariable_Calculus",
    "title": "00-01-02 Derivatives and Multivariable Calculus",
    "chapter": "00",
    "order": 4,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers derivatives and essential multivariable calculus concepts that form the foundation for optimization theory and algorithms. --- Derivatives and Rate of Change The derivative of a single variable function represents its instantaneous rate of change, which is fundamental to understanding how functions behave locally. Basic Derivative Concepts Slope between two points: MATH Derivative instantaneous rate of change : MATH The derivative tells us how quickly the function is changing at any given point, which is essential for finding optimal points where the rate of change is zero. Level Curves of Functions Level curves are a fundamental concept in multivariable calculus used to visualize functions of two variables, typically denoted as MATH . They provide a way to represent a 3D surface in a 2D plane. A level curve of a function MATH is the set of all points MATH in the domain of MATH where the function takes a constant value: MATH Examples: - For MATH , the level curves are circles: MATH - For MATH , the level curves are parallel lines: MATH Level curves help us understand: 1. The topography of the function 2. Directions of steepest ascent and descent 3. Locations of potential optima --- Multivariable Calculus Key Concepts Partial Derivatives For a function MATH , the partial derivative with respect to MATH is: MATH This measures how MATH changes when only MATH varies while all other variables remain fixed. Gradient Vector The gradient is a vector composed of all partial derivatives: MATH The gradient points in the direction of steepest increase of the function and is perpendicular to level curves. Hessian Matrix The Hessian matrix contains all second-order partial derivatives: MATH \\nabla^2 f \\mathbf x = \\mathbf H = \\begin pmatrix \\frac \\partial^2 f \\partial x 1^2 & \\frac \\partial^2 f \\partial x 1 \\partial x 2 & \\cdots & \\frac \\partial^2 f \\partial x 1 \\partial x n \\\\ \\frac \\partial^2 f \\partial x 2 \\partial x 1 & \\frac \\partial^2 f \\partial x 2^2 & \\cdots & \\frac \\partial^2 f \\partial x 2 \\partial x n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac \\partial^2 f \\partial x n \\partial x 1 & \\frac \\partial^2 f \\partial x n \\partial x 2 & \\cdots & \\frac \\partial^2 f \\partial x n^2 \\end pmatrix MATH The Hessian provides information about the curvature of the function and is crucial for: - Determining the nature of critical points minimum, maximum, or saddle point - Second-order optimization methods like Newton's method --- Chain Rule for Multivariable Functions The chain rule is fundamental for computing derivatives of composite functions, which frequently appear in optimization problems. Basic Chain Rule For a function MATH where MATH and MATH : MATH General Chain Rule For MATH where each MATH : MATH Applications in Optimization The chain rule is essential for: 1. Gradient Computation : Computing gradients of composite objective functions 2. Constraint Handling : Dealing with constraints that are functions of other variables 3. Algorithm Implementation : Backpropagation in neural networks and automatic differentiation 4. Sensitivity Analysis : Understanding how changes in parameters affect optimal solutions Example: Optimization with Constraints Consider minimizing MATH subject to MATH . Using the constraint to eliminate one variable: MATH , so we minimize: MATH Using the chain rule: MATH Setting MATH gives MATH , so the optimal point is MATH . This demonstrates how multivariable calculus concepts work together to solve optimization problems systematically.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_01_02_Derivatives_and_Multivariable_Calculus/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_01_03_Gradient_and_Directional_Derivatives",
    "title": "00-01-03 Gradient and Directional Derivatives",
    "chapter": "00",
    "order": 5,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson explores the gradient vector and directional derivatives, which are central concepts in optimization for understanding how functions change in different directions. --- Gradient Vector The gradient MATH is a vector composed of the partial derivatives of the function MATH with respect to each of its variables. It indicates the direction of the steepest ascent of the function at a given point. Definition and Computation For a function of two variables, MATH , its gradient is: MATH For a function of MATH variables, MATH : MATH Example: Computing a Gradient For MATH : MATH MATH Therefore: MATH At the point MATH : MATH --- Directional Derivatives The directional derivative measures the rate of change of MATH when we move in any chosen direction MATH . Here MATH must be a unit vector length 1 . Definition For a function MATH and unit vector MATH : MATH Geometric Interpretation The directional derivative can be written as: MATH where MATH is the angle between MATH and MATH , and MATH is the magnitude of the gradient. Example: Computing Directional Derivatives Using our previous example MATH at point MATH where MATH : Direction 1: MATH positive x-direction MATH Direction 2: MATH positive y-direction MATH Direction 3: MATH 45° diagonal MATH --- Maximum and Minimum Rates of Change Key Properties From the formula MATH , we can determine: 1. Maximum Rate of Change : Occurs when MATH i.e., MATH - Direction: MATH same direction as gradient - Maximum rate: MATH 2. Minimum Rate of Change : Occurs when MATH i.e., MATH - Direction: MATH opposite to gradient - Minimum rate: MATH 3. Zero Rate of Change : Occurs when MATH i.e., MATH - Direction: Any vector perpendicular to MATH Summary of Gradient Properties - The gradient MATH points in the direction of steepest increase - The direction MATH points in the direction of steepest decrease - The magnitude MATH gives the maximum rate of change - When MATH , the point is a critical point potential optimum --- Relation to Level Curves At any point on a level curve MATH , the gradient vector MATH is orthogonal perpendicular to the tangent line of the level curve at that point. Why This Matters This orthogonality property is fundamental because: 1. Level curves represent constant function values : Moving along a level curve doesn't change the function value, so the directional derivative is zero. 2. Gradient points to steepest increase : The direction that increases the function value most rapidly must be perpendicular to the direction that doesn't change it at all. 3. Optimization insight : To find extrema, we look for points where the gradient is zero critical points or where the gradient is perpendicular to the constraint boundary. Applications in Optimization Understanding gradients and directional derivatives is crucial for: 1. Gradient Descent : Moving in the direction MATH to minimize MATH 2. Gradient Ascent : Moving in the direction MATH to maximize MATH 3. Constrained Optimization : Using the relationship between gradients and level curves 4. Convergence Analysis : Understanding when algorithms will converge to optimal solutions 5. Step Size Selection : Determining how far to move in the gradient direction The gradient provides both the direction to move and information about how quickly the function is changing, making it the foundation for most optimization algorithms.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_01_03_Gradient_and_Directional_Derivatives/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_01_04_Taylor_Series",
    "title": "00-01-04 Taylor Series",
    "chapter": "00",
    "order": 6,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers Taylor series expansions, which are fundamental for approximating functions and understanding the local behavior of functions in optimization algorithms. --- Taylor Series Definition The Taylor series is a representation of a function as an infinite sum of terms calculated from the values of the function's derivatives at a single point. It provides a way to approximate complex functions using polynomials. Single Variable Taylor Series A Taylor series is a series expansion of a function MATH about a point MATH : MATH In expanded form: MATH Maclaurin Series When the expansion is around MATH , the Taylor series is called a Maclaurin series : MATH Common Maclaurin Series Exponential Function: MATH Sine Function: MATH Cosine Function: MATH Natural Logarithm for MATH and MATH , the point MATH is a saddle point, not a minimum. --- Practical Considerations Convergence and Accuracy 1. Radius of Convergence : Taylor series only converge within a certain radius from the expansion point 2. Truncation Error : Using finite terms introduces approximation errors 3. Computational Cost : Higher-order terms require more derivative computations Optimization Algorithm Choice - First-order methods gradient descent : Use only gradient information, slower but cheaper per iteration - Second-order methods Newton : Use Hessian information, faster convergence but expensive per iteration - Quasi-Newton methods : Approximate the Hessian, balancing speed and computational cost The Taylor series expansion helps us approximate complex functions with simpler polynomial functions around a specific point, which is vital for optimization algorithms and understanding local behavior of functions.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_01_04_Taylor_Series/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_02_03_local_convergence_analysis",
    "title": "14-02-03 Local convergence analyisis",
    "chapter": "",
    "order": 7,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config ; The second important property of Newton's method is that convergence is guaranteed near the solution when certain conditions are satisfied. This is called local convergence. Since the pure Newton's method we have discussed from 14-01 % multilang post url contents/chapter14/2021-03-26-14 01 newton method % does not guarantee convergence, we later devise the damped Newton's method that ensures convergence by adjusting the step size using the same backtracking line search covered in Chapter 6 % multilang post url contents/chapter06/21-03-20-06 00 gradient descent % , and analyze its convergence. >Theorem: Let MATH be continuously differentiable, and let MATH be a root of function MATH , i.e., MATH . >If MATH is non-singular, then the following a and b are satisfied: \\\\ > a If there exists a positive MATH >0 such that MATH \\| x^ 0 -x^ \\star \\| \\begin align >\\lim k \\rightarrow \\infty \\frac \\|\\| x^ k+1 -x^ \\star \\|\\| \\|\\| x^ k -x^ \\star \\|\\| =0. >\\end align \\\\ > b If MATH is Lipschitz continuous near MATH , then there exists a positive K >0 such that the following equation quadratic convergence is satisfied: >\\begin align >\\|\\|x^ k+1 - x^ \\star \\|\\| \\leq K \\|\\| x^ k -x^ \\star \\|\\|^ 2 . >\\end align Proof of a >We organize MATH up to 1st order using Taylor expansion. Since 2nd order and higher terms are bounded by a constant multiple of the norm of the 1st order term, we can represent it using little-o notation as follows: >\\begin align >0=F x^ \\star = F x^ k +\\nabla F x^ k x^ \\star -x^ k +o \\|\\|x^ k -x^ \\star \\|\\| .\\\\ >\\end align >Multiply both sides by MATH and organize. Since little-o is treated as a constant term, it can be ignored. >\\begin align >x^ k -x^ \\star -\\nabla F x^ k ^ -1 F x^ k = o \\|\\|x^ k -x^ \\star \\|\\| . >\\end align >Using Newton's method MATH , we can obtain the following result: >\\begin align >x^ k+1 -x^ \\star =o \\|\\|x^ k -x^ \\star \\|\\| , >\\end align >Therefore, when MATH , we can prove a using the limit-definition of little-o wikipedia https://en.wikipedia.org/wiki/Big O notation . >\\begin align >\\lim k\\rightarrow \\infty \\frac \\|\\|x^ k+1 -x^ \\star \\|\\| \\|\\|x^ k -x^ \\star \\|\\| = \\lim k\\rightarrow \\infty \\frac o \\|\\|x^ k -x^ \\star \\|\\| \\|\\|x^ k -x^ \\star \\|\\| . >\\end align Proof of b The process is identical to proving that the convergence rate in the Damped phase of 14-05 % multilang post url contents/chapter14/2021-03-26-14 05 convergence analysis % is quadratic. Therefore, it is omitted. Example : divergence case We briefly examine an example where convergence is not guaranteed with pure Newton's method. Fig 1 pure Newton's method applied on root finding : divergence case image-link https://slideplayer.com/slide/4998677/ As shown in the figure, depending on the initial point MATH , the solution can diverge.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_02_03_local_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_02_Basic_Linear_Algebra",
    "title": "00-02 Basic Linear Algebra",
    "chapter": "00",
    "order": 7,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers essential linear algebra concepts needed for optimization, organized into three main sections for systematic learning.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_02_Basic_Linear_Algebra/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_02_01_Vectors_and_Vector_Spaces",
    "title": "00-02-01 Vectors and Vector Spaces",
    "chapter": "00",
    "order": 8,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson introduces vectors, vector spaces, and fundamental concepts that form the foundation for understanding linear algebra in optimization contexts. --- Vectors and Vector Spaces MATH What is a Vector? - Vectors: Think of a vector as an arrow in space, representing both a direction and a magnitude length . Mathematically, it's an ordered list of numbers, like coordinates. For example, a vector in 2D space could be MATH , meaning 3 units along the x-axis and 4 units along the y-axis. - Geometric vs Algebraic View: - Geometric: Vectors are arrows with direction and magnitude - Algebraic: Vectors are ordered lists of real numbers Vector Spaces - Vector Space MATH : This is the collection of all possible vectors that have MATH components numbers . For instance, MATH includes all 2-component vectors, representing all points or arrows in a 2D plane. - Examples: - MATH the plane - MATH 3D space Vector Operations Vector Addition: MATH Scalar Multiplication: MATH --- Linear Independence, Basis, and Dimension Linear Independence A set of vectors MATH is linearly independent if the only solution to: MATH is MATH . Intuitive Understanding: A set of vectors is \"linearly independent\" if no vector in the set can be created by scaling and adding the other vectors in the set. They all point in \"different enough\" directions. Example in MATH : - MATH and MATH are linearly independent - MATH and MATH are linearly dependent since MATH Basis A basis for a vector space is a minimal set of linearly independent vectors that can be combined scaled and added to create any other vector in that space. It's like a fundamental set of building blocks. Properties of a Basis: 1. The vectors are linearly independent 2. They span the entire vector space 3. Every vector in the space can be written uniquely as a linear combination of basis vectors Standard Basis for MATH : MATH Dimension The dimension of a vector space is simply the number of vectors in any of its bases. It tells you how many independent directions are needed to describe the space. - MATH - MATH - MATH --- Norms of Vectors A norm is a function that assigns a \"length\" or \"size\" to a vector. It generalizes the concept of distance from the origin. Properties of Norms Any norm MATH must satisfy three properties: 1. Non-negativity: MATH , and MATH if and only if MATH 2. Homogeneity: MATH for any scalar MATH 3. Triangle Inequality: MATH Common Norms Euclidean Norm L2 Norm : MATH This is the \"ordinary\" distance we're familiar with. Manhattan Norm L1 Norm : MATH Also called \"taxicab norm\" - the distance a taxi would travel in a city with a grid layout. Maximum Norm L∞ Norm : MATH The largest component in absolute value. Example: For MATH : - MATH - MATH - MATH --- Inner Products Dot Product The dot product or inner product is the most common way to multiply two vectors, producing a scalar result. Definition For two vectors MATH and MATH in MATH : MATH Geometric Interpretation MATH where MATH is the angle between the vectors. Properties 1. Commutative: MATH 2. Distributive: MATH 3. Homogeneous: MATH Special Cases - Orthogonal vectors: MATH perpendicular - Parallel vectors: MATH - Self dot product: MATH Example For MATH and MATH : MATH --- Applications in Optimization Understanding vectors and vector spaces is crucial for optimization because: 1. Decision Variables: Optimization problems often involve finding the best values for multiple variables, naturally represented as vectors. 2. Gradients: The gradient of a function is a vector pointing in the direction of steepest increase. 3. Constraints: Linear constraints in optimization can be expressed using dot products: MATH . 4. Distance and Similarity: Different norms provide different ways to measure distances between solutions or the size of changes. 5. Orthogonality: Many optimization concepts rely on perpendicularity, such as the relationship between gradients and level curves. 6. Linear Combinations: Feasible regions are often defined as linear combinations of vectors convex hulls, cones, etc. . The vector space framework provides the mathematical foundation for formulating and solving optimization problems systematically.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_02_01_Vectors_and_Vector_Spaces/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_02_02_Matrices_and_Linear_Transformations",
    "title": "00-02-02 Matrices and Linear Transformations",
    "chapter": "00",
    "order": 9,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers matrices, matrix operations, and linear transformations, which are fundamental tools for representing and solving optimization problems. --- Matrices and Matrix Operations What is a Matrix? A matrix is a rectangular grid of numbers arranged in rows and columns. Matrices represent data, transformations, systems of equations, and relationships between variables. General Form: MATH \\mathbf A = \\begin pmatrix a 11 & a 12 & \\cdots & a 1n \\\\ a 21 & a 22 & \\cdots & a 2n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a m1 & a m2 & \\cdots & a mn \\end pmatrix MATH This is an MATH matrix MATH rows, MATH columns . Example: MATH is a MATH matrix. Matrix Addition Matrices are added by summing corresponding elements. Both matrices must have the same dimensions. MATH Example: MATH Scalar Multiplication Multiply every element of the matrix by the scalar: MATH Matrix Multiplication For matrices MATH and MATH , the product MATH is formed by taking the dot product of rows from MATH and columns from MATH : MATH Example: MATH Important: Matrix multiplication is not commutative : MATH in general. --- Linear Transformations A linear transformation is a function MATH that preserves vector addition and scalar multiplication. Every linear transformation can be represented by a matrix. Definition A transformation MATH is linear if and only if: 1. Additivity: MATH 2. Homogeneity: MATH These can be combined into: MATH Matrix-Vector Multiplication If MATH is an MATH matrix and MATH is an MATH column vector, their product MATH is an MATH column vector: MATH \\mathbf w = \\mathbf Av = \\begin pmatrix a 11 v 1 + a 12 v 2 + \\cdots + a 1n v n \\\\ a 21 v 1 + a 22 v 2 + \\cdots + a 2n v n \\\\ \\vdots \\\\ a m1 v 1 + a m2 v 2 + \\cdots + a mn v n \\end pmatrix MATH Example: MATH --- Common 2D Transformations Understanding geometric transformations helps visualize how matrices affect vectors. Scaling Scaling Matrix: MATH - Scales x-coordinates by MATH and y-coordinates by MATH - Example: MATH doubles x-values and triples y-values Rotation Rotation Matrix counter-clockwise by angle MATH : MATH - Example: 90° rotation: MATH - Transforms MATH Reflection Reflection across x-axis: MATH Reflection across y-axis: MATH Reflection across line MATH : MATH Shearing Horizontal Shear: MATH Transforms MATH --- Special Types of Matrices Identity Matrix The identity matrix MATH acts like the number 1 for matrix multiplication: MATH \\mathbf I n = \\begin pmatrix 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end pmatrix MATH Property: MATH for any compatible matrix MATH . Transpose The transpose MATH flips a matrix across its main diagonal: MATH Properties: - MATH - MATH - MATH Symmetric Matrices A matrix is symmetric if MATH : MATH Symmetric matrices have special properties important in optimization. Inverse Matrix The inverse MATH of a square matrix MATH satisfies: MATH For 2×2 matrices: MATH where MATH and MATH . Note: Not all matrices have inverses. A matrix is invertible non-singular if and only if its determinant is non-zero. --- Applications in Optimization Matrices and linear transformations are fundamental in optimization for several reasons: 1. System of Linear Equations Many optimization problems involve solving MATH : - Unique solution: MATH when MATH is invertible - Least squares: Minimize MATH when no exact solution exists 2. Quadratic Forms Quadratic functions appear frequently in optimization: MATH The matrix MATH determines the curvature properties of the function. 3. Linear Programming Standard form: Minimize MATH subject to MATH , MATH 4. Constraint Representation - Equality constraints: MATH - Inequality constraints: MATH 5. Transformations of Variables Change of variables: MATH can simplify optimization problems. Example: Portfolio Optimization In finance, we might minimize portfolio risk: MATH where MATH is the vector of portfolio weights and MATH is the covariance matrix of asset returns. Understanding matrices and linear transformations provides the tools to formulate, analyze, and solve a wide variety of optimization problems efficiently.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_02_02_Matrices_and_Linear_Transformations/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/15_03_02_suboptimality_gap",
    "title": "15-03-02 Suboptimality gap",
    "chapter": "",
    "order": 9,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "앞 절에서 구한 barrier problem과 original problem의 solution인 MATH 와 MATH 의 suboptimality gap은 어떻게 될까? What is the suboptimality gap between MATH , the solution to the barrier problem, and MATH , the solution to the original problem, as derived in the previous section? 따라서, 다음의 식을 구할 수 있다. > MATH \\begin align If convexity is guaranteed, the function is always greater than its tangent, so MATH holds. The tangent is the first-order Taylor approximation Therefore, we can derive the following equation: 비슷하게 MATH 가 성립하므로 다음의 식을 구할 수 있다. Similarly, since MATH holds, we can derive the following equation: h i x^ t - h i x^ \\le \\nabla h i x^ t ^T x^ t - x^ , \\quad i = i, \\cdots , m \\end align MATH Derivation of suboptimality gap 이 두 식에서 suboptimality gap을 유도해 보도록 하겠다. 오른쪽 항은 위의 두 convexity 조건에 의해 도출된다. Let's use these two equations to derive the suboptimality gap. The right-hand side is derived from the two convexity conditions above. f x^ t - f x^ + \\sum i=1 ^ m u i t h i x^ t - h i x^ & \\le \\left\\langle \\nabla f x^ t + \\sum i=1 ^ m u i t \\nabla h i x^ t , \\quad x^ t - x^ \\right\\rangle \\\\\\ & = \\left\\langle -tA^Tv, \\quad x^ t - x^ \\right\\rangle \\\\\\ \\end align MATH 이 식에서 오른쪽 항을 내적해 보면 MATH 이고 MATH 이므로 전체가 0이 된다. 따라서, 첫번째 식의 세번째 항을 오른쪽으로 넘겨서 정리해 보면 다음과 같은 결과를 얻을 수 있다. If we look at the right-hand side of this equation as an inner product, since MATH and MATH , the whole term becomes zero. Therefore, moving the third term of the first equation to the right and simplifying, we get the following result: f x^ t - f x^ & \\le - \\sum i=1 ^ m u i t h i x^ t - h i x^ \\\\\\ & = \\frac m t + \\sum i=1 ^ m u i t h i x^ \\\\\\ & \\le \\frac m t \\end align MATH 두번째 라인의 첫번째 항은 KKT condition에서 MATH 를 만족하므로 MATH 가 된다. 두번째 항도 KKT condition에서 MATH 이므로 제거할 수 있다. 결과적으로 다음과 같은 suboptimality gap을 구할 수 있으며 이는 유용한 stopping criterion이 된다. 참고로, 이 결과는 다음 장에서 duality gap으로도 유도할 수 있다. > MATH \\begin align f x^ t - f x^ \\le \\frac m t \\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/15_03_02_suboptimality_gap/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_02_03_Eigenvalues_and_Eigenvectors",
    "title": "00-02-03 Eigenvalues and Eigenvectors",
    "chapter": "00",
    "order": 10,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers eigenvalues and eigenvectors, which are crucial for understanding the behavior of linear transformations and quadratic functions in optimization. --- Definition and Intuition When a matrix transforms a vector, it usually changes both the vector's direction and its length. However, eigenvectors are special vectors that, when transformed by a given matrix, only get scaled but do not change their direction. Mathematical Definition For a square matrix MATH and a non-zero vector MATH : - MATH is an eigenvector of MATH - MATH is the corresponding eigenvalue if they satisfy the eigenvalue equation : MATH Geometric Interpretation - Eigenvectors: Non-zero vectors that maintain their direction under the transformation MATH - Eigenvalues: The scalar factors by which the eigenvectors are scaled Visual Understanding: - If MATH : The eigenvector is stretched - If MATH for MATH : All eigenvalues of MATH are positive - Positive semidefinite MATH : All eigenvalues are non-negative - Negative definite MATH f \\mathbf x 0 - Local maximum: Hessian is negative definite all eigenvalues 0 MATH \\lambda 2 = 5 - \\sqrt 5 > 0 MATH , the Hessian is positive definite, confirming that the origin is a global minimum. The condition number is MATH , indicating reasonably good conditioning for optimization algorithms. Understanding eigenvalues and eigenvectors provides deep insights into the geometric and analytical properties of optimization problems, enabling better algorithm design and convergence analysis.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_02_03_Eigenvalues_and_Eigenvectors/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_03_Real_Analysis_and_Set_Theory",
    "title": "00-03 Real Analysis And Set Theory",
    "chapter": "00",
    "order": 11,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers essential concepts from real analysis and set theory needed for optimization, organized into two main sections for comprehensive understanding.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_03_Real_Analysis_and_Set_Theory/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_03_01_Set_Theory_Fundamentals",
    "title": "00-03-01 Set Theory Fundamentals",
    "chapter": "00",
    "order": 12,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers fundamental concepts from set theory that provide the mathematical foundation for understanding optimization problems, constraints, and feasible regions. --- Introduction to Set Theory Set theory provides the foundation for modern mathematics and is essential for understanding optimization concepts. A set is simply a collection of distinct objects, called elements or members. Basic Notation - Set notation: MATH - Element membership: MATH x is in A or MATH x is not in A - Empty set: MATH the set with no elements - Set builder notation: MATH the set of all x such that property P x holds Examples - MATH explicit listing - MATH : MATH is strictly greater than MATH - MATH : MATH is greater than or equal to MATH Properties of Inequalities 1. Transitivity: If MATH and MATH , then MATH 2. Addition: If MATH , then MATH for any MATH 3. Multiplication by Positive: If MATH and MATH , then MATH 4. Multiplication by Negative: If MATH and MATH , then MATH inequality flips! Interval Notation - Open interval: MATH - Closed interval: MATH - Half-open intervals: MATH , MATH - Unbounded intervals: MATH , MATH , MATH --- Applications in Optimization Set theory concepts are fundamental to optimization: 1. Feasible Regions The feasible region is the set of all points satisfying the constraints: MATH 2. Level Sets For a function MATH , the level set at level MATH is: MATH 3. Constraint Qualification Understanding when constraint sets have \"nice\" properties like being closed or having non-empty interior affects the existence and characterization of optimal solutions. 4. Convergence Analysis Sequences and limits are essential for analyzing whether optimization algorithms converge to optimal solutions. 5. Set Operations in Algorithms - Intersection: Finding points that satisfy multiple constraints - Union: Combining feasible regions from different scenarios - Complement: Understanding infeasible regions Example: In linear programming, the feasible region is: MATH This is the intersection of half-spaces, demonstrating how set operations naturally arise in optimization problem formulation. Understanding set theory provides the rigorous mathematical foundation needed to formulate optimization problems precisely and analyze their properties systematically.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_03_01_Set_Theory_Fundamentals/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/15_06_barrier_method_v2",
    "title": "15-06 Barrier method v.2",
    "chapter": "",
    "order": 12,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "이전 알고리즘에서는 central path에 있는 solution을 생성했는데, 실제 central path는 optimal로 가는 과정 \"means to an end\" 일 뿐이다. 따라서, 문제를 정확히 풀 필요는 없다. In the previous algorithm, we generated solutions along the central path, but in reality, the central path is just a means to reach the optimal solution. Therefore, it is not necessary to solve the problem exactly. Algorithm For this reason, Barrier method v.2 solves the barrier problem approximately. 단, 단계 2의 MATH 와 단계 3-2의 MATH 부분이 approximation으로 바뀌었다. The steps of the algorithm are the same as those in Barrier method v.1. 1. MATH 이고 MATH 을 선택한다. However, in step 2, MATH and in step 3-2, MATH are now approximations. 3. While MATH 1. Choose MATH and set MATH . 2. At MATH , solve the barrier problem to obtain MATH . 3. While MATH 3-1. Choose MATH . 3-2. Initialize Newton's method with MATH . warm start At MATH , solve the barrier problem to obtain MATH . end while Barrier method v.2에서는 다음 두 가지 사항이 매우 중요하다. 얼마나 근사를 잘 할 수 있는가? How close should each approximation be? In Barrier method v.2, the following two issues are very important: How close should each approximation be? How many Newton steps are needed at each centering step? In the following figure, you can see that when the barrier method is applied to a problem with MATH constraints, linear convergence occurs even as MATH becomes large. That is, it has a log scale with respect to MATH . Fig 1 m에 대해 newton iteration과 suboptimality gap 분석 1 다르게 보면 MATH 인 초기 suboptimal gap duality gap 을 줄이기 위해 필요한 newton step은 MATH 에 대해 천천히 증가한다. 아래 그림을 보면 MATH 이 크게 증하하더라도 각 centering step 별로 20~30 newton step 정도만 필요하다. 단, 한 newton step은 문제의 크기에 따라 크게 달라진다. Fig 2 m의 증가와 newton iteration 수 분석 1",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/15_06_barrier_method_v2/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_03_02_Topology_in_Real_Analysis",
    "title": "00-03-02 Topology in Real Analysis",
    "chapter": "00",
    "order": 13,
    "owner": "GitHub Copilot",
    "lesson_type": "required",
    "content": "This lesson covers essential topological concepts from real analysis that are crucial for understanding the structure of feasible regions, continuity, and the existence of optimal solutions in optimization problems. --- Introduction to Topology Topology studies the properties of space that are preserved under continuous deformations. In optimization, topological concepts help us understand the structure of feasible regions and the behavior of functions, particularly regarding the existence and characterization of optimal solutions. Metric Spaces and Distance Before discussing topology, we need the concept of distance. In MATH , the standard Euclidean distance between points MATH and MATH is: MATH Open Balls and Neighborhoods An open ball centered at MATH with radius MATH is: MATH such that the open ball MATH is entirely contained within MATH : MATH Intuitive Understanding An open set has the property that if you're inside it, you can move a small distance in any direction and still remain inside the set. There's always some \"wiggle room\" around every point. Examples of Open Sets In MATH : - MATH first quadrant, excluding axes - MATH itself In MATH : - Any open ball MATH - MATH itself - MATH empty set - vacuously open Properties of Open Sets 1. The union of any collection of open sets is open 2. The intersection of finitely many open sets is open 3. MATH and MATH are both open --- Closed Sets A closed set is defined as a set that contains all of its boundary points. Equivalently, a set MATH is closed if its complement MATH is an open set . Formal Definition A set MATH is closed if it contains all its limit points. That is, if a sequence of points MATH from MATH converges to a point MATH , then MATH must also be in MATH : MATH Examples of Closed Sets In MATH : - MATH - MATH - MATH single point - MATH integers In MATH : - MATH closed unit disk - MATH first quadrant, including axes - MATH single point In MATH : - Any closed ball MATH - MATH itself - MATH empty set - Any finite set Properties of Closed Sets 1. The intersection of any collection of closed sets is closed 2. The union of finitely many closed sets is closed 3. MATH and MATH are both closed Important Note Sets can be: - Open but not closed: MATH - Closed but not open: MATH - Both open and closed: MATH , MATH - Neither open nor closed: MATH , MATH --- Boundary, Interior, and Closure Boundary The boundary of a set MATH , denoted MATH , consists of points that are \"on the edge\" of the set. A point MATH is a boundary point of MATH if every open ball centered at MATH intersects both MATH and its complement MATH : MATH Interior The interior of a set MATH , denoted MATH or MATH , includes all points strictly \"inside\" the set, excluding the boundary: MATH Closure The closure of a set MATH , denoted MATH or MATH , is the smallest closed set containing MATH : MATH Example Analysis For the interval MATH in MATH : - Interior: MATH - Boundary: MATH - Closure: MATH For the open disk MATH such that MATH - Closed: As defined above Examples of Compact Sets In MATH : - MATH any closed, bounded interval - MATH single point - Any finite set In MATH : - MATH closed unit disk - MATH unit square - Any finite set of points In MATH : - Any closed ball MATH - Any closed, bounded rectangle MATH Non-Compact Sets - MATH bounded but not closed - MATH closed but not bounded - MATH not bounded - MATH bounded but not closed, since 0 is a limit point not in the set --- Continuity of Functions Point-wise Continuity A function MATH is continuous at a point MATH if for every MATH , there exists MATH such that for all MATH : MATH Intuitive meaning: Small changes in input lead to small changes in output. Global Continuity MATH is continuous on MATH if it's continuous at every point in MATH . Sequential Characterization MATH is continuous at MATH if and only if for every sequence MATH in MATH converging to MATH : MATH --- Important Theorems for Optimization Extreme Value Theorem If MATH is continuous on a compact set MATH , then MATH attains its maximum and minimum on MATH . This is fundamental for optimization: it guarantees that continuous objective functions have optimal solutions on compact feasible regions. Proof idea: Compactness ensures that the supremum and infimum of MATH on MATH are actually achieved at points in MATH . Intermediate Value Theorem If MATH is continuous on MATH and MATH is between MATH and MATH , then there exists MATH such that MATH . This helps establish the existence of solutions to equations MATH . Bolzano-Weierstrass Theorem Every bounded sequence in MATH has a convergent subsequence. This is crucial for proving convergence of optimization algorithms. Weierstrass Approximation Theorem Every continuous function on a closed interval can be uniformly approximated by polynomials. This justifies using polynomial approximations in optimization algorithms. --- Applications in Optimization 1. Existence of Solutions Compact feasible sets guarantee optimal solutions exist: - If the feasible region MATH is compact and the objective function MATH is continuous, then the optimization problem MATH has a solution. 2. Constraint Qualification Understanding topological properties of constraint sets: - Regular points: Points where constraint gradients are linearly independent - Interior point methods: Require the feasible region to have non-empty interior 3. Convergence Analysis Analyzing whether optimization algorithms converge: - Closed sets: Ensure limit points of convergent sequences remain feasible - Compactness: Guarantees convergent subsequences exist 4. Local vs Global Optima Using neighborhoods to define optimality: - Local minimum: MATH for all MATH in some neighborhood of MATH - Global minimum: MATH for all MATH in the feasible region 5. Feasible Region Analysis Determining properties of constraint sets: - Linear constraints: Define closed sets half-spaces - Nonlinear constraints: May create sets that are neither open nor closed - Compact feasible regions: Guarantee existence of optimal solutions Example: Portfolio Optimization Consider minimizing portfolio risk subject to constraints: MATH \\begin align \\min \\mathbf w \\quad & \\mathbf w ^T \\mathbf \\Sigma \\mathbf w \\\\ \\text s.t. \\quad & \\mathbf 1 ^T \\mathbf w = 1 \\\\ & \\mathbf w \\geq \\mathbf 0 \\end align MATH The feasible region MATH is: - Closed: It's the intersection of closed sets - Bounded: The constraint MATH with MATH bounds the feasible region - Compact: Being closed and bounded in MATH Since the objective function MATH is continuous and MATH is compact, the Extreme Value Theorem guarantees that an optimal portfolio exists. Understanding topology and real analysis provides the rigorous foundation needed to prove that optimization problems have solutions and that algorithms will find them. These concepts are essential for both theoretical analysis and practical algorithm design.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_03_02_Topology_in_Real_Analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_04_Probability_and_Statistics",
    "title": "00-04 Probability and Statistics",
    "chapter": "00",
    "order": 13,
    "owner": "AI Assistant",
    "lesson_type": "",
    "content": "Probability and Statistics for Convex Optimization Probability and statistics form a crucial foundation for understanding many optimization problems, especially in machine learning and data science. This section introduces the essential probabilistic concepts that frequently appear in convex optimization, from maximum likelihood estimation to Bayesian optimization. Why Probability Matters in Optimization Many optimization problems arise from statistical modeling: - Maximum Likelihood Estimation MLE : Finding parameters that maximize the likelihood of observed data - Bayesian Optimization : Using probabilistic models to guide the search for optimal solutions - Stochastic Optimization : Dealing with uncertainty and randomness in objective functions - Regularization : Adding probabilistic priors to prevent overfitting - Risk Minimization : Optimizing expected loss over probability distributions Key Topics Covered 1. Basic Probability Theory : Sample spaces, events, and probability axioms 2. Common Probability Distributions : Normal, exponential, and other distributions crucial for optimization 3. Expectation and Variance : Computing and optimizing expected values 4. Bayes' Theorem : Foundation for Bayesian optimization and inference 5. Statistical Estimation : Connecting probability theory to optimization problems Connection to Convex Optimization Understanding probability helps you: - Formulate Problems : Convert real-world uncertainty into mathematical optimization problems - Choose Objective Functions : Select appropriate loss functions based on probabilistic assumptions - Interpret Results : Understand confidence intervals and statistical significance of solutions - Handle Noise : Deal with measurement errors and stochastic processes - Design Algorithms : Develop robust optimization methods that work under uncertainty This foundation will be essential as we explore how probabilistic models lead to convex optimization problems in machine learning, statistics, and engineering applications. 💡 Learning Path: Start with basic probability concepts, then explore how they connect to optimization through maximum likelihood estimation and Bayesian methods. Each lesson builds toward understanding how uncertainty and randomness create optimization problems.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_04_Probability_and_Statistics/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_04_01_Basic_Probability_Theory",
    "title": "00-04-01 Basic Probability Theory",
    "chapter": "00",
    "order": 14,
    "owner": "AI Assistant",
    "lesson_type": "",
    "content": "Basic Probability Theory Probability theory provides the mathematical framework for reasoning about uncertainty, which is fundamental to many optimization problems in machine learning and data science. 1. Sample Space and Events Sample Space Ω : The set of all possible outcomes of an experiment. Event A : A subset of the sample space representing a collection of outcomes. Examples: - Coin flip: Ω = H, T - Die roll: Ω = 1, 2, 3, 4, 5, 6 - Continuous: Ω = 0, 1 for uniform random variable Interactive Sample Space Visualization Visualization: Click to generate random samples. Different colors represent different events. Experiment Type Coin Flip Dice Roll Uniform 0,1 Generate Sample Clear Statistics: Total samples: 0 Event A: 0 Event B: 0 P A ≈ 0.000 P B ≈ 0.000 2. Probability Axioms Kolmogorov Axioms For any probability measure P, the following axioms must hold: Axiom 1: Non-negativity MATH Axiom 2: Normalization MATH Axiom 3: Countable Additivity For mutually exclusive events MATH : MATH 3. Basic Properties and Rules Complement Rule MATH Addition Rule For any two events A and B: MATH Multiplication Rule MATH 4. Conditional Probability The probability of event A given that event B has occurred: MATH Interpretation : Conditional probability updates our belief about A when we have information about B. Conditional Probability Visualization Venn Diagram: Blue circle is event A, red circle is event B. Purple intersection shows A ∩ B. Adjust Probabilities P A : 0.4 P B : 0.5 Overlap: 0.2 Probabilities: P A = 0.400 P B = 0.500 P A ∩ B = 0.200 P A ∪ B = 0.700 Conditional: P A|B = 0.400 P B|A = 0.500 5. Independence Two events A and B are independent if: MATH Equivalently: MATH Interpretation : Knowledge about one event doesn't change the probability of the other. 6. Random Variables A random variable X is a function that assigns a real number to each outcome in the sample space: MATH Types of Random Variables: Discrete : Takes countable values e.g., number of heads in coin flips - Probability Mass Function PMF : MATH Continuous : Takes uncountable values e.g., height, weight - Probability Density Function PDF : MATH - MATH 7. Connection to Optimization Probability theory connects to optimization in several ways: Maximum Likelihood Estimation Find parameters θ that maximize the likelihood: MATH Expected Value Optimization Minimize expected loss: MATH Bayesian Optimization Use probability distributions to model uncertainty in objective functions and guide search for optimal solutions. Probability in Optimization Example MLE Example: Finding the parameter μ that maximizes likelihood of observed data from Normal μ, 1 . MLE Demo True μ: 2.0 Sample Size: 20 Generate Data & Find MLE Results: True μ: 2.000 Sample mean: -- MLE estimate: -- Error: -- Key Takeaways 1. Foundation : Probability axioms provide the mathematical foundation for reasoning about uncertainty 2. Conditional Probability : Essential for updating beliefs with new information 3. Independence : Simplifies calculations and modeling assumptions 4. Random Variables : Bridge between abstract probability and concrete applications 5. Optimization Connection : Many optimization problems arise from probabilistic modeling Understanding these basics prepares you for more advanced topics like Bayesian inference, maximum likelihood estimation, and stochastic optimization that are central to modern machine learning and data science. // Sample Space Visualization class SampleSpaceDemo constructor this.canvas = document.getElementById 'sampleSpaceCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; this.samples = ; this.experimentType = 'coin'; this.setupControls ; this.draw ; setupControls const radios = document.querySelectorAll 'input name=\"experiment\" ' ; const generateBtn = document.getElementById 'generate-sample' ; const clearBtn = document.getElementById 'clear-samples' ; radios.forEach radio => radio.addEventListener 'change', e => this.experimentType = e.target.value; this.samples = ; this.updateStats ; this.draw ; ; ; generateBtn.addEventListener 'click', => this.generateSample ; clearBtn.addEventListener 'click', => this.samples = ; this.updateStats ; this.draw ; ; this.canvas.addEventListener 'click', => this.generateSample ; generateSample let sample; switch this.experimentType case 'coin': sample = value: Math.random = 4, // Event A: 4, 5, or 6 eventB: diceValue % 2 === 0 // Event B: Even ; break; case 'uniform': const uniformValue = Math.random ; sample = value: uniformValue.toFixed 3 , x: uniformValue this.width - 40 + 20, y: Math.random this.height - 40 + 20, eventA: uniformValue > 0.5, // Event A: > 0.5 eventB: uniformValue s.eventA .length; const eventBCount = this.samples.filter s => s.eventB .length; document.getElementById 'total-samples' .textContent = total; document.getElementById 'event-a-count' .textContent = eventACount; document.getElementById 'event-b-count' .textContent = eventBCount; document.getElementById 'prob-a' .textContent = total > 0 ? eventACount / total .toFixed 3 : '0.000'; document.getElementById 'prob-b' .textContent = total > 0 ? eventBCount / total .toFixed 3 : '0.000'; draw this.ctx.clearRect 0, 0, this.width, this.height ; // Draw background this.ctx.fillStyle = ' f8f9fa'; this.ctx.fillRect 0, 0, this.width, this.height ; // Draw samples this.samples.forEach sample => // Determine color based on events let color = ' 666'; if sample.eventA && sample.eventB color = ' 9c27b0'; // Both events else if sample.eventA color = ' 2196f3'; // Event A only else if sample.eventB color = ' f44336'; // Event B only this.ctx.fillStyle = color; this.ctx.beginPath ; this.ctx.arc sample.x, sample.y, 5, 0, 2 Math.PI ; this.ctx.fill ; // Draw value this.ctx.fillStyle = ' 000'; this.ctx.font = '10px Arial'; this.ctx.textAlign = 'center'; this.ctx.fillText sample.value, sample.x, sample.y - 8 ; ; // Draw legend this.ctx.fillStyle = ' 000'; this.ctx.font = '12px Arial'; this.ctx.textAlign = 'left'; this.ctx.fillText 'Legend:', 10, 20 ; this.ctx.fillStyle = ' 2196f3'; this.ctx.beginPath ; this.ctx.arc 20, 35, 4, 0, 2 Math.PI ; this.ctx.fill ; this.ctx.fillStyle = ' 000'; this.ctx.fillText 'Event A only', 30, 38 ; this.ctx.fillStyle = ' f44336'; this.ctx.beginPath ; this.ctx.arc 20, 50, 4, 0, 2 Math.PI ; this.ctx.fill ; this.ctx.fillStyle = ' 000'; this.ctx.fillText 'Event B only', 30, 53 ; this.ctx.fillStyle = ' 9c27b0'; this.ctx.beginPath ; this.ctx.arc 20, 65, 4, 0, 2 Math.PI ; this.ctx.fill ; this.ctx.fillStyle = ' 000'; this.ctx.fillText 'Both A and B', 30, 68 ; // Conditional Probability Visualization class ConditionalProbDemo constructor this.canvas = document.getElementById 'conditionalCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; this.probA = 0.4; this.probB = 0.5; this.overlap = 0.2; this.setupControls ; this.draw ; setupControls const probASlider = document.getElementById 'prob-a-slider' ; const probBSlider = document.getElementById 'prob-b-slider' ; const overlapSlider = document.getElementById 'overlap-slider' ; probASlider.addEventListener 'input', e => this.probA = parseFloat e.target.value ; document.getElementById 'prob-a-value' .textContent = this.probA.toFixed 1 ; this.updateCalculations ; this.draw ; ; probBSlider.addEventListener 'input', e => this.probB = parseFloat e.target.value ; document.getElementById 'prob-b-value' .textContent = this.probB.toFixed 1 ; this.updateCalculations ; this.draw ; ; overlapSlider.addEventListener 'input', e => this.overlap = parseFloat e.target.value ; document.getElementById 'overlap-value' .textContent = this.overlap.toFixed 1 ; // Ensure overlap doesn't exceed min probA, probB const maxOverlap = Math.min this.probA, this.probB ; if this.overlap > maxOverlap this.overlap = maxOverlap; overlapSlider.value = this.overlap; document.getElementById 'overlap-value' .textContent = this.overlap.toFixed 1 ; this.updateCalculations ; this.draw ; ; this.updateCalculations ; updateCalculations const probUnion = this.probA + this.probB - this.overlap; const probAGivenB = this.probB > 0 ? this.overlap / this.probB : 0; const probBGivenA = this.probA > 0 ? this.overlap / this.probA : 0; document.getElementById 'display-prob-a' .textContent = this.probA.toFixed 3 ; document.getElementById 'display-prob-b' .textContent = this.probB.toFixed 3 ; document.getElementById 'display-prob-ab' .textContent = this.overlap.toFixed 3 ; document.getElementById 'display-prob-union' .textContent = probUnion.toFixed 3 ; document.getElementById 'display-prob-a-given-b' .textContent = probAGivenB.toFixed 3 ; document.getElementById 'display-prob-b-given-a' .textContent = probBGivenA.toFixed 3 ; draw this.ctx.clearRect 0, 0, this.width, this.height ; // Draw universe rectangle this.ctx.strokeStyle = ' 000'; this.ctx.lineWidth = 2; this.ctx.strokeRect 50, 50, 300, 200 ; this.ctx.fillStyle = ' 000'; this.ctx.font = '14px Arial'; this.ctx.fillText 'Ω Sample Space ', 55, 45 ; // Calculate circle parameters const centerAX = 150; const centerAY = 150; const centerBX = 250; const centerBY = 150; // Calculate radii based on probabilities area proportional to probability const radiusA = Math.sqrt this.probA 10000 / Math.PI ; const radiusB = Math.sqrt this.probB 10000 / Math.PI ; // Draw circle A this.ctx.globalAlpha = 0.3; this.ctx.fillStyle = ' 2196f3'; this.ctx.beginPath ; this.ctx.arc centerAX, centerAY, radiusA, 0, 2 Math.PI ; this.ctx.fill ; // Draw circle B this.ctx.fillStyle = ' f44336'; this.ctx.beginPath ; this.ctx.arc centerBX, centerBY, radiusB, 0, 2 Math.PI ; this.ctx.fill ; // Draw intersection approximate if this.overlap > 0 this.ctx.fillStyle = ' 9c27b0'; const overlapRadius = Math.sqrt this.overlap 5000 / Math.PI ; this.ctx.beginPath ; this.ctx.arc centerAX + centerBX / 2, centerAY + centerBY / 2, overlapRadius, 0, 2 Math.PI ; this.ctx.fill ; this.ctx.globalAlpha = 1.0; // Draw circle outlines this.ctx.strokeStyle = ' 2196f3'; this.ctx.lineWidth = 2; this.ctx.beginPath ; this.ctx.arc centerAX, centerAY, radiusA, 0, 2 Math.PI ; this.ctx.stroke ; this.ctx.strokeStyle = ' f44336'; this.ctx.beginPath ; this.ctx.arc centerBX, centerBY, radiusB, 0, 2 Math.PI ; this.ctx.stroke ; // Labels this.ctx.fillStyle = ' 000'; this.ctx.font = '16px Arial'; this.ctx.fillText 'A', centerAX - 40, centerAY ; this.ctx.fillText 'B', centerBX + 30, centerBY ; if this.overlap > 0 this.ctx.fillText 'A∩B', centerAX + centerBX / 2 - 15, centerAY + centerBY / 2 + 5 ; // MLE Optimization Demo class MLEDemo constructor this.canvas = document.getElementById 'optimizationCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; this.trueMu = 2.0; this.sampleSize = 20; this.data = ; this.setupControls ; this.draw ; setupControls const trueMuSlider = document.getElementById 'true-mu-slider' ; const sampleSizeSlider = document.getElementById 'sample-size-slider' ; const generateBtn = document.getElementById 'generate-mle-data' ; trueMuSlider.addEventListener 'input', e => this.trueMu = parseFloat e.target.value ; document.getElementById 'true-mu-value' .textContent = this.trueMu.toFixed 1 ; document.getElementById 'display-true-mu' .textContent = this.trueMu.toFixed 3 ; ; sampleSizeSlider.addEventListener 'input', e => this.sampleSize = parseInt e.target.value ; document.getElementById 'sample-size-value' .textContent = this.sampleSize; ; generateBtn.addEventListener 'click', => this.generateDataAndFindMLE ; generateDataAndFindMLE // Generate data from Normal trueMu, 1 this.data = ; for let i = 0; i sum + x, 0 / this.data.length; const error = Math.abs sampleMean - this.trueMu ; // Update display document.getElementById 'sample-mean' .textContent = sampleMean.toFixed 3 ; document.getElementById 'mle-estimate' .textContent = sampleMean.toFixed 3 ; document.getElementById 'mle-error' .textContent = error.toFixed 3 ; this.draw ; draw this.ctx.clearRect 0, 0, this.width, this.height ; if this.data.length === 0 this.ctx.fillStyle = ' 666'; this.ctx.font = '16px Arial'; this.ctx.textAlign = 'center'; this.ctx.fillText 'Click \"Generate Data & Find MLE\" to start', this.width / 2, this.height / 2 ; return; // Draw axes this.ctx.strokeStyle = ' ddd'; this.ctx.lineWidth = 1; const marginX = 50; const marginY = 50; const plotWidth = this.width - 2 marginX; const plotHeight = this.height - 2 marginY; // X-axis this.ctx.beginPath ; this.ctx.moveTo marginX, this.height - marginY ; this.ctx.lineTo this.width - marginX, this.height - marginY ; this.ctx.stroke ; // Y-axis this.ctx.beginPath ; this.ctx.moveTo marginX, marginY ; this.ctx.lineTo marginX, this.height - marginY ; this.ctx.stroke ; // Find data range const minX = Math.min ...this.data - 1; const maxX = Math.max ...this.data + 1; // Draw likelihood function this.ctx.strokeStyle = ' 2196f3'; this.ctx.lineWidth = 2; this.ctx.beginPath ; for let i = 0; i sum + x, 0 / this.data.length; const mleX = marginX + sampleMean - minX / maxX - minX plotWidth; this.ctx.strokeStyle = ' f44336'; this.ctx.lineWidth = 2; this.ctx.beginPath ; this.ctx.moveTo mleX, marginY ; this.ctx.lineTo mleX, this.height - marginY ; this.ctx.stroke ; // Mark true value const trueX = marginX + this.trueMu - minX / maxX - minX plotWidth; this.ctx.strokeStyle = ' 4caf50'; this.ctx.lineWidth = 2; this.ctx.setLineDash 5, 5 ; this.ctx.beginPath ; this.ctx.moveTo trueX, marginY ; this.ctx.lineTo trueX, this.height - marginY ; this.ctx.stroke ; this.ctx.setLineDash ; // Draw data points this.ctx.fillStyle = ' 666'; for const x of this.data const pointX = marginX + x - minX / maxX - minX plotWidth; this.ctx.beginPath ; this.ctx.arc pointX, this.height - marginY + 10, 2, 0, 2 Math.PI ; this.ctx.fill ; // Labels this.ctx.fillStyle = ' 000'; this.ctx.font = '12px Arial'; this.ctx.textAlign = 'center'; this.ctx.fillText 'μ', this.width / 2, this.height - 10 ; this.ctx.save ; this.ctx.translate 15, this.height / 2 ; this.ctx.rotate -Math.PI / 2 ; this.ctx.fillText 'Log-Likelihood', 0, 0 ; this.ctx.restore ; // Legend this.ctx.textAlign = 'left'; this.ctx.fillText '— Likelihood', 10, 20 ; this.ctx.fillStyle = ' f44336'; this.ctx.fillText '— MLE', 10, 35 ; this.ctx.fillStyle = ' 4caf50'; this.ctx.fillText '--- True μ', 10, 50 ; // Initialize when DOM is loaded document.addEventListener 'DOMContentLoaded', function new SampleSpaceDemo ; new ConditionalProbDemo ; new MLEDemo ; ; input type=\"range\" -webkit-appearance: none; appearance: none; height: 5px; background: ddd; outline: none; border-radius: 5px; input type=\"range\" ::-webkit-slider-thumb -webkit-appearance: none; appearance: none; width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; input type=\"range\" ::-moz-range-thumb width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; border: none; canvas border-radius: 5px; .demo-container margin: 20px 0;",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_04_01_Basic_Probability_Theory/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter00/00_04_02_Common_Probability_Distributions",
    "title": "00-04-02 Common Probability Distributions",
    "chapter": "00",
    "order": 15,
    "owner": "AI Assistant",
    "lesson_type": "",
    "content": "Common Probability Distributions Understanding key probability distributions is essential for optimization problems in machine learning and statistics. These distributions frequently appear as assumptions in models, priors in Bayesian methods, and error models in regression. 1. Discrete Distributions Bernoulli Distribution Models a single trial with two outcomes success/failure . Parameters : MATH probability of success PMF : MATH for MATH Mean : MATH Variance : MATH Applications : Binary classification, coin flips, A/B testing Binomial Distribution Models the number of successes in MATH independent Bernoulli trials. Parameters : MATH trials , MATH success probability PMF : MATH for MATH Mean : MATH Variance : MATH Poisson Distribution Models the number of events in a fixed interval when events occur independently at a constant rate. Parameters : MATH rate parameter PMF : MATH for MATH Mean : MATH Variance : MATH Applications : Count data, rare events, queueing theory Interactive Discrete Distributions Visualization: Probability mass functions of discrete distributions. Distribution Type Bernoulli Binomial Poisson p: 0.5 n: 10 λ: 3.0 Statistics: Mean: 0.500 Variance: 0.250 Mode: 0 or 1 2. Continuous Distributions Uniform Distribution All values in an interval are equally likely. Parameters : MATH with MATH variance PDF : MATH Mean : MATH Variance : MATH Properties : - Symmetric around MATH - 68-95-99.7 rule - Central Limit Theorem - Maximum entropy for given mean and variance Exponential Distribution Models waiting times between events in a Poisson process. Parameters : MATH rate parameter PDF : MATH for MATH Mean : MATH Variance : MATH Properties : Memoryless property Beta Distribution Flexible distribution on MATH , often used for modeling probabilities. Parameters : MATH shape parameters PDF : MATH for MATH Mean : MATH Variance : MATH Interactive Continuous Distributions Visualization: Probability density functions of continuous distributions. Distribution Type Uniform Normal Exponential Beta a: 0 b: 1 μ: 0 σ: 1.0 λ: 1.0 α: 2 β: 2 Statistics: Mean: 0.500 Variance: 0.083 Support: 0, 1 3. Multivariate Distributions Multivariate Normal Distribution Extension of the normal distribution to multiple dimensions. Parameters : MATH mean vector , MATH covariance matrix, positive definite PDF : MATH Properties : - Marginal distributions are normal - Linear combinations are normal - Conditional distributions are normal Multivariate Normal Distribution 2D Visualization: Contour plot of bivariate normal distribution. Samples shown as dots. Parameters μ₁: 0.0 μ₂: 0.0 σ₁: 1.0 σ₂: 1.0 ρ correlation : 0.0 Generate Samples Covariance Matrix: Σ₁₁: 1.000 Σ₁₂: 0.000 Σ₂₂: 1.000 Det Σ : 1.000 4. Applications in Optimization Maximum Likelihood Estimation Many optimization problems involve finding parameters that maximize the likelihood of observed data under a specific distribution: MATH Bayesian Optimization Prior distributions encode beliefs about parameters before seeing data: MATH Regularization Distributions can be used as priors to regularize optimization problems: - L2 regularization ↔ Gaussian prior - L1 regularization ↔ Laplace prior Stochastic Optimization Distributions model noise and uncertainty in objective functions and constraints. Key Insights for Optimization 1. Model Selection : Choose distributions that match your data's characteristics 2. Parameter Estimation : Use MLE or Bayesian methods to estimate distribution parameters 3. Uncertainty Quantification : Distributions provide natural ways to quantify uncertainty 4. Regularization : Prior distributions can prevent overfitting 5. Computational Efficiency : Some distributions have closed-form solutions for common operations Understanding these distributions and their properties is crucial for formulating and solving optimization problems in machine learning, statistics, and engineering applications. // Discrete Distributions Demo class DiscreteDistributionsDemo constructor this.canvas = document.getElementById 'discreteCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; this.distType = 'bernoulli'; this.params = p: 0.5, n: 10, lambda: 3.0 ; this.setupControls ; this.draw ; setupControls const radios = document.querySelectorAll 'input name=\"discrete-dist\" ' ; const pSlider = document.getElementById 'p-slider' ; const nSlider = document.getElementById 'n-slider' ; const lambdaSlider = document.getElementById 'lambda-slider' ; radios.forEach radio => radio.addEventListener 'change', e => this.distType = e.target.value; this.updateParameterVisibility ; this.updateStats ; this.draw ; ; ; pSlider.addEventListener 'input', e => this.params.p = parseFloat e.target.value ; document.getElementById 'p-value' .textContent = this.params.p.toFixed 1 ; this.updateStats ; this.draw ; ; nSlider.addEventListener 'input', e => this.params.n = parseInt e.target.value ; document.getElementById 'n-value' .textContent = this.params.n; this.updateStats ; this.draw ; ; lambdaSlider.addEventListener 'input', e => this.params.lambda = parseFloat e.target.value ; document.getElementById 'lambda-value' .textContent = this.params.lambda.toFixed 1 ; this.updateStats ; this.draw ; ; this.updateParameterVisibility ; this.updateStats ; updateParameterVisibility document.getElementById 'p-param' .style.display = this.distType === 'bernoulli' || this.distType === 'binomial' ? 'block' : 'none'; document.getElementById 'n-param' .style.display = this.distType === 'binomial' ? 'block' : 'none'; document.getElementById 'lambda-param' .style.display = this.distType === 'poisson' ? 'block' : 'none'; updateStats let mean, variance, mode; switch this.distType case 'bernoulli': mean = this.params.p; variance = this.params.p 1 - this.params.p ; mode = this.params.p > 0.5 ? '1' : this.params.p n return 0; return this.factorial n / this.factorial k this.factorial n - k ; getProbability k switch this.distType case 'bernoulli': return k === 0 ? 1 - this.params.p : k === 1 ? this.params.p : 0 ; case 'binomial': if k this.params.n return 0; return this.binomialCoeff this.params.n, k Math.pow this.params.p, k Math.pow 1 - this.params.p, this.params.n - k ; case 'poisson': if k radio.addEventListener 'change', e => this.distType = e.target.value; this.updateParameterVisibility ; this.updateStats ; this.draw ; ; ; // Setup all sliders const sliders = 'a', 'b', 'mu', 'sigma', 'exp-lambda', 'alpha', 'beta' ; sliders.forEach slider => const element = document.getElementById slider + '-slider' ; if element element.addEventListener 'input', e => const value = parseFloat e.target.value ; const param = slider === 'exp-lambda' ? 'lambda' : slider; this.params param = value; const valueSpan = document.getElementById slider + '-value' ; if valueSpan valueSpan.textContent = value.toFixed 1 ; this.updateStats ; this.draw ; ; ; this.updateParameterVisibility ; this.updateStats ; updateParameterVisibility document.getElementById 'uniform-params' .style.display = this.distType === 'uniform' ? 'block' : 'none'; document.getElementById 'normal-params' .style.display = this.distType === 'normal' ? 'block' : 'none'; document.getElementById 'exponential-params' .style.display = this.distType === 'exponential' ? 'block' : 'none'; document.getElementById 'beta-params' .style.display = this.distType === 'beta' ? 'block' : 'none'; updateStats let mean, variance, support; switch this.distType case 'uniform': mean = this.params.a + this.params.b / 2; variance = Math.pow this.params.b - this.params.a, 2 / 12; support = MATH this.params.b ; break; case 'normal': mean = this.params.mu; variance = this.params.sigma this.params.sigma; support = ' -∞, ∞ '; break; case 'exponential': mean = 1 / this.params.lambda; variance = 1 / this.params.lambda this.params.lambda ; support = ' 0, ∞ '; break; case 'beta': mean = this.params.alpha / this.params.alpha + this.params.beta ; variance = this.params.alpha this.params.beta / Math.pow this.params.alpha + this.params.beta, 2 this.params.alpha + this.params.beta + 1 ; support = ' 0, 1 '; break; document.getElementById 'continuous-mean' .textContent = mean.toFixed 3 ; document.getElementById 'continuous-variance' .textContent = variance.toFixed 3 ; document.getElementById 'continuous-support' .textContent = support; gamma z // Stirling's approximation for gamma function if z = this.params.a && x = 0 ? this.params.lambda Math.exp -this.params.lambda x : 0; case 'beta': if x 1 return 0; const B = this.gamma this.params.alpha this.gamma this.params.beta / this.gamma this.params.alpha + this.params.beta ; return Math.pow x, this.params.alpha - 1 Math.pow 1 - x, this.params.beta - 1 / B; getRange switch this.distType case 'uniform': return this.params.a - 0.5, this.params.b + 0.5 ; case 'normal': return this.params.mu - 4 this.params.sigma, this.params.mu + 4 this.params.sigma ; case 'exponential': return 0, 5 / this.params.lambda ; case 'beta': return 0, 1 ; draw this.ctx.clearRect 0, 0, this.width, this.height ; const marginX = 50; const marginY = 50; const plotWidth = this.width - 2 marginX; const plotHeight = this.height - 2 marginY; // Draw axes this.ctx.strokeStyle = ' ddd'; this.ctx.lineWidth = 1; this.ctx.beginPath ; this.ctx.moveTo marginX, this.height - marginY ; this.ctx.lineTo this.width - marginX, this.height - marginY ; this.ctx.moveTo marginX, marginY ; this.ctx.lineTo marginX, this.height - marginY ; this.ctx.stroke ; const minX, maxX = this.getRange ; // Find max PDF for scaling let maxPDF = 0; for let i = 0; i const element = document.getElementById slider + '-slider' ; element.addEventListener 'input', e => this.params slider = parseFloat e.target.value ; document.getElementById slider + '-value' .textContent = this.params slider .toFixed 1 ; this.updateStats ; this.draw ; ; ; document.getElementById 'generate-samples' .addEventListener 'click', => this.generateSamples ; this.draw ; ; this.updateStats ; updateStats const cov11 = this.params.sigma1 this.params.sigma1; const cov12 = this.params.rho this.params.sigma1 this.params.sigma2; const cov22 = this.params.sigma2 this.params.sigma2; const det = cov11 cov22 - cov12 cov12; document.getElementById 'cov11' .textContent = cov11.toFixed 3 ; document.getElementById 'cov12' .textContent = cov12.toFixed 3 ; document.getElementById 'cov22' .textContent = cov22.toFixed 3 ; document.getElementById 'det-cov' .textContent = det.toFixed 3 ; generateSamples this.samples = ; const n = 100; for let i = 0; i this.ctx.strokeStyle = colors idx ; this.ctx.lineWidth = 1; this.ctx.beginPath ; const a = level this.params.sigma1; const b = level this.params.sigma2; const angle = 0.5 Math.atan2 2 this.params.rho this.params.sigma1 this.params.sigma2, this.params.sigma1 this.params.sigma1 - this.params.sigma2 this.params.sigma2 ; for let i = 0; i 0 this.ctx.fillStyle = ' 2196f3'; this.samples.forEach x1, x2 => const plotX = marginX + x1 + 4 / 8 plotWidth; const plotY = this.height - marginY - x2 + 4 / 8 plotHeight; if plotX >= marginX && plotX = marginY && plotY input type=\"range\" -webkit-appearance: none; appearance: none; height: 5px; background: ddd; outline: none; border-radius: 5px; input type=\"range\" ::-webkit-slider-thumb -webkit-appearance: none; appearance: none; width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; input type=\"range\" ::-moz-range-thumb width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; border: none; canvas border-radius: 5px; .demo-container margin: 20px 0;",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter00/00_04_02_Common_Probability_Distributions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter01/01_00_Introduction",
    "title": "01 Introduction",
    "chapter": "01",
    "order": 1,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "Introduction to Mathematical Optimization Problems—especially Convex Optimization Problems.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter01/01_00_Introduction/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter01/01_01_optimization_problems",
    "title": "01-01 Optimization problems?",
    "chapter": "01",
    "order": 2,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "An optimization problem is a problem in which, among many possible candidates, we seek the optimal solution optimal value or a value close to the optimal. In data science and machine learning , optimization is everywhere: - Training neural networks : Finding weights that minimize prediction error - Linear regression : Finding the best-fit line that minimizes squared errors - Clustering : Optimizing cluster centers to minimize within-cluster variance - Feature selection : Choosing the best subset of features to maximize model performance - Hyperparameter tuning : Finding optimal learning rates, regularization parameters, etc. Every time you train a machine learning model, you're solving an optimization problem! Mathematical optimization problems A mathematical optimization problem can be expressed as follows: > MATH \\begin align >&\\min x\\in D \\ && f x \\\\ >&\\text subject to && g i x \\le 0,\\ i = 1, ..., m \\\\ >&&& h j x = 0,\\ j = 1,\\ ..., r >\\end align MATH Mathematical Optimization Problem in standard form MATH is the optimization variable MATH is the objective or cost function MATH are the inequality constraint functions MATH are the equality constraint functions The vector MATH that minimizes the objective function MATH over the feasible domain the set of all points satisfying the constraints is denoted as MATH and called the optimal solution. Constraints can be classified into two types: 1. Explicit constraints: Constraints that are directly specified in the optimization problem. In the standard form above, the constraints expressed by the functions MATH and MATH are explicit. If there are no explicit constraints, the problem is called an unconstrained problem. 2. Implicit constraints: Constraints that are not directly specified, but arise from the intersection of the domains of the objective and constraint functions. MATH Note: MATH means the domain of the function MATH . > Example: implicit constraint ↔ explicit constraint > >Suppose the optimization problem is given as follows: > > MATH > >Here, the domain of the objective function MATH is MATH , so MATH is an implicit constraint. If we write this as an optimization problem with explicit constraints: > > MATH > 💡 Pro Tip : Start with convex formulations when possible - they're easier to solve and understand. Only move to complex non-convex models when simpler approaches fail to meet your requirements.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter01/01_01_optimization_problems/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter01/01_02_convex_optimization_problem",
    "title": "01-02 Convex optimization problem",
    "chapter": "01",
    "order": 3,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "A convex optimization problem is a type of optimization problem. > MATH \\begin align >&\\min x\\in D \\ &&f x \\\\ >&\\text subject to && g i x \\le 0,\\ i = 1, ..., m \\\\ >&&& h j x = 0,\\ j = 1,\\ ..., r >\\end align MATH Convex Optimization Problem in standard form 3 Here, the objective function MATH and the inequality constraint functions MATH are convex, and the equality constraint functions MATH are affine. An affine function is a linear function plus a constant: > MATH are affine: MATH What does it mean for a function to be convex? To understand this, we first need to understand convex sets. Convex sets A line segment connecting two points MATH and MATH is defined as: > MATH with MATH Given a set, if the line segment between any two points MATH and MATH in the set is also contained in the set, we call it a convex set. In other words, a set MATH is convex if: > MATH , MATH MATH MATH For example, in the figure below, only the leftmost shape is a convex set. Fig1 left: a convex set, mid & right: non-convex sets 2 Convex functions A convex function is defined as follows: > MATH is convex if MATH is a convex set and, > > MATH for all MATH Geometrically, this means that for any two points MATH and MATH on the graph of MATH , the line segment connecting them lies above the graph between MATH and MATH . Fig2 Convex Function 2 Relation between a convex set and a convex function There is a close relationship between convex functions and convex sets: > A function MATH is convex if and only if its epigraph is a convex set. What is an epigraph? 'Epi' means 'above', so the epigraph of MATH is the set of points above the graph of MATH . Formally, the epigraph is defined as: > MATH \\eqalign & \\text epigraph of f: \\mathbb R ^n \\rightarrow \\mathbb R \\\\ & \\text epi f = \\ x, t \\in \\mathbb R ^ n+1 \\mid x \\in \\text dom f, f x \\le t\\ MATH Fig3 Epigraph 2 If MATH is a convex function, then MATH is always a convex set, and vice versa. This is a key property connecting the definitions of convex functions and convex sets. Convex and concave functions A function MATH is concave if MATH is convex. Equivalently, MATH is concave if: > MATH for all MATH Geometrically, this means that the line segment connecting any two points on the graph lies below the graph of the function. A concave function \"curves downward\" while a convex function \"curves upward\". What about concave functions? Why do we emphasize convex functions so much, and seemingly \"ignore\" concave ones? - We \"don't care\" about concave functions separately because they're just the mirror image of convex ones. Always reformulate maximization of concave MATH as minimization of convex MATH . Nice property of convex optimization problems A local minimum of a convex function is always a global minimum. For convex optimization problems, solutions are generally easier to find than for non-convex problems, because convex functions have the following property: > If MATH is convex and MATH is a locally optimal point i.e., a local minimum , then MATH is also a globally optimal point. Let's prove this by contradiction: > Proof by contradiction: > >Suppose for a convex function MATH , MATH is a locally optimal point but not a globally optimal point. Let MATH be a feasible global optimal point, so for any positive MATH , MATH and MATH \\theta=\\frac \\rho 2\\|y-x\\| 2 MATH z = \\theta y + 1 - \\theta x = x + \\theta y - x MATH . Then: > >1. MATH is a convex combination of two feasible points MATH , so it is also feasible. > >2. MATH \\|z - x\\| 2 = \\theta \\|y - x\\| 2 = \\frac \\rho 2 >3. MATH f z \\le \\theta f y + 1 - \\theta f x >Points 2 and 3 contradict the assumption that MATH is a locally optimal point, so by contradiction, any locally optimal point MATH is also globally optimal. Convex combination A convex combination of MATH is defined as: > MATH with MATH If MATH is a convex set and MATH , then MATH as well.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter01/01_02_convex_optimization_problem/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter01/01_03_goals_and_topics",
    "title": "01-03 Goals and Topics",
    "chapter": "01",
    "order": 4,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "Goals The goal of this course is to develop the following abilities: Recognize when a given problem is a convex optimization problem Formulate a given situation as a convex optimization problem Select the most appropriate algorithm to solve a defined convex optimization problem Topics To achieve these goals, the following topics will be covered: Convex sets, functions, optimization problems Examples and applications Algorithms In particular, the main focus will be on algorithms. Algorithms There are many different methods for solving optimization problems. The performance of each method depends on the properties of the problem being solved. To choose the most efficient algorithm, a deep understanding of both the problem and the algorithms is required. Let's look at an example: total variation denoising. Example: Total variation denoising Fig1 Total Variation Denoising 3 Suppose you receive a noisy image middle , and you want to remove the noise to obtain a solution right close to the true image left . If each pixel value is MATH , this problem can be formulated as the following optimization problem, commonly called the 2D fused lasso or 2D total variation denoising problem: > MATH E: the set of edges between all neighboring MATH MATH : Least squares loss. Forces MATH to be close to MATH MATH : Total variation smoothing. Used when the change between neighboring pixels is not large piecewise constant . Choosing the right smoothing method requires careful consideration of the problem's characteristics. For more details on total variation smoothing, see Chapter 6.1.2 and 6.3 in Reference 1. The convex optimization problem above can be solved using the Specialized ADMM http://stanford.edu/~boyd/admm.html algorithm, which yields the solution on the right after 20 iterations. Specialized ADMM, 20 iterations Fig2 Specialized ADMM Result 3 Proximal gradient descent, 1000 iterations Fig3 Proximal Gradient Descent Result 3 Coordinate descent, 10K cycles Fig4 Coordinate Descent Result 3 As shown above, for the 2D fused lasso problem, Specialized ADMM performs best among the three methods. However, for other problems, the other two methods may outperform Specialized ADMM. In later chapters, we will analyze various algorithms and problems to learn how to select the most appropriate algorithm.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter01/01_03_goals_and_topics/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter01/01_04_brief_history_of_convex_optimization",
    "title": "01-04 Brief history of convex optimization",
    "chapter": "01",
    "order": 5,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Lịch sử phát triển của Tối ưu hóa Lồi Tối ưu hóa lồi có một lịch sử phát triển phong phú, từ những nền tảng lý thuyết đầu tiên đến các ứng dụng hiện đại trong công nghệ và khoa học. Hãy cùng khám phá hành trình này qua các giai đoạn quan trọng. 🎯 Tổng quan Timeline 1900-1970 Phát triển nền tảng lý thuyết 1947-1990 Các thuật toán đột phá 1990-nay Bùng nổ ứng dụng 📚 Giai đoạn 1: Nền tảng Lý thuyết 1900-1970 Những người tiên phong Hermann Minkowski 1864-1909 - Đặt nền móng cho hình học lồi với khái niệm \"tập lồi\" - Phát triển lý thuyết về các đa diện lồi - Ảnh hưởng: Tạo ra ngôn ngữ toán học cơ bản cho tối ưu hóa lồi Leonid Kantorovich 1912-1986 - Phát triển lý thuyết lập trình tuyến tính vào những năm 1930 - Giải quyết bài toán phân bổ tài nguyên tối ưu - Đóng góp: Chứng minh tính khả thi của việc tối ưu hóa trong kinh tế John von Neumann 1903-1957 - Phát triển lý thuyết trò chơi và mối liên hệ với tối ưu hóa - Đưa ra định lý minimax cơ bản - Ảnh hưởng: Kết nối tối ưu hóa với lý thuyết quyết định Các khái niệm nền tảng được hình thành: - Tập lồi và hàm lồi : Định nghĩa chính xác và tính chất cơ bản - Điều kiện tối ưu : Phát triển các điều kiện cần và đủ - Duality theory : Khái niệm về bài toán đối ngẫu ⚙️ Giai đoạn 2: Cách mạng Thuật toán 1947-1990 1947: Thuật toán Simplex - Bước ngoặt lịch sử George Dantzig đã tạo ra cuộc cách mạng với thuật toán Simplex: Ý tưởng cốt lõi: Di chuyển dọc theo các cạnh của đa diện khả thi để tìm điểm tối ưu tại một đỉnh. Tại sao quan trọng? - Lần đầu tiên có thuật toán thực tế để giải bài toán lập trình tuyến tính - Mở ra khả năng ứng dụng trong logistics, sản xuất, quân sự - Hiệu quả cao trong thực tế dù có độ phức tạp tệ nhất là exponential 1960s: Phương pháp Điểm trong đầu tiên Fiacco & McCormick phát triển: - Barrier methods cho bài toán có ràng buộc bất đẳng thức - Ý tưởng: Thêm hàm penalty để \"đẩy\" nghiệm vào bên trong miền khả thi Dikin đóng góp: - Affine scaling method - Cải thiện hướng tiếp cận của barrier methods 1970s: Phương pháp Ellipsoid và Subgradient Shor, Nemirovski, Yudin phát triển: - Ellipsoid method : Thuật toán đa thức đầu tiên cho LP - Subgradient methods : Xử lý hàm không khả vi Ý nghĩa lý thuyết: - Chứng minh LP thuộc lớp P polynomial time - Mở rộng khả năng giải các bài toán không trơn 1984: Đột phá của Karmarkar Narendra Karmarkar tạo ra cuộc cách mạng thứ hai: - Thuật toán interior-point với độ phức tạp \\\\ O n^ 3.5 L \\\\ - Hiệu quả thực tế cao hơn simplex cho bài toán lớn - Khởi đầu cho kỷ nguyên interior-point methods Late 1980s-1990s: Tổng quát hóa Nesterov & Nemirovski 1994 mở rộng: - Interior-point methods cho tối ưu lồi phi tuyến - Self-concordant functions - Polynomial-time algorithms cho broader class 🚀 Giai đoạn 3: Bùng nổ Ứng dụng 1990-nay Trước 1990: Giới hạn trong Operations Research Các ứng dụng chính: - Transportation problems : Tối ưu hóa vận chuyển hàng hóa - Production planning : Lập kế hoạch sản xuất - Resource allocation : Phân bổ tài nguyên trong doanh nghiệp - Military logistics : Ứng dụng trong quân sự WWII và Cold War Tại sao giới hạn? - Máy tính chưa đủ mạnh - Thiếu software tools - Chưa nhận thức được tiềm năng trong engineering Từ 1990: Cách mạng Ứng dụng 🎛️ Control Systems Hệ thống Điều khiển - Model Predictive Control MPC : Điều khiển dự báo - Robust control : Điều khiển bền vững - Ví dụ : Điều khiển nhiệt độ trong nhà máy, autopilot máy bay 📡 Signal Processing Xử lý Tín hiệu - Compressed sensing : Khôi phục tín hiệu từ ít mẫu - Image denoising : Khử nhiễu ảnh - Ví dụ : MRI imaging, radar processing 📱 Communications Truyền thông - Beamforming : Định hướng sóng trong antenna arrays - Power allocation : Phân bổ công suất trong mạng wireless - Ví dụ : 5G networks, satellite communications 💻 Circuit Design Thiết kế Mạch - Gate sizing : Tối ưu kích thước transistor - Power optimization : Tối ưu tiêu thụ năng lượng - Ví dụ : CPU design, mobile chip optimization 🆕 Các lớp bài toán mới Semidefinite Programming SDP Tối ưu hóa trên ma trận bán xác định dương Ứng dụng: Relaxation của bài toán combinatorial Second-Order Cone Programming SOCP Tối ưu hóa với ràng buộc hình nón bậc hai Ứng dụng: Robust optimization, portfolio optimization Robust Optimization Tối ưu hóa với uncertainty trong dữ liệu Ứng dụng: Finance, supply chain management 🔮 Xu hướng Hiện tại và Tương lai Machine Learning Integration - Convex relaxations của neural networks - Optimization trong training : Adam, RMSprop - Regularization : L1, L2 penalties Big Data Applications - Distributed optimization : Xử lý dữ liệu lớn - Online algorithms : Học trực tuyến - Streaming optimization : Tối ưu real-time Quantum Computing - Quantum convex optimization : Thuật toán lượng tử - Variational quantum algorithms : QAOA, VQE 💡 Bài học từ Lịch sử 1. Lý thuyết dẫn đường cho thực hành : Nền tảng toán học vững chắc là cần thiết 2. Công nghệ thúc đẩy ứng dụng : Máy tính mạnh mở ra khả năng mới 3. Interdisciplinary collaboration : Sự kết hợp giữa các lĩnh vực tạo ra đột phá 4. Practical needs drive innovation : Nhu cầu thực tế thúc đẩy phát triển thuật toán --- 🎯 Takeaway chính Tối ưu hóa lồi đã phát triển từ một lĩnh vực toán học thuần túy thành công cụ không thể thiếu trong công nghệ hiện đại. Sự kết hợp giữa lý thuyết vững chắc và thuật toán hiệu quả đã tạo ra những ứng dụng đột phá trong mọi lĩnh vực của cuộc sống. 🎮 Khám phá Tương tác Simplex Algorithm Visualization Minh họa Thuật toán Simplex Thử nghiệm với bài toán lập trình tuyến tính đơn giản: Bài toán: Maximize \\\\ c 1x 1 + c 2x 2\\\\ Subject to: \\\\ x 1, x 2 \\geq 0\\\\ và các ràng buộc tuyến tính c₁: 3 c₂: 2 Chạy Simplex Reset Timeline Explorer Khám phá Timeline Tương tác Năm: 1947 1947: Thuật toán Simplex George Dantzig phát triển thuật toán Simplex, mở đầu kỷ nguyên tối ưu hóa thực tế. Tác động: Cách mạng trong operations research Ứng dụng Hiện đại Ứng dụng trong Cuộc sống Portfolio Optimization Supply Chain Machine Learning Signal Processing 📈 Portfolio Optimization Bài toán: Phân bổ vốn đầu tư để tối đa hóa lợi nhuận và giảm thiểu rủi ro Công thức: \\\\ \\min \\frac 1 2 w^T\\Sigma w - \\mu^T w\\\\ Ứng dụng: Quỹ đầu tư, ngân hàng, bảo hiểm // Simplex Algorithm Demo function runSimplex const canvas = document.getElementById 'simplex-canvas' ; const ctx = canvas.getContext '2d' ; const c1 = parseFloat document.getElementById 'c1' .value ; const c2 = parseFloat document.getElementById 'c2' .value ; // Clear canvas ctx.clearRect 0, 0, canvas.width, canvas.height ; // Draw feasible region simple example ctx.fillStyle = 'rgba 0, 122, 204, 0.2 '; ctx.beginPath ; ctx.moveTo 50, 250 ; ctx.lineTo 200, 250 ; ctx.lineTo 200, 100 ; ctx.lineTo 100, 50 ; ctx.lineTo 50, 100 ; ctx.closePath ; ctx.fill ; // Draw objective function direction ctx.strokeStyle = ' ff6b35'; ctx.lineWidth = 3; ctx.beginPath ; ctx.moveTo 125, 175 ; ctx.lineTo 125 + c1 20, 175 - c2 20 ; ctx.stroke ; // Add labels ctx.fillStyle = ' 333'; ctx.font = '14px Arial'; ctx.fillText 'Feasible Region', 60, 200 ; ctx.fillText Objective: MATH c2.toFixed 1 x₂ , 220, 50 ; // Show steps document.getElementById 'simplex-steps' .innerHTML = Các bước Simplex: Bắt đầu tại đỉnh 0,0 Kiểm tra hướng cải thiện: MATH c2.toFixed 1 Di chuyển dọc theo cạnh có gradient tốt nhất Dừng khi không thể cải thiện thêm ; function resetDemo document.getElementById 'c1' .value = 3; document.getElementById 'c2' .value = 2; document.getElementById 'c1-value' .textContent = '3'; document.getElementById 'c2-value' .textContent = '2'; document.getElementById 'simplex-steps' .innerHTML = ''; const canvas = document.getElementById 'simplex-canvas' ; const ctx = canvas.getContext '2d' ; ctx.clearRect 0, 0, canvas.width, canvas.height ; // Timeline Explorer const timelineEvents = 1900: title: \"1900: Nền tảng Hình học\", desc: \"Minkowski phát triển lý thuyết tập lồi\", impact: \"Tạo nền tảng toán học\" , 1930: title: \"1930: Kantorovich\", desc: \"Lý thuyết lập trình tuyến tính đầu tiên\", impact: \"Ứng dụng trong kinh tế\" , 1947: title: \"1947: Thuật toán Simplex\", desc: \"Dantzig tạo ra thuật toán thực tế đầu tiên\", impact: \"Cách mạng operations research\" , 1960: title: \"1960s: Interior Point\", desc: \"Fiacco & McCormick phát triển barrier methods\", impact: \"Mở rộng khả năng giải bài toán\" , 1984: title: \"1984: Karmarkar\", desc: \"Interior-point polynomial-time algorithm\", impact: \"Đột phá về độ phức tạp\" , 1994: title: \"1994: Nesterov & Nemirovski\", desc: \"Tổng quát hóa cho convex optimization\", impact: \"Nền tảng cho ứng dụng hiện đại\" , 2010: title: \"2010s: Machine Learning\", desc: \"Tích hợp với AI và Big Data\", impact: \"Ứng dụng rộng rãi trong công nghệ\" , 2024: title: \"2024: Quantum Computing\", desc: \"Thuật toán tối ưu lượng tử\", impact: \"Tương lai của tính toán\" ; function updateTimeline const year = parseInt document.getElementById 'year-slider' .value ; document.getElementById 'current-year' .textContent = year; // Find closest event let closestYear = 1900; for let eventYear in timelineEvents if parseInt eventYear $ app.title Bài toán: $ app.problem Công thức: \\\\ $ app.formula \\\\ Ứng dụng: $ app.usage ; // Re-render MathJax if window.MathJax MathJax.typesetPromise document.getElementById 'app-content' ; // Event listeners document.addEventListener 'DOMContentLoaded', function // Slider updates document.getElementById 'c1' .addEventListener 'input', function document.getElementById 'c1-value' .textContent = this.value; ; document.getElementById 'c2' .addEventListener 'input', function document.getElementById 'c2-value' .textContent = this.value; ; document.getElementById 'year-slider' .addEventListener 'input', updateTimeline ; // Initialize updateTimeline ; showApplication 'portfolio' ; ; .timeline-container position: relative; margin: 20px 0; .timeline-item display: flex; margin: 10px 0; padding: 10px; border-left: 3px solid 007acc; background: f8f9fa; .timeline-year font-weight: bold; color: 007acc; min-width: 100px; .timeline-content margin-left: 20px; .info-box background: e3f2fd; border: 1px solid 2196f3; border-radius: 5px; padding: 15px; margin: 20px 0; .info-box h4 margin-top: 0; color: 1976d2; / Interactive Demo Styles / simplex-demo, timeline-explorer, applications-demo background: f8f9fa; border: 1px solid dee2e6; border-radius: 8px; padding: 20px; margin: 20px 0; .controls margin: 15px 0; .controls label display: inline-block; margin: 5px 10px 5px 0; min-width: 40px; .controls input type=\"range\" width: 150px; margin: 0 10px; .controls button background: 007acc; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; margin: 5px; .controls button:hover background: 005a9e; simplex-canvas border: 1px solid ccc; margin: 10px 0; background: white; .year-slider margin: 15px 0; .year-slider input type=\"range\" width: 300px; year-info background: white; padding: 15px; border-radius: 5px; border-left: 4px solid 007acc; .app-selector margin: 15px 0; .app-btn background: 28a745; color: white; border: none; padding: 10px 15px; border-radius: 5px; cursor: pointer; margin: 5px; font-size: 14px; .app-btn:hover background: 218838; .app-example background: white; padding: 15px; border-radius: 5px; border-left: 4px solid 28a745; margin-top: 15px; .problem-setup background: white; padding: 15px; border-radius: 5px; margin: 10px 0; simplex-steps background: white; padding: 15px; border-radius: 5px; margin-top: 10px; simplex-steps ol margin: 10px 0; padding-left: 20px; simplex-steps li margin: 5px 0;",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter01/01_04_brief_history_of_convex_optimization/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_Convex_sets",
    "title": "02 Convex Sets",
    "chapter": "02",
    "order": 0,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "In this chapter, we will study the concept of convex sets, which form the foundation of convex optimization. Background Convex optimization refers to techniques for finding maxima or minima by defining problems using convex functions. Convex sets are closely related to convex functions in two main ways: Convex functions are defined over convex sets. The domain and range of a function are defined as convex sets, and the main properties of convex functions are determined by these sets. If you can transform an optimization problem into one involving a convex function, it becomes easier to solve. Sometimes, it is difficult to determine whether a problem is defined by a convex function. In such cases, you can check whether the epigraph of the function is a convex set to determine if the function is convex. Content In this chapter, we will cover the definition and examples of convex sets, their main properties, and operations that preserve convexity.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_Convex_sets/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_01_Affine_and_Convex_Sets",
    "title": "02-01 Affine and Convex Sets",
    "chapter": "02",
    "order": 1,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "In this chapter, we will explore the concepts and definitions centered around convex sets. We introduce three types of sets: the most basic is the affine set, and by imposing additional constraints on the affine set, we define the convex set and the cone. Interestingly, these sets can be thought of as collections of many lines line , line segments, or rays. An affine set is formed by gathering many lines, a convex set by gathering many line segments, and a cone by gathering many rays. This perspective makes it easier to understand these concepts. Also, a cone is sometimes called a nonnegative homogeneous set, which refers to the property that it extends only in one direction from the origin, helping to clarify the naming.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_01_Affine_and_Convex_Sets/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_01_01_Line_line_segment_ray",
    "title": "02-01-01 Line, line segment, ray",
    "chapter": "02",
    "order": 2,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "To define affine sets, convex sets, and cones, let's first look at lines, line segments, and rays. A line is an infinitely extending straight path passing through two points in both directions. In contrast, a line segment is a straight path defined only between two points, and a ray starts at one point and extends infinitely in one direction through another point. The figure below shows a line and a line segment. Depending on the range of the parameter MATH , you can imagine how a line, line segment, or ray is defined. MATH Reference When you use two points included in a set to create a line, line segment, or ray, whether these are included in the set determines the definition of the set. You can also define sets using multiple points and their affine, convex, or conic combinations. Details will be explained in the following sections. Line A Line passing through two points MATH and MATH is defined as: > MATH with MATH Line segment A Line segment is defined by restricting MATH to the interval 0, 1 : > MATH with MATH Alternatively, you can express it as: > MATH with MATH Ray A Ray starts at one point and extends infinitely in one direction: > MATH with MATH Or equivalently: > MATH with MATH Now you can see that the range of MATH is MATH for lines, MATH for line segments, and MATH for rays. Furthermore, you will find that the ranges of MATH are the same in the affine sets, convex sets, and conic sets that we will define later.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_01_01_Line_line_segment_ray/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_01_02_Affine_set",
    "title": "02-01-02 Affine set",
    "chapter": "02",
    "order": 3,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "An affine set is a collection with no boundaries, such as a point, line, plane, or hyperplane. To determine if a set is affine, check if the line passing through any two points in the set is also contained within the set. The absence of boundaries means that if any space has a boundary, it cannot be an affine set. Let's define this mathematically. Affine set A set MATH is an affine set if for any two points MATH , the line passing through them is also in MATH : > MATH with MATH This can be interpreted as a linear combination of two points in MATH , where the sum of the coefficients is 1. If the result is always in MATH , then MATH is an affine set. Affine combination A linear combination of several points where the sum of the coefficients is 1 is called an affine combination : > MATH with MATH If every affine combination of points in a set MATH is also in MATH , then MATH is an affine set. Affine hull The set of all affine combinations of points in MATH is called the affine hull of MATH , denoted as aff MATH : > MATH Relationship between affine set and subspace If MATH is an affine set and MATH , then MATH is a subspace: > MATH Thus, \"An affine set MATH is a translation of a linear subspace MATH by MATH ,\" where MATH can be any point in MATH . The dimension of MATH is the same as that of MATH . Reference Proof that MATH is a subspace To prove MATH is a subspace, show that it is closed under addition and scalar multiplication. That is, for MATH and MATH , MATH . This follows from the definition above.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_01_02_Affine_set/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_01_05_Hyperplane",
    "title": "02-01-05 Hyperplane",
    "chapter": "02",
    "order": 4.5,
    "owner": "AI Assistant",
    "lesson_type": "",
    "content": "A hyperplane is one of the most fundamental geometric objects in convex optimization and linear algebra. It serves as a building block for understanding more complex convex sets and plays a crucial role in optimization algorithms, machine learning, and geometric analysis. Definition of Hyperplane A hyperplane in MATH is a set of the form: > MATH where MATH is a nonzero vector MATH and MATH is a scalar. The vector MATH is called the normal vector to the hyperplane, and it determines the orientation of the hyperplane. The scalar MATH determines the position of the hyperplane relative to the origin. Geometric Interpretation - In MATH : A hyperplane is a line - In MATH : A hyperplane is a plane - In MATH MATH : A hyperplane is an MATH -dimensional subspace The hyperplane divides the entire space MATH into two halfspaces : - Positive halfspace : MATH - Negative halfspace : MATH Properties of Hyperplanes 1. Affine Set Property Every hyperplane is an affine set . This means that if MATH , then the entire line passing through them is also contained in MATH : MATH Proof : If MATH and MATH , then: MATH 2. Convex Set Property Since every affine set is convex, hyperplanes are convex sets . For any MATH and MATH : MATH 3. Closed Set Property Hyperplanes are closed sets because they are the level sets of continuous linear functions. 4. Dimension A hyperplane in MATH has dimension MATH . Alternative Representations 1. Point-Normal Form If we know a point MATH on the hyperplane and the normal vector MATH , the hyperplane can be written as: MATH This is equivalent to MATH , so MATH . 2. Matrix Form A hyperplane can also be represented using matrix notation. If MATH is a MATH matrix row vector , then: MATH 3. Parametric Form A hyperplane can be parameterized using a basis for its null space. If MATH is an orthonormal basis for the null space of MATH , and MATH is any point on the hyperplane, then: MATH Distance from Point to Hyperplane The distance from a point MATH to the hyperplane MATH is given by: MATH This formula comes from projecting the vector from any point on the hyperplane to MATH onto the normal direction. Derivation Let MATH be the closest point on the hyperplane to MATH . Then MATH is parallel to the normal vector MATH : MATH Since MATH , we have MATH . Substituting: MATH Solving for MATH : MATH The distance is MATH . Examples Example 1: Line in MATH The hyperplane MATH represents a line in the plane. - Normal vector: MATH - The line passes through points MATH and MATH - Distance from origin: MATH Example 2: Plane in MATH The hyperplane MATH represents a plane in 3D space. - Normal vector: MATH - The plane passes through points MATH , MATH , and MATH Example 3: Hyperplane through Origin The hyperplane MATH always passes through the origin and is actually a subspace of dimension MATH . Interactive Visualization Normal vector a₁: 1.0 Normal vector a₂: 1.0 Offset b: 0.0 Reset Hyperplane equation: x₁ + x₂ = 0 Distance from origin: 0.0 class HyperplaneVisualizer constructor this.canvas = document.getElementById 'hyperplane-canvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; // Parameters this.a1 = 1; this.a2 = 1; this.b = 0; // Scale and offset for coordinate system this.scale = 40; this.centerX = this.width / 2; this.centerY = this.height / 2; this.setupControls ; this.draw ; setupControls const a1Slider = document.getElementById 'a1-slider' ; const a2Slider = document.getElementById 'a2-slider' ; const bSlider = document.getElementById 'b-slider' ; const resetBtn = document.getElementById 'reset-btn' ; a1Slider.addEventListener 'input', e => this.a1 = parseFloat e.target.value ; document.getElementById 'a1-value' .textContent = this.a1.toFixed 1 ; this.updateDisplay ; ; a2Slider.addEventListener 'input', e => this.a2 = parseFloat e.target.value ; document.getElementById 'a2-value' .textContent = this.a2.toFixed 1 ; this.updateDisplay ; ; bSlider.addEventListener 'input', e => this.b = parseFloat e.target.value ; document.getElementById 'b-value' .textContent = this.b.toFixed 1 ; this.updateDisplay ; ; resetBtn.addEventListener 'click', => this.a1 = 1; this.a2 = 1; this.b = 0; a1Slider.value = 1; a2Slider.value = 1; bSlider.value = 0; document.getElementById 'a1-value' .textContent = '1.0'; document.getElementById 'a2-value' .textContent = '1.0'; document.getElementById 'b-value' .textContent = '0.0'; this.updateDisplay ; ; updateDisplay this.draw ; this.updateInfo ; updateInfo // Update equation const eq = MATH this.a2.toFixed 1 x₂ = $ this.b.toFixed 1 ; document.getElementById 'equation' .textContent = eq; // Update distance from origin const distance = Math.abs this.b / Math.sqrt this.a1 this.a1 + this.a2 this.a2 ; document.getElementById 'distance' .textContent = distance.toFixed 3 ; worldToScreen x, y return x: this.centerX + x this.scale, y: this.centerY - y this.scale ; draw // Clear canvas this.ctx.clearRect 0, 0, this.width, this.height ; // Draw coordinate system this.drawCoordinateSystem ; // Draw hyperplane line in 2D this.drawHyperplane ; // Draw normal vector this.drawNormalVector ; // Draw distance from origin this.drawDistanceFromOrigin ; drawCoordinateSystem this.ctx.strokeStyle = ' ddd'; this.ctx.lineWidth = 1; // Grid lines for let i = -10; i 1e-10 // Line is not vertical x1 start = -10; y1 start = this.b - this.a1 x1 start / this.a2; x1 end = 10; y1 end = this.b - this.a1 x1 end / this.a2; else // Line is vertical x1 start = x1 end = this.b / this.a1; y1 start = -10; y1 end = 10; const start = this.worldToScreen x1 start, y1 start ; const end = this.worldToScreen x1 end, y1 end ; this.ctx.beginPath ; this.ctx.moveTo start.x, start.y ; this.ctx.lineTo end.x, end.y ; this.ctx.stroke ; // Label const midX = start.x + end.x / 2; const midY = start.y + end.y / 2; this.ctx.fillStyle = ' 2196F3'; this.ctx.font = 'bold 14px Arial'; this.ctx.fillText 'Hyperplane', midX + 10, midY - 10 ; drawNormalVector if Math.abs this.a1 Relationship to Other Concepts Connection to Affine Sets Every hyperplane is an affine set, but not every affine set is a hyperplane. Hyperplanes are specifically MATH -dimensional affine sets in MATH . Connection to Linear Algebra The hyperplane MATH is the level set of the linear function MATH at level MATH . The gradient of this function is constant and equal to MATH , which explains why MATH is perpendicular to the hyperplane. Connection to Optimization In constrained optimization, equality constraints often define hyperplanes that restrict the feasible region. The method of Lagrange multipliers exploits the fact that at an optimal point, the gradient of the objective function is parallel to the normal vector of the constraint hyperplane.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_01_05_Hyperplane/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_01_03_Convex-set",
    "title": "02-01-03 Convex set",
    "chapter": "02",
    "order": 4,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Now let's look at the core concept of this chapter: the convex set. Intuitively, a convex set is a set with no \"dents\" or \"holes\" inside. To determine if a set is convex, check if the line segment connecting any two points in the set is also contained within the set. Convex set A set MATH is a convex set if for any two points MATH , the line segment connecting them is also in MATH : > MATH with MATH , MATH This means that for any two points in MATH , all points on the line segment between them are also in MATH . The figure below shows examples of convex sets. The triangle on the left is convex, but the shape with a cavity on the right is not convex because the line segment between some points leaves the set. Fig1 Convex Set 1 Convex combination A linear combination of several points where the coefficients are non-negative and sum to 1 is called a convex combination : > A point of the form MATH with MATH If every convex combination of points in a set MATH is also in MATH , then MATH is a convex set. Convex hull The set of all convex combinations of points in MATH is called the convex hull of MATH , denoted as conv MATH : > conv MATH The figure below shows the convex hull for a set of 15 points and a shape with a cavity. Convex Hull Convex Hull",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_01_03_Convex-set/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_01_04_Convex_cone",
    "title": "02-01-04 Cone",
    "chapter": "02",
    "order": 5,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "A cone is a set that extends infinitely in certain directions, like a beam of light from a source, but is not defined in the opposite direction. To determine if a set is a cone, check if the ray starting from the origin and passing through any point in the set is also contained within the set. Thus, a cone must include the origin. Because a cone has boundaries, it cannot be an affine set. Let's define this mathematically. Cone A set MATH is a cone or nonnegative homogeneous set if for any point MATH , the ray MATH is also in MATH for MATH : > MATH with MATH , MATH Reference Unlike affine or convex sets, when defining a cone, the starting point of the ray is assumed to be the origin, so only one point is used. Convex Cone A set MATH is a convex cone if it is both a cone and convex: > MATH with MATH , MATH The figure below shows a pie-shaped convex cone. In the figure, MATH and MATH are points in the cone, and MATH and MATH are non-negative scalars. Fig1 Convex Cone 1 Conic combination A linear combination of several points where all coefficients are non-negative is called a conic combination or nonnegative linear combination : > A point of the form MATH with MATH If every conic combination of points in a set MATH is also in MATH , then MATH is a conic set. Conic hull The set of all conic combinations of points in MATH is called the conic hull of MATH . The conic hull is always the smallest convex cone containing MATH : > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_01_04_Convex_cone/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_02_Some_important_examples",
    "title": "02-02 Some important examples",
    "chapter": "02",
    "order": 6,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "This section reviews major examples of convex sets. Trivial ones: empty set, point, line, line segment, ray Hyperplane: MATH , for given MATH , MATH Halfspace: MATH for MATH Affine space: MATH , for given MATH Euclidean ball & ellipsoid Norm ball: MATH , for given norm MATH , radius MATH Convex cone: norm cone, normal cone, positive semidefinite cone",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_02_Some_important_examples/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_02_01_Convex_sets_examples",
    "title": "02-02-01 Convex set examples",
    "chapter": "02",
    "order": 7,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Convex sets include a variety of forms, from trivial ones like points and lines to hyperplanes, halfspaces, balls, ellipsoids, polyhedra, and cones. Hyperplanes A hyperplane is an MATH -dimensional subset that divides an MATH -dimensional space in half, defined as follows. Here, MATH is the normal vector and MATH is the offset from the origin. A hyperplane is both a convex set and an affine set. > MATH with MATH In the figure below, any MATH on the hyperplane satisfies that MATH is orthogonal to MATH . Thus, MATH , so if MATH , then MATH . Fig1 Hyperplane 1 Halfspaces A halfspace is one side of a space divided by a hyperplane. Thus, a single hyperplane MATH defines two halfspaces. A halfspace is a convex set but not an affine set. > MATH or MATH with MATH For the hyperplane MATH , the halfspace MATH is in the direction of the normal vector MATH , while MATH is in the direction of MATH . Fig2 Halfspace 1 Note: The interior of MATH , that is MATH , is called an open halfspace. Key Property: Halfspaces are Convex Take any two points MATH and MATH in the halfspace: - Both satisfy: MATH and MATH For any convex combination: MATH where MATH Check if this point is in the halfspace: MATH Since MATH satisfies the inequality, all points between MATH and MATH are in the halfspace → halfspace is convex. Euclidean balls A Euclidean ball is another convex set, defined as follows. MATH is the Euclidean norm, MATH . MATH is the center and MATH is the radius. Thus, MATH contains all points within radius MATH from center MATH . > MATH Alternatively, the Euclidean ball can be expressed as: > MATH Ellipsoids An ellipsoid is a convex set related to the Euclidean ball, defined as: > MATH Here, MATH means MATH is symmetric and positive definite. The vector MATH is the center of the ellipsoid, and the matrix MATH determines how far the ellipsoid extends in each direction from the center. The axes of the ellipsoid are MATH , where MATH are the eigenvalues of MATH . Thus, a ball is a special case of an ellipsoid with MATH . The figure below shows an ellipsoid. The center MATH is a point, and the major and minor axes are drawn as line segments. Fig3 Ellipsoid 1 The ellipsoid can also be expressed as: > MATH Here, MATH is a square nonsingular matrix. If MATH , the expression matches the previous one, and MATH is symmetric and positive definite. If MATH is symmetric positive semidefinite and singular, it is called a degenerate ellipsoid, and its affine dimension equals the rank of MATH . A degenerate ellipsoid is still convex. Norm balls A norm ball is the set of points within radius MATH from center MATH , defined using an arbitrary norm. While a Euclidean ball uses the Euclidean norm, a norm ball can use any norm. If MATH is any norm on MATH , the norm ball is defined as: > MATH When the p-norm is defined as: > MATH The shape of the norm ball depends on the value of MATH . The figure below shows the shape of the norm ball in 3D for different values of MATH . The norm ball is convex if MATH . Fig4 Norm ball 1 The next figure shows the shape of the norm ball in 2D for different values of MATH . Fig4 Norm ball 2 Polyhedra A polyhedron is defined as the intersection of linear inequalities and equalities. Affine sets subspaces, hyperplanes, lines , rays, line segments, and halfspaces are all polyhedra. Polyhedra are convex sets, and a bounded polyhedron is called a polytope. > MATH A single equality MATH can be represented by two inequalities MATH and MATH . Thus, polyhedra can be defined using only inequalities. The figure below shows a pentagonal polyhedron formed by the intersection of five halfspaces, with outward normal vectors MATH . Fig5 Polyhedra 1 In matrix form, a polyhedron can be defined as: > MATH where MATH A = \\begin bmatrix a^T 1 \\\\ \\vdots \\\\ a^T m \\end bmatrix , MATH MATH C = \\begin bmatrix c^T 1 \\\\ \\vdots \\\\ c^T p \\end bmatrix MATH Simplexes A simplex is the simplest polygon that can be formed in MATH -dimensional space, constructed from MATH points. If there are MATH points MATH that are affinely independent, the simplex is defined as the convex hull of these MATH points. Affinely independent means MATH are linearly independent. > MATH The figure below shows simplexes from 0 to 3 dimensions: a point in 0D, a line segment in 1D, a triangle in 2D, and a tetrahedron in 3D. Fig6 Simplex source - wikipedia A common example of a simplex is the probability simplex: > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_02_01_Convex_sets_examples/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_02_02_Convex_cone_examples",
    "title": "02-02-02 Convex Cone examples",
    "chapter": "02",
    "order": 8,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Examples of convex cones include the norm cone, normal cone, and positive semidefinite cone. Norm cone A norm cone is a cone defined by all points MATH such that MATH in MATH , where the norm can be arbitrary. > MATH , for a norm MATH The figure below shows the norm cone for the MATH norm MATH , also called the second-order cone or ice cream cone. Fig1 Norm Cone 1 Normal Cone For a set MATH and a point MATH , the normal cone is defined as: > MATH The normal cone consists of all vectors MATH such that the inner product with MATH is always less than or equal to zero for all MATH . This means the angle between MATH and MATH is between 90 and 270 degrees i.e., MATH \\cos\\theta Fig2 Normal Cone 3 Positive semidefinite cone The positive semidefinite cone MATH is defined as follows, where MATH represents MATH symmetric matrices: > MATH MATH is a convex cone because for MATH and MATH , we have MATH . This is called the positive semidefinite cone . The following figure shows the boundary of the positive semidefinite cone in MATH plotted in MATH . Since matrix MATH is positive semidefinite, its determinant must be non-negative. MATH X = \\begin bmatrix x, y \\\\\\ y, z \\end bmatrix \\in \\mathbb S ^2 + \\iff x \\ge 0, z \\ge 0, xz \\ge y^2 MATH Fig3 Positive semidefinite cone 1",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_02_02_Convex_cone_examples/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_03_Operations_that_preserve_convexity",
    "title": "02-03 Operations that preserve convexity of convex set",
    "chapter": "02",
    "order": 9,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "This section discusses operations that preserve the convexity of convex sets. These operations are useful for determining convexity or constructing desired convex sets from simple ones like hyperplanes, halfspaces, and norm balls. Operations that preserve convexity include: Intersection Affine functions Perspective function Linear-fractional functions Intersection The intersection of convex sets is convex. That is, if MATH and MATH are convex, then MATH is convex. This property holds even for infinitely many sets. Subspaces, affine sets, and convex cones are also closed under intersection. Convexity can be expressed as the intersection of infinitely many halfspaces, and the converse also holds. That is, a closed convex set MATH can be defined as the intersection of all halfspaces containing MATH : > MATH Affine functions Let MATH and MATH . The function MATH defined by MATH is called an affine function. If MATH is convex and MATH is convex, then: The affine image MATH is convex. The affine preimage MATH is convex. Applying affine functions such as scaling and translation, projection, sum of two sets, and partial sum to convex sets results in convex sets. The solution set of a linear matrix inequality MATH with MATH is also convex. A hyperbolic cone MATH with MATH , MATH is also convex. Perspective function The perspective function models how objects appear smaller when farther away and larger when closer, similar to how a camera projects images. The object is in MATH and its image is in MATH . The perspective function is defined as MATH with dom MATH and MATH , where MATH . This function normalizes the last coordinate to 1 and drops it, reducing the dimension from MATH to MATH . If MATH dom MATH is convex, then the image MATH is also convex. The perspective function works like a pinhole camera: objects farther from the pinhole are projected smaller. The figure below illustrates this principle, showing that objects within the same captured ray are projected identically.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_03_Operations_that_preserve_convexity/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_04_Generalized_inequalities",
    "title": "02-04 Generalized inequalities",
    "chapter": "02",
    "order": 10,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "In one-dimensional real space MATH , given two numbers 1 and 2, we can say 2 is greater than 1. But in MATH -dimensional real space MATH , given two points MATH and MATH , can we say which is greater? This is not straightforward. This section introduces generalized inequalities for comparing order between two points in MATH , and also discusses the concepts of minimum and minimal elements of a set. Proper cone A convex cone MATH is called a proper cone if it satisfies: K is closed includes its boundary . K is solid its interior is nonempty . K is pointed does not contain any line i.e., MATH . If a pointed & closed convex cone is defined in a subspace of dimension MATH or less, its interior is empty, since it cannot contain an open ball in MATH dimensions. Thus, it is not solid and cannot be a proper cone. For example, a 2D pie-shaped pointed & closed convex cone in MATH is not a proper cone. See Wikipedia: Interior topology https://en.wikipedia.org/wiki/Interior topology for the definition of interior. Generalized inequality Using a proper cone, we can define a partial ordering in MATH called a generalized inequality . It has properties similar to the standard ordering in MATH : > MATH Similarly, strict partial ordering is defined as: > MATH int MATH If MATH , then MATH is the usual MATH in MATH . Properties of generalized inequalities Generalized inequality MATH satisfies: Preserved under addition : if MATH and MATH , then MATH . Transitive : if MATH and MATH then MATH . Preserved under nonnegative scaling : if MATH and MATH then MATH . Reflexive : MATH . Antisymmetric : if MATH and MATH , then MATH . Preserved under limits : if MATH for MATH and MATH as MATH , then MATH . Strict generalized inequalities have corresponding properties. Minimum and minimal elements The most significant difference between ordering in MATH and generalized ordering in MATH is linear ordering . In MATH , we can compare any two points with MATH or MATH , but generalized inequality cannot always do this. Therefore, defining the concepts of maximum and minimum in the context of generalized inequality is expected to be much more complex. Minimum elements If MATH satisfies MATH for all MATH , then MATH is a minimum element of set MATH . Similarly, maximum can be defined in the same manner. If a minimum exists in a set, it is unique. That is, there exists only one minimum. If a point MATH is the minimum of MATH , then MATH . Here, MATH means that according to MATH all points are comparable with MATH and are greater than or equal to MATH . Minimal elements A similar concept is minimal . If MATH and for all MATH , the condition MATH holds only when MATH , then MATH is a minimal element of set MATH . Similarly, maximal can be defined in the same manner. A set can have multiple minimal elements. If a point MATH is minimal in MATH , then MATH . Here, MATH means that according to MATH all points are comparable with MATH and are less than or equal to MATH . In the case of MATH , minimal and minimum are the same and correspond to the general definition of minimum. Minimum and minimal in MATH cone Consider the MATH cone MATH . The inequality MATH means that MATH is to the upper right of MATH . When MATH , saying that MATH is the minimum means that all points in MATH are to the upper right of MATH . Saying that MATH is minimal means that there are no points in MATH to the lower left of MATH . In the figure below, MATH has a minimum MATH . The set MATH is shown in light gray, and since MATH , MATH is the minimum. MATH has a minimal element MATH . The set MATH is shown in light gray, and since MATH , MATH is minimal. Fig1 Minimum and minimal elements 1",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_04_Generalized_inequalities/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_05_Separating_and_supporting_hyperplanes",
    "title": "02-05 Separating and supporting hyperplanes",
    "chapter": "02",
    "order": 11,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "This section introduces two theorems that characterize convex sets: the separating hyperplane theorem and the supporting hyperplane theorem . Separating hyperplane theorem Suppose there are several disjoint convex sets with no intersection. How can we separate them? The simplest way is to draw a line between the sets. This method is widely used in classification and is supported by the separating hyperplane theorem . If there are two disjoint convex sets MATH and MATH , then for MATH , MATH and for MATH , MATH for some MATH and MATH . In other words, the affine function MATH is nonpositive on MATH and nonnegative on MATH . The hyperplane MATH is called a separating hyperplane for MATH and MATH . The figure below shows a separating hyperplane dividing two disjoint convex sets MATH and MATH . Fig1 Separating hyperplane theorem 1 The converse of the separating hyperplane theorem does not hold. That is, the existence of a separating hyperplane does not guarantee that the sets are disjoint. For example, if MATH , then MATH separates MATH and MATH . Strict separation If the separating hyperplane satisfies the stronger condition MATH implies MATH , this is called strict separation . Disjoint closed convex sets do not always require strict separation, but in many cases, this condition holds. Supporting hyperplanes theorem The supporting hyperplane theorem states that for any nonempty convex set MATH and any point MATH on the boundary bd MATH , there exists a supporting hyperplane at MATH . What is a supporting hyperplane? Suppose MATH is a boundary point of MATH . If for all MATH , MATH MATH , then the hyperplane MATH is a supporting hyperplane for MATH at MATH . Note The boundary is defined as MATH bd MATH cl MATH MATH int MATH , i.e., the closure minus the interior. Geometrically, the supporting hyperplane MATH is tangent to MATH at MATH and the halfspace MATH contains MATH . Fig 2 Supporting hyperplane 1",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_05_Separating_and_supporting_hyperplanes/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_06_Dual_cones_and_generalized_inequalities",
    "title": "02-06 Dual cones and generalized inequalities",
    "chapter": "02",
    "order": 12,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "This section discusses dual cones paired with cones, and dual generalized inequalities. Using dual generalized inequalities allows comparison using scalar inner products, making comparisons much easier.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_06_Dual_cones_and_generalized_inequalities/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_06_01_Dual_cones",
    "title": "02-06-01 Dual cones",
    "chapter": "02",
    "order": 13,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Dual cones A dual cone is defined as a pair with a cone, denoted as MATH for the cone and MATH for the dual cone. The dual cone is always convex, regardless of whether MATH is convex. The dual cone is defined as the set of points MATH such that the inner product with any MATH in MATH is nonnegative: > MATH For MATH and MATH , the inner product being nonnegative means the angle between the vectors is in the range where MATH , i.e., MATH and MATH . Thus, the boundary of the dual cone is formed in the direction of the negative normal vector of the supporting hyperplane of the cone. The figure below shows the region where the dual cone is defined. In summary, the region of the dual cone is the set of all directions of the negative normal vectors of supporting hyperplanes of the cone at the origin. Fig1 Dual cone definition region Geometrically, if MATH , then MATH is the normal of the supporting hyperplane of MATH at the origin. The next figure shows that on the left, the halfspace with inward normal MATH contains the cone MATH , so MATH . On the right, the halfspace with inward normal MATH does not contain MATH , so MATH . Fig2 Dual cone and supporting hyperplane normal 1 Dual cones examples Here are some examples of cones and their duals. The first three are self-dual , meaning the cone and its dual are the same. The last example shows that the dual of the MATH cone is the MATH cone, and vice versa. MATH MATH MATH MATH MATH cone is self-dual The figure below shows that the MATH cone is self-dual. That is, for MATH on the boundary, the normal MATH of the supporting hyperplane at MATH matches the boundary of MATH , and MATH is the boundary of the dual cone MATH , so MATH and MATH coincide. MATH The dual cone of MATH cone is MATH cone The figure below shows that the dual cone of the MATH cone is the MATH cone. That is, when MATH is a boundary point, the normal MATH of the supporting hyperplane at MATH enters the interior of MATH and coincides with the boundary of the MATH cone. MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_06_01_Dual_cones/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter02/02_06_02_Dual_generalized_inequalities",
    "title": "02-06-02 Dual generalized inequalities",
    "chapter": "02",
    "order": 14,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "If we can define a generalized inequality using a proper cone, we can also define a dual generalized inequality using the dual cone, provided the dual cone is proper. This section defines dual generalized inequalities using a proper dual cone and redefines minimum and minimal elements using dual inequalities. Dual generalized inequalities Given a proper dual cone, the generalized inequality is defined as follows. For a point MATH , if the inner product with every MATH in MATH is nonnegative, then MATH is nonnegative in the dual cone MATH . Here, MATH is called the dual of MATH , i.e., the dual generalized inequality . > MATH for all MATH Key properties of generalized and dual inequalities MATH if and only if MATH for all MATH . MATH if and only if MATH \\lambda^T x Fig1 Minimum element 1 Minimal element The necessary and sufficient conditions for minimal elements differ slightly. For MATH and MATH , if MATH is a minimizer of MATH , then MATH is minimal. In other words, if MATH is minimal, then MATH does not have a unique minimizer. Therefore, for the same MATH , there can be multiple minimal elements, and there can be multiple minimal elements for different MATH . The figure below illustrates the existence of multiple minimal elements. The region with the thick black line at the bottom left indicates the area where minimal elements exist. Here, MATH is the minimizer of MATH and since MATH , it is minimal. Another minimizer, MATH , also exists. Fig2 Minimal element 1 However, the converse is not true. Even if a point MATH is minimal in a set MATH , it may not be the minimizer of MATH for some MATH and MATH . The figure below shows an example of a minimal element that is not a minimizer. Here, the convexity of the set seems to play a crucial role in this converse not being true. Fig3 An example of a minimal element that is not a minimizer 1 This converse theorem does not strengthen with MATH . In the left figure below, MATH is minimal, but not the minimizer of MATH . The right figure shows that MATH is not minimal, but is the minimizer of MATH . MATH Optimal production frontier Consider a product that needs to be produced using n resources labor, electricity, natural gas, water, etc. . This product can be produced in several ways. For each production method, there is a resource vector MATH , where MATH denotes the amount of resource MATH consumed. It is assumed that the resource consumption MATH is nonnegative, and the resources have high value. The production set MATH is defined as the set of all resource vectors MATH . A production method with a minimal resource vector is called Pareto optimal or efficient . The set of minimal elements of MATH is called the efficient production frontier . Let's briefly look at Pareto optimality. Suppose there are two production methods, one with resource vector MATH MATH and the other with resource vector MATH MATH . If for all MATH , MATH , and for some MATH , MATH , then we can say that MATH is better than MATH . In other words, a method that does not use more resources than another, or uses at least one resource less, is considered better. That is, this corresponds to the case where MATH and MATH . If there is no method better than MATH , then MATH is said to be Pareto optimal. By minimizing the following expression, we can find the Pareto optimal production method. Here, MATH can be considered as the price of resource MATH . Minimizing MATH with respect to MATH yields the cheapest production method. Since the prices are positive, the result of the minimization is always Pareto optimal. > MATH MATH The figure below illustrates this situation well. In the figure, MATH are Pareto optimal, while MATH are not. Fig5 Optimal production frontier 1",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter02/02_06_02_Dual_generalized_inequalities/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_00_Convex_functions",
    "title": "03 Convex functions",
    "chapter": "03",
    "order": 1,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this chapter, we will study the definition, examples, key properties of convex functions, and operations that preserve convexity.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_00_Convex_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_01_00_Basic_properties_and_examples",
    "title": "03-01 Basic properties and examples",
    "chapter": "03",
    "order": 2,
    "owner": "Minjoo Lee",
    "lesson_type": "required",
    "content": "This section covers the definition of convex functions, representative types of convex functions, and their key properties.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_01_00_Basic_properties_and_examples/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_01_01_convex_functions_definition",
    "title": "03-01-01 Definition",
    "chapter": "03",
    "order": 3,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Convex function A function MATH is convex if its domain is a convex set and for any two points MATH , the function satisfies: > MATH , > > with MATH , for all MATH This means that for any two points MATH , the value of MATH at their convex combination is less than or equal to the convex combination of their function values. Geometrically, the graph of MATH lies below the line segment connecting MATH and MATH . Fig1 Convex Function 2 Strictly convex A function MATH is strictly convex if for any two distinct points MATH and MATH f \\theta x+ 1-\\theta y > with MATH , is convex. Note strongly convex ⇒ strictly convex ⇒ convex Concave function A function MATH is concave if MATH is convex. All affine functions MATH satisfy: > MATH \\begin aligned f \\theta x+ 1-\\theta y &= a^T \\theta x+ 1-\\theta y +b \\\\ &= \\theta a^T x + 1-\\theta a^T y + \\theta b + 1-\\theta b \\\\ &= \\theta f x + 1-\\theta f y \\\\ \\end aligned MATH > MATH That is, affine functions are always convex, and simultaneously concave.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_01_01_convex_functions_definition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_01_02_examples_of_convex_function",
    "title": "03-01-02 Examples of convex functions",
    "chapter": "03",
    "order": 4,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "This section reviews representative examples of convex functions, including: Exponential function Power function Affine Quadratic Least squares loss Norm Indicator function Support function Max function Univariate function Exponential function: For any real number MATH , MATH is convex. > MATH is convex for any MATH Power function: For MATH , depending on the range of MATH , MATH can be convex or concave. > MATH is convex on MATH for any MATH or MATH > MATH is concave on MATH for any MATH Affine function As covered in 03-01-01 % multilang post url contents/chapter03/21-02-12-03 01 01 convex functions definition % , all affine functions are both convex and concave. On MATH and MATH : > MATH is convex and concave On MATH : > MATH is convex and concave Quadratic function Consider the quadratic function MATH , where MATH and MATH . If MATH is positive semidefinite, then MATH is convex. For MATH : > MATH is convex with MATH Q: Why is MATH convex if MATH is positive semidefinite? A: In a quadratic function, the second derivative is the Hessian matrix. The Hessian determines the curvature of the function, and if it is positive semidefinite, the function curves upwards. That is, the curvature in the direction of the Hessian's eigenvectors is nonnegative. Thus, if the second derivative is positive semidefinite, the function is convex. Least squares loss For any matrix MATH , MATH is always positive semidefinite, which means that MATH is always convex. > MATH is convex for any MATH Norm All norms on MATH are convex. Let MATH be a norm. By definition, > MATH \\begin aligned f \\theta x+ 1−\\theta y \\le \\theta f x + 1−\\theta f y , \\text with \\theta \\le \\theta \\le 1, \\text for all x,y \\in \\text dom f, \\end aligned MATH > MATH \\begin aligned \\|x\\| p = \\left \\sum i=1 ^ n x i^p\\right ^ 1/p \\text for p \\geq 1, \\|x\\| = \\max i=1,.., n |x i|\\\\ \\end aligned MATH Indicator function For a given set MATH , if the indicator function is defined as infinity MATH for elements not in MATH and as zero for elements in MATH , then the indicator function is convex. In other words, by defining the function to be infinitely large outside the set MATH and zero within it, the convexity property is preserved. > MATH I C x = \\begin cases 0, & x \\in C\\\\ \\infty, & x \\notin C\\\\ \\end cases MATH Support function Consider a set MATH . Regardless of whether MATH is convex, the support function of MATH is convex. > MATH = MATH is convex For more on the definition of the support function, refer to the Wikipedia definition https://en.wikipedia.org/wiki/Support function . Max function The max function of a finite collection of convex functions is convex. In other words, the upper envelope formed by connecting the maxima of a set of convex functions is convex. > MATH is convex",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_01_02_examples_of_convex_function/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_01_03_key_properties_of_convex_functions",
    "title": "03-01-03 Key properties of convex functions",
    "chapter": "03",
    "order": 5,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Epigraph characterization As discussed in Section 1.2, MATH is convex if and only if its epigraph is a convex set, and vice versa. > MATH is convex MATH is a convex set Convex sublevel sets If a function MATH is convex, its sublevel sets are also convex. > MATH , for all MATH Note Sublevel set For a function MATH , MATH is called the MATH -sublevel set . First-order characterization If a function MATH is differentiable , the following holds: If the domain MATH is convex and for all MATH , MATH , then MATH is convex, and vice versa. > MATH is convex MATH is convex, and MATH for all MATH The figure below illustrates the first-order condition for a differentiable convex function MATH . Fig1 Convex Function 1 This condition is known as the tangent line condition or tangent hyperplane condition in higher dimensions . It essentially states that a convex function always lies above or on all of its tangent lines or hyperplanes . No matter where you draw a tangent to a convex function, the function's actual values will never dip below that tangent. Example Consider the simple convex function MATH . Its derivative which is its gradient in 1D is MATH . Let's pick an arbitrary point MATH . The equation of the tangent line to MATH at MATH is given by: MATH Substituting MATH and MATH : MATH The convexity condition requires that for any MATH : MATH MATH Let's simplify the right side: MATH MATH Now, move all terms to one side: MATH This expression is a perfect square: MATH This inequality is always true for any real numbers MATH and MATH , because the square of any real number is always non-negative. This confirms that MATH satisfies the tangent line condition and is indeed a convex function. Second-order characterization If a function MATH is twice differentiable, it has the following property: - If the second derivative MATH is positive semidefinite for all MATH and MATH is convex, then MATH is convex, and vice versa. > MATH is convex MATH for all MATH : convex - If the second derivative MATH is positive definite for all MATH , then MATH is strictly convex. > if MATH for all MATH , then MATH is strictly convex - In other words, the curvature is always nonnegative. Jensen's inequality Let MATH be a convex function and MATH be positive weights such that MATH . Then, the following inequality holds: MATH If a function MATH is convex, it satisfies the following inequality: > MATH > Extension : > MATH is a random variable supported on MATH , then MATH Fig2 Jensen's Inequality 2 Isn't Jensen's inequality exactly the definition of a convex function? The answer is no—Jensen's inequality is a consequence and generalization of the convexity definition, not the definition itself. - This extends the two-point case MATH in the definition to any finite number of points and can be further generalized to integrals for probability measures . - Why It's a Generalization: The basic definition is for two points binary convex combination . Jensen applies it iteratively to more points. For instance: - For MATH , Jensen reduces exactly to the definition. - For MATH , you can apply the definition recursively: First combine two points, then with the third. --- Short Example Let's use the convex function MATH . Consider two numbers: MATH and MATH . We want to compare MATH with MATH . 1. Calculate the function of the average: The average of MATH and MATH is MATH . Applying the function: MATH . 2. Calculate the average of the function values: MATH . MATH . The average of these function values is MATH . Comparing the two results: MATH . This demonstrates Jensen's inequality: MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_01_03_key_properties_of_convex_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_02_operations_that_preserve_convexity",
    "title": "03-02 Operations that preserve convexity",
    "chapter": "03",
    "order": 6,
    "owner": "Minjoo Lee",
    "lesson_type": "required",
    "content": "This section discusses operations that preserve the convexity of convex functions. - Nonnegative weighted sum - Composition with affine functions - Pointwise maximum - Perspective function - Linear-fractional function Nonnegative weighted sum Convex functions have the following properties with respect to scalar multiplication and addition: • When a convex function MATH exists, multiplying it by any nonnegative number still results in a convex function MATH . > MATH is convex MATH is convex for MATH • When two convex functions MATH exist, their sum is also convex. > MATH are convex MATH is convex • A nonnegative weighted sum of convex functions MATH is convex. > MATH are convex MATH is convex, MATH Composition 1. Affine composition If function MATH is convex, then MATH is also convex. > MATH is convex MATH is convex 2. General composition Suppose we have function MATH that maps from MATH -dimensional to 1-dimensional space and function MATH that maps from 1-dimensional to 1-dimensional space. The composition function MATH is convex or concave in the following cases: > composition of MATH and MATH : > MATH • If MATH is convex, MATH is convex, and MATH is nondecreasing, then MATH is convex. • If MATH is concave, MATH is convex, and MATH is nonincreasing, then MATH is convex. • If MATH is concave, MATH is concave, and MATH is nondecreasing, then MATH is concave. • If MATH is convex, MATH is concave, and MATH is nonincreasing, then MATH is concave. Note The monotonicity of the extended-value extension MATH must be preserved. Example • If MATH is convex, then MATH is convex. • If MATH is concave and positive, then MATH is convex. 3. Vector composition Suppose we have function MATH that maps from MATH -dimensional to MATH -dimensional space and function MATH that maps from MATH -dimensional to 1-dimensional space. Then the composition function MATH is convex or concave in the following cases: >composition of MATH and MATH : > MATH • If MATH is convex and MATH is convex, and MATH is nondecreasing in each argument, then MATH is convex. • If MATH is convex and MATH is concave, and MATH is nonincreasing in each argument, then MATH is concave. Example • If MATH are concave and positive, then MATH is concave. • If MATH are convex, then MATH is convex. Pointwise maximum The pointwise maximum of functions is defined as follows and is convex: 1. Pointwise maximum > MATH are convex functions MATH , MATH is convex 2. Pointwise supremum If MATH is convex in MATH for each MATH , then MATH is convex. > MATH is convex in MATH for each MATH > MATH with MATH If function MATH is convex, then MATH is also convex. Example • When MATH is positive, if MATH is convex, then MATH is convex. • Negative logarithm When relative entropy MATH is convex on MATH , then MATH is convex. • If MATH is convex, then MATH is convex under the following condition: > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_02_operations_that_preserve_convexity/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_03_the_conjugate_function",
    "title": "03-03 The conjugate function",
    "chapter": "03",
    "order": 7,
    "owner": "Minjoo Lee",
    "lesson_type": "required",
    "content": "This section introduces the conjugate function also known as the convex conjugate or Fenchel conjugate , a fundamental concept in convex analysis and duality theory that provides a powerful tool for transforming optimization problems. Definition and Mathematical Foundation For a function MATH , the conjugate function MATH is defined as: MATH where MATH denotes the supremum least upper bound over all MATH in the domain of MATH . Geometric Interpretation The conjugate function has a beautiful geometric interpretation: - MATH represents the maximum gap between the linear function MATH and the original function MATH - Geometrically, it measures how much the hyperplane with slope MATH can be \"lifted above\" the graph of MATH - The conjugate transforms the function from the \"primal space\" to the \"dual space\" of slopes Why is it Important? The conjugate function is used to: 1. Transform optimization problems into their corresponding dual problems 2. Provide analytical tools for duality theory covered in Chapter 11 3. Enable direct substitution in Lagrange Duality without explicit differentiation 4. Establish connections between primal and dual optimal solutions Fig1 Conjugate function 2 Fundamental Properties The conjugate function has several remarkable properties that make it a powerful analytical tool: 1. Convexity Property - MATH is always convex , regardless of whether MATH is convex or not - This is because MATH is the pointwise supremum of affine functions MATH - The supremum of any collection of convex affine functions is convex 2. Fenchel's Inequality For any MATH and MATH : MATH This fundamental inequality establishes a lower bound relationship between a function and its conjugate. 3. Conjugate of Conjugate Biconjugate - In general: MATH the biconjugate is a lower bound - If MATH is closed and convex : MATH perfect recovery - This property is crucial for duality theory 4. Subdifferential Relationship If MATH is closed and convex, then for any MATH : MATH This establishes a beautiful symmetry between primal and dual spaces. Detailed Examples Example 1: Negative Logarithm Consider MATH for MATH . Step-by-step calculation: MATH To find the supremum, we differentiate with respect to MATH : MATH This gives us MATH valid only when MATH : The supremum is MATH Result: MATH f^ y = \\begin cases 0, & \\text if \\lvert y \\rvert \\leq 1 \\\\ +\\infty, & \\text if \\lvert y \\rvert > 1 \\end cases MATH This is the indicator function of the interval MATH . Example 4: Exponential Function Consider MATH for MATH . MATH Setting the derivative to zero: MATH , so MATH valid for MATH . Result: MATH f^ y = \\begin cases y \\log y - y, & \\text if y > 0 \\\\ 0, & \\text if y = 0 \\\\ +\\infty, & \\text if y 🎯 Launch Interactive Conjugate Function Explorer The interactive tool allows you to: - Visualize different function types and their conjugates side-by-side - Adjust parameters to see how they affect the conjugate - Explore tangent lines to understand the geometric interpretation - Compare multiple examples with detailed mathematical explanations Summary and Key Takeaways The conjugate function is a powerful mathematical tool that: 1. Transforms functions from primal to dual space through the operation MATH 2. Always produces convex functions , regardless of the original function's convexity 3. Establishes fundamental inequalities like Fenchel's inequality: MATH 4. Enables duality theory by connecting primal and dual optimization problems 5. Provides analytical tools for solving complex optimization problems Understanding conjugate functions is essential for: - Convex optimization theory and algorithm development - Lagrange duality and dual problem formulation - Modern optimization methods like proximal algorithms - Variational analysis and mathematical economics The geometric intuition of \"maximum gap between linear and nonlinear functions\" provides a visual understanding that complements the analytical definition, making this abstract concept more accessible to learners.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_03_the_conjugate_function/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_04_00_Quasiconvex_functions",
    "title": "03-04 Quasiconvex functions",
    "chapter": "03",
    "order": 8,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "This section introduces quasiconvex functions, their definitions, examples, and basic properties.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_04_00_Quasiconvex_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_04_01_quasiconvex_functions_definition_and_examples",
    "title": "03-04-01 Quasiconvex functions: definition and examples",
    "chapter": "03",
    "order": 9,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "A function MATH is quasiconvex if all its sublevel sets MATH are convex for every MATH . Quasiconvex functions generalize convex functions and appear frequently in optimization problems. Definition A function MATH is called quasiconvex or unimodal if its domain MATH and all sublevel sets MATH see 03-01-03 % multilang post url contents/chapter03/21-02-12-03 01 03 key properties of convex functions % are convex. > MATH is quasiconvex if MATH and > MATH for MATH are convex. If function MATH is quasiconvex, then MATH is called quasiconcave . > MATH is quasiconcave if MATH and > MATH for MATH are convex. When MATH is both quasiconvex and quasiconcave, it is called quasilinear , and the function's domain and all level sets MATH are convex. The following figure shows an example of a quasiconvex function. Fig1 quasiconvex function on R 1 For MATH , the MATH -sublevel set MATH is convex, namely the interval MATH . The MATH -sublevel set MATH is the interval MATH . Convex functions have convex sublevel sets and are quasiconvex, but the converse is not true. > MATH : convex MATH MATH : quasiconvex Examples Let's examine various examples of quasiconvex functions. Logarithm MATH on MATH is quasiconvex. It is also quasiconcave, so it has the property of being quasilinear. > MATH on MATH Ceiling function The ceiling function is quasiconvex and also quasiconcave . > MATH Length of vector If we define the length of MATH as the largest index of nonzero components, > MATH This satisfies > MATH for MATH on MATH which defines a subspace, so it is quasiconvex. Note: A subspace is closed under addition and scalar multiplication. Any subspace of MATH is also a convex set. Linear-fractional function Under the following conditions, function MATH is both quasiconvex and quasiconcave, i.e., quasilinear. > MATH with MATH Distance ratio function For MATH , when function MATH is defined as follows, representing the ratio of Euclidean distances from MATH to MATH and from MATH to MATH , MATH is quasiconvex on the halfspace MATH . > MATH Under the condition MATH , this becomes a convex set in the form of a Euclidean ball, so MATH is quasiconvex.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_04_01_quasiconvex_functions_definition_and_examples/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_04_02_basic_properties_of_quasiconvex_functions",
    "title": "03-04-02 Basic properties of quasiconvex functions",
    "chapter": "03",
    "order": 10,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "This section covers the basic properties of quasiconvex functions, including their relationship to convex functions and their behavior under various operations. Modified Jensen's inequality Quasiconvex functions can be defined through Jensen's inequality as follows: > MATH for all MATH The figure below shows that if function MATH is quasiconvex, then the value of MATH along the line segment between two points does not exceed the maximum of MATH at the endpoints. Fig1 quasiconvex function on MATH . The values of MATH between MATH and MATH are less than MATH . Quasiconvex function on MATH A continuous function MATH is quasiconvex if and only if it satisfies at least one of the following conditions: • MATH is nondecreasing • MATH is nonincreasing • There exists a point MATH such that MATH is nonincreasing on MATH and nondecreasing on MATH . Fig2 quasiconvex function on MATH . It is nonincreasing for MATH where MATH , and nondecreasing for MATH where MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_04_02_basic_properties_of_quasiconvex_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_04_03_differentiable_quasiconvex_functions",
    "title": "03-04-03 Differentiable quasiconvex functions",
    "chapter": "03",
    "order": 11,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "This section discusses the properties and characterizations of differentiable quasiconvex functions. First-order conditions Let MATH be a differentiable function. If MATH is convex and the following condition is satisfied, then MATH is quasiconvex. > MATH is quasiconvex MATH MATH for all MATH Fig1 Three level curves of a quasiconvex function MATH . MATH is the normal vector that defines the supporting hyperplane of the sublevel set MATH at MATH . The first-order condition for quasiconvexity appears similar to the first-order characterization of convexity see 03-01-03 % multilang post url contents/chapter03/21-02-12-03 01 03 key properties of convex functions % , but there are important differences. For example, if MATH is convex and MATH , then MATH is a global minimizer of MATH , but this does not always hold for quasiconvex functions. Second-order conditions When MATH is twice differentiable, second-order conditions apply. If MATH is quasiconvex, then for all MATH and all MATH , the following holds: > MATH is quasiconvex, MATH for all MATH , all MATH For quasiconvex functions on MATH : > MATH is quasiconvex, MATH That is, if there exists any point with zero slope, the second derivative value is non-negative. Returning to MATH , the second-order condition also satisfies the following properties: 1 When MATH , we must always have MATH . 2 If MATH , then MATH , where MATH acts as the Hessian matrix and is positive semidefinite on the MATH -dimensional subspace MATH . The MATH -dimensional subspace MATH means the MATH -dimensional subspace orthogonal to MATH . It is MATH -dimensional because MATH is the gradient of an MATH -dimensional function MATH , reducing the dimension by one.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_04_03_differentiable_quasiconvex_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_04_04_operations_that_preserve_quasiconvexity",
    "title": "03-04-04 Operations that preserve quasiconvexity",
    "chapter": "03",
    "order": 12,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "This section discusses operations that preserve the quasiconvexity of functions. Nonnegative weighted maximum When MATH is a quasiconvex function, the nonnegative weighted maximum MATH is quasiconvex. > MATH with MATH is quasiconvex This concept can be extended as follows: > MATH with MATH , >where MATH is quasiconvex in MATH for each MATH . Composition If MATH is quasiconvex and MATH is nondecreasing, then the composition MATH satisfies quasiconvexity. > MATH is quasiconvex if MATH is nondecreasing and MATH is quasiconvex. Composing a quasiconvex function with affine or linear-fractional transformations results in a quasiconvex function. If MATH is quasiconvex, then MATH is also quasiconvex, and MATH is also quasiconvex on the set MATH . Minimization If MATH satisfies quasiconvexity and MATH is a convex set, then the following condition holds: > MATH is quasiconvex if MATH is quasiconvex in MATH and MATH is a convex set. Representation via family of convex functions The sublevel sets of a quasiconvex function MATH can be represented by inequalities of convex functions. A family of convex functions is MATH for MATH , defined as follows: > MATH That is, the MATH -sublevel set of quasiconvex function MATH becomes the 0-sublevel set of convex function MATH . Here, MATH represents the index of convex function MATH . For all MATH , the following is satisfied: > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_04_04_operations_that_preserve_quasiconvexity/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_05_log_concave_and_log_convex_functions",
    "title": "03-05 Log-concave and log-convex functions",
    "chapter": "03",
    "order": 13,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "This section introduces log-concave and log-convex functions, which are important in probability, statistics, and optimization. Definition The definitions of log-concave and log-convex functions are as follows. MATH is Logarithmically concave or log-concave If MATH for all MATH and MATH is concave, then MATH is called logarithmically concave or log-concave. > MATH is log-concave for MATH for all MATH : > MATH for MATH . MATH is Logarithmically convex or log-convex If MATH for all MATH and MATH is convex, then MATH is called logarithmically convex or log-convex. Therefore, if MATH is log-convex, then MATH is log-concave. > MATH is log-convex for MATH for all MATH MATH is log-concave. It is sometimes convenient to allow MATH values to be 0, in which case MATH . In such cases, if the extended-value function MATH is concave, then MATH can be called log-concave. Log-convex functions and log-concave functions are quasiconvex and quasiconcave, respectively, because the logarithm is monotonically increasing. Examples Affine function If MATH is defined as follows, then it is log-concave. > MATH on MATH Powers MATH on MATH is log-convex when MATH and log-concave when MATH . Exponentials MATH is both log-convex and log-concave. The cumulative distribution function of a Gaussian density MATH is log-concave. Gamma function MATH is log-convex for MATH . Determinant MATH is log-concave on MATH . Determinant over trace MATH is log-concave on MATH . Properties Twice differentiable log-convex / concave functions If MATH is twice differentiable and MATH is convex, then the following equation holds: > MATH MATH is log-convex MATH MATH for all MATH , and MATH is log-concave MATH MATH for all MATH . Multiplication Log-convexity and log-concavity are closed under multiplication and positive scaling. If MATH and MATH are log-concave, then the pointwise product MATH is also log-concave. This is because MATH , and both MATH and MATH are concave functions. Addition and Integration In general, the sum of log-concave functions is not log-concave. However, log-convexity is preserved under addition. For example, let MATH and MATH be log-convex functions, i.e., MATH and MATH are convex. By the composition rules for convex functions, the following holds: > MATH This is convex. The left side is convex because: 1. log-convex functions are convex, 2. applying the exponential function to convex functions preserves convexity, 3. the sum of convex functions is convex, and 4. the logarithm of convex functions is convex. Therefore, the entire result is convex. In conclusion, the sum of two log-convex functions is log-convex. Generalizing this, if MATH is log-convex for each MATH , then MATH is log-convex. > MATH Integration of log-concave functions In certain cases, log-concavity is also preserved under integration. If MATH is log-concave, then MATH is a log-concave function for MATH . > MATH is log-concave MATH MATH is log-concave , MATH for each MATH . Based on this, we can confirm that the marginal distribution of a log-concave probability density is log-concave. Log-concavity is also closed under convolution operations. If MATH and MATH are log-concave on MATH , then their convolution is also log-concave. > MATH , MATH are log-concave on MATH is log-concave.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_05_log_concave_and_log_convex_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_06_convexity_with_respect_to_generalized_inequalities",
    "title": "03-06 Convexity with respect to generalized inequalities",
    "chapter": "03",
    "order": 14,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "This section discusses convexity with respect to generalized inequalities, extending the concept of convexity beyond standard real-valued functions. In spaces other than MATH , we use the definition of cones for generalized inequality expressions that extend the ordering concept commonly used in MATH space see 02-01-04 % multilang post url contents/chapter02/21-02-08-02 01 04 Convex cone % . In this section, we examine the concepts of monotonicity and convexity that extend beyond MATH space using the concept of cones. Monotonicity with respect to a generalized inequality Suppose MATH is a proper cone represented by MATH . A convex cone MATH is a proper cone if it satisfies the following conditions: • MATH is closed contains its boundary • MATH is solid has nonempty interior • MATH is pointed contains no line We define MATH -nondecreasing as follows: > MATH is MATH -nondecreasing if MATH Also, when the following condition is satisfied, we say it is MATH -increasing : > MATH is MATH -increasing if MATH for all MATH , then it is increasing. Similarly, monotonicity can be expressed as an extended concept in generalized inequalities. When the domain is convex, a differentiable function MATH being MATH -nondecreasing means satisfying the following equation. Note that unlike simple scalars, the gradient MATH must be nonnegative in the dual inequality. > A differentiable function MATH is MATH -nondecreasing MATH MATH for all MATH If the following condition is satisfied, MATH is called MATH -increasing . As with scalars, the converse does not hold. > MATH for all MATH MATH MATH is MATH -increasing. Convexity with respect to generalized inequality Let MATH be a proper cone associated with the generalized inequality MATH . Then, if MATH is called MATH -convex for all MATH and MATH , the following inequality holds: > MATH is MATH -convex MATH MATH with MATH f \\theta x + 1 - \\theta y \\prec K \\theta f x + 1 - \\theta f y MATH x \\neq y MATH 0 Differentiable MATH -convex functions If a differentiable function MATH is MATH -convex and the function domain is convex, then the following equation holds: > MATH for all MATH Here, MATH is the derivative or Jacobian matrix of MATH at point MATH . If MATH is strictly MATH -convex and the function domain is convex, then the following equation holds: > MATH for all MATH , MATH Composition theorem Many results from composition can be generalized to MATH -convexity. For example, if MATH is MATH -convex, MATH is convex, and the extended-value extension MATH of MATH is MATH -nondecreasing, then MATH is convex. This generalizes the fact that the composition of a convex function with a nondecreasing convex function is convex. The condition that MATH is MATH -nondecreasing means that MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_06_convexity_with_respect_to_generalized_inequalities/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter03/03_07_euclidean_norm_convexity_proof",
    "title": "03-01-04 Euclidean Norm - L2 - is Convex",
    "chapter": "03",
    "order": 15,
    "owner": "AI Assistant",
    "lesson_type": "",
    "content": "The Euclidean norm also called the MATH -norm of a vector MATH is defined as: MATH We will prove that this function is convex using three different methods: 1. Definition-based proof using Jensen's inequality 2. Second derivative test Hessian analysis 3. Triangle inequality approach Method 1: Definition-Based Proof Theorem : The Euclidean norm MATH is convex on MATH . Proof : To prove convexity, we need to show that for any MATH and MATH : MATH Let MATH . Then: MATH \\begin align \\|z\\| 2^2 &= \\|\\theta x + 1-\\theta y\\| 2^2 \\\\ &= \\theta x + 1-\\theta y ^T \\theta x + 1-\\theta y \\\\ &= \\theta^2 x^T x + 2\\theta 1-\\theta x^T y + 1-\\theta ^2 y^T y \\\\ &= \\theta^2 \\|x\\| 2^2 + 2\\theta 1-\\theta x^T y + 1-\\theta ^2 \\|y\\| 2^2 \\end align MATH By the Cauchy-Schwarz inequality : MATH Therefore: MATH The right-hand side can be factored as: MATH Taking square roots of both sides: MATH This proves that the Euclidean norm is convex. MATH Method 2: Second Derivative Test Hessian Analysis For twice-differentiable functions, we can use the second derivative test : a function is convex if its Hessian matrix is positive semidefinite. Analysis : The Euclidean norm MATH is not differentiable at MATH . However, for MATH , we can compute: MATH MATH The Hessian matrix is: MATH Verification of positive semidefiniteness : For any vector MATH : MATH By Cauchy-Schwarz inequality: MATH Therefore: MATH This shows MATH , confirming convexity for MATH . Method 3: Triangle Inequality Approach Alternative Proof using Minkowski Inequality : The Euclidean norm satisfies the triangle inequality : MATH For convexity, let MATH and MATH where MATH : MATH This directly establishes the convexity condition. Key Properties and Applications Properties of Euclidean Norm Convexity 1. Strict Convexity : The Euclidean norm is actually strictly convex on any line not passing through the origin. 2. Homogeneity : MATH for any scalar MATH . 3. Subadditivity : MATH Triangle inequality . Applications in Optimization The convexity of the Euclidean norm has important implications: 1. Least Squares Problems : The objective function MATH is convex. 2. Regularization : MATH -regularization terms like MATH preserve convexity. 3. Constrained Optimization : Norm constraints MATH define convex feasible sets. Conclusion We have proven that the Euclidean norm is convex using three different approaches: 1. Direct definition : Using Jensen's inequality and Cauchy-Schwarz 2. Second derivative test : Showing the Hessian is positive semidefinite 3. Triangle inequality : Leveraging the fundamental norm property This convexity property is fundamental in optimization theory and has wide-ranging applications in machine learning, signal processing, and numerical analysis. The interactive visualization above demonstrates how the convexity condition MATH holds for any choice of points and convex combination parameter MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter03/03_07_euclidean_norm_convexity_proof/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_00_Convex_optimization_basics",
    "title": "04 Convex optimization basics",
    "chapter": "04",
    "order": 1,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Convex Optimization Basics This chapter introduces the main properties of convex problems and several techniques commonly used in solving them.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_00_Convex_optimization_basics/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_01_Basic_terminology",
    "title": "04-01 Basic terminology",
    "chapter": "04",
    "order": 2,
    "owner": "YoungJae Choung",
    "lesson_type": "required",
    "content": "Convex Optimization Basics Let's review the basic terminology used in convex optimization problems. A convex optimization problem is defined as: > MATH \\begin aligned &\\text minimize x \\in D && f x \\\\ &\\text subject to && g i x \\leq 0, \\quad i = 1, \\dotsc, m \\\\ &&& h j x = 0, \\quad j = 1, \\dotsc, r ,\\\\\\\\ \\end aligned MATH >where MATH and MATH , MATH are all convex, > MATH are all affine, >and the optimization domain is MATH . MATH is called the criterion or objective function . MATH is called the inequality constraint function . MATH is called the equality constraint function . If MATH and MATH for all MATH and MATH for all MATH , then MATH is a feasible point . For all feasible points MATH , the minimum value of MATH is called the optimal value , denoted as MATH . If MATH is feasible and MATH , then MATH is called optimal , a solution , or a minimizer . If MATH is feasible and MATH , then MATH is called MATH -suboptimal . If MATH is feasible and MATH , then MATH is active at MATH . A convex minimization problem can be converted to a concave maximization problem. > MATH \\begin aligned &\\text maximize x \\in D &&-f x \\\\ &\\text subject to &&g i x \\leq 0, i = 1, .., m\\\\ &&&h j x = 0, j = 1, \\dotsc, r,\\\\\\\\ \\end aligned MATH >where MATH and MATH , MATH are all convex, > MATH are all affine, >and the optimization domain is MATH . Feasible Set A feasible set also known as a feasible region or solution space is the set of all feasible points. It represents all the choices that are allowed by the problem's conditions. The feasible set, often denoted by MATH or MATH , is defined as: > MATH Properties of Feasible Sets 1. Convexity: If all inequality constraint functions MATH are convex and all equality constraint functions MATH are affine linear , then the feasible set MATH is a convex set . This property is crucial in convex optimization, as it guarantees that any local optimum is also a global optimum. 2. Boundedness: A feasible set can be bounded enclosed within a finite region or unbounded extending infinitely in some direction . 3. Empty Set: It is possible for the feasible set to be empty MATH . This means there are no points that satisfy all the given constraints, and the optimization problem is said to be infeasible . 4. Polytope/Polyhedron Đa hình / Đa diện : In linear programming, if the feasible set is non-empty and bounded, it is called a polytope . If it is non-empty but potentially unbounded, it is called a polyhedron . Both are convex sets. 5. Vertices Extreme Points : For linear programming problems, if an optimal solution exists, it can always be found at one of the vertices also known as extreme points or corner points of the feasible set. This is the basis for algorithms like the Simplex method.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_01_Basic_terminology/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_02_Convex_solution_sets",
    "title": "04-02 Convex solution sets",
    "chapter": "04",
    "order": 3,
    "owner": "YoungJae Choung",
    "lesson_type": "required",
    "content": "Let's understand the properties of convex solution sets. Let MATH denote the set of solutions to a convex problem: > MATH \\begin aligned X opt = &\\text arg \\min x &&f x \\\\ &\\text subject to &&g i x \\leq 0, i = 1, .., m \\\\ &&&h j x = 0, j = 1, .., r \\\\\\\\ \\end aligned MATH Key property 1 > MATH is a convex set. Proof > If MATH and MATH are solutions: > 1. The domain set MATH is convex, so for MATH , MATH . > 2. MATH and MATH are convex and affine functions, so the following conditions hold: MATH \\begin aligned g i tx + 1-t y \\leq tg i x + 1-t g i y \\leq 0, \\\\ h j tx + 1-t y = th j x + 1-t h j y = 0 \\\\ \\end aligned MATH > 3. MATH is a convex function, so: > >\\begin aligned > f tx+ 1-t y &\\leq tf x + 1-t f y \\\\ > = tf^ \\star + 1-t f^ \\star \\\\ > = f^ \\star >\\end aligned >where MATH minimum value. >Thus, MATH is also a solution. Geometric interpretation In a convex function, any local optimum is also a global optimum. If the solution set contains multiple elements, it must look like the following: Fig1 geometric interpretation of convexity of the solution set Key property 2 >If MATH is strictly convex, then the solution is unique. That is, MATH contains only one element. MATH being strictly convex means that MATH always satisfies the following property: > MATH f tx + 1-t y > MATH That is, MATH is downward convex with no flat segments, and the solution of MATH is unique.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_02_Convex_solution_sets/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_03_Optimality_conditions",
    "title": "04-03 Optimality conditions",
    "chapter": "04",
    "order": 4,
    "owner": "YoungJae Choung",
    "lesson_type": "required",
    "content": "First order optimality condition For more background on convex functions, see Chapter 3: Key Properties of Convex Functions % multilang post url contents/chapter03/20-02-08-03 01 key properties of convex functions % . > MATH \\begin aligned &\\min x &&f x \\\\ &\\text subject to &&x \\in C \\end aligned MATH For a convex problem where the objective function MATH is differentiable , the following condition is necessary and sufficient for an optimal point MATH : > MATH \\nabla f x ^ T y-x \\geq 0 \\\\ > \\text for all y \\in C MATH This is called the first-order condition for optimality . MATH defines a hyperplane passing through MATH in set MATH , and MATH points in the direction of movement toward the optimal point MATH . If the above condition is satisfied, set MATH is contained in the half-space opposite to MATH , so MATH is an optimal point. Fig1 geometric interpretation of first-order condition for optimality 3 Important special case When MATH unconstrained optimization , the optimality condition is: > MATH In this case, MATH points toward the optimal point MATH , and MATH means there is no further direction to move to decrease MATH at MATH . --- Mathematical Foundation While first-order conditions use the gradient MATH , second-order conditions utilize the Hessian matrix : MATH H f x = \\nabla^2 f x = \\begin bmatrix \\frac \\partial^2 f \\partial x 1^2 & \\frac \\partial^2 f \\partial x 1 \\partial x 2 & \\cdots & \\frac \\partial^2 f \\partial x 1 \\partial x n \\\\ \\frac \\partial^2 f \\partial x 2 \\partial x 1 & \\frac \\partial^2 f \\partial x 2^2 & \\cdots & \\frac \\partial^2 f \\partial x 2 \\partial x n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac \\partial^2 f \\partial x n \\partial x 1 & \\frac \\partial^2 f \\partial x n \\partial x 2 & \\cdots & \\frac \\partial^2 f \\partial x n^2 \\end bmatrix MATH The Hessian captures the curvature of the function at point MATH , providing information about the local shape of the objective function. Second-Order Optimality Conditions Consider the unconstrained optimization problem: MATH where MATH is twice continuously differentiable. Necessary Conditions Second-Order If MATH is a local minimum of MATH , then: 1. First-order necessary condition : MATH 2. Second-order necessary condition : MATH positive semidefinite Sufficient Conditions Second-Order If at point MATH : 1. MATH first-order condition 2. MATH positive definite Then MATH is a strict local minimum of MATH . Understanding Positive Definiteness A symmetric matrix MATH is: - Positive definite MATH if MATH for all MATH - Positive semidefinite MATH if MATH for all MATH Practical tests for positive definiteness: 1. Eigenvalue test : All eigenvalues are positive 2. Leading principal minors : All leading principal minors are positive 3. Cholesky decomposition : MATH exists with MATH lower triangular Geometric Interpretation The second-order conditions provide information about the curvature at the critical point: - MATH : The function curves upward in all directions → strict local minimum - MATH : The function curves downward in all directions → strict local maximum - MATH : Non-negative curvature → possible minimum - Indefinite Hessian : Mixed curvature → saddle point Fig1 Geometric interpretation of second-order conditions Detailed Examples Example 1: Quadratic Function Consider MATH Step 1: Find critical points MATH Solving: MATH Step 2: Compute Hessian MATH Step 3: Check positive definiteness - Eigenvalues: MATH , MATH - Leading principal minors: MATH , MATH Conclusion : MATH → MATH is a strict local minimum. Example 2: Non-Convex Function Consider MATH Step 1: Find critical points MATH Critical points: MATH , MATH , MATH , MATH Step 2: Analyze MATH MATH Conclusion : MATH is a strict local maximum. Step 3: Analyze MATH MATH Conclusion : MATH is a strict local minimum. Example 3: Saddle Point Consider MATH Analysis at MATH : - MATH ✓ - MATH indefinite Conclusion : MATH is a saddle point neither minimum nor maximum . Constrained Optimization: Second-Order Conditions For constrained problems: MATH The bordered Hessian of the Lagrangian is used: MATH Second-order sufficient condition : The bordered Hessian has the correct inertia number of negative eigenvalues equals the number of constraints . Comparison: First vs Second-Order Conditions | Aspect | First-Order | Second-Order | |--------|-------------|--------------| | Information | Gradient slope | Hessian curvature | | Necessary condition | MATH | MATH and MATH | | Sufficient condition | Not available for unconstrained | MATH and MATH | | Strength | Weaker | Stronger | | Computational cost | MATH | MATH | | Distinguishes | Critical points | Minima, maxima, saddle points | Interactive Visualization Explore how second-order conditions work in practice: 🎯 Launch Second-Order Conditions Explorer The interactive tool demonstrates: - Hessian eigenvalue analysis for different function types - Visual classification of critical points minimum, maximum, saddle - Contour plots showing local curvature behavior - Step-by-step calculations for second-order tests Summary and Key Takeaways Second-order optimality conditions provide stronger characterization of optimal points: Key Results: 1. Necessary conditions : MATH and MATH 2. Sufficient conditions : MATH and MATH 3. Classification power : Can distinguish between minima, maxima, and saddle points Practical Importance: - Algorithm design : Foundation for Newton-type methods - Convexity analysis : Essential for verifying convex functions - Robustness : Stronger guarantees than first-order conditions alone - Optimization theory : Bridge between local and global optimality Computational Considerations: - Cost : MATH storage and computation for Hessian - Approximation : Quasi-Newton methods reduce computational burden - Numerical stability : Eigenvalue computations require careful implementation Understanding second-order conditions is essential for advanced optimization theory and the development of efficient numerical algorithms. They provide the mathematical foundation for many modern optimization methods and offer deeper insights into the structure of optimization problems.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_03_Optimality_conditions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_04_Partial_optimization",
    "title": "04-04 Partial optimization",
    "chapter": "04",
    "order": 5,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Reminder: % multilang post url contents/chapter03/21-02-12-03 02 operations that preserve convexity % If MATH is a convex set and MATH is convex in MATH , then MATH is convex in MATH . Thus, partial optimization in a convex problem constructed with multivariate functions preserves convexity. Fig1 partial optimization of a convex problem 3 Example: hinge form of SVMs For a non-separable set, the SVM problem is defined as: > MATH \\begin aligned &\\min \\beta, \\beta 0 , \\xi &&\\frac 1 2 \\|\\beta\\| 2^2 + C \\sum i=1 ^ n \\xi i \\\\ &\\text subject to && \\xi i \\ge 0, \\\\ &&&y i x i ^T \\beta + \\beta 0 \\ge 1 - \\xi i , \\\\ &&&i = 1, .., n \\\\ \\end aligned MATH The above constraints can be expressed as a single constraint: > MATH \\begin aligned \\xi i \\ge \\max\\ 0, 1 - y i x i ^T \\beta + \\beta 0 \\ \\\\ \\end aligned MATH In this case, MATH is the minimum value for MATH , and we can define MATH as: > MATH \\begin aligned \\frac 1 2 \\|\\beta\\| 2 ^ 2 + C \\sum i=1 ^ n \\xi i &\\ge \\frac 1 2 \\|\\beta\\| 2 ^ 2 + C \\sum i=1 ^ n \\max 0, 1 - y i x i ^T \\beta + \\beta 0 \\\\ &= \\min\\ \\frac 1 2 \\|\\beta\\| 2 ^ 2 + C \\sum i=1 ^ n \\xi i \\quad | \\quad \\xi i \\ge 0, \\ y i x i ^T \\beta + \\beta 0 \\ge 1 - \\xi i , \\ i = 1, .., n\\ \\\\ &= \\tilde f \\beta, \\beta 0 \\\\ \\end aligned MATH By using the simplified MATH as the objective function, a more straightforward solution can be obtained. In the given problem, MATH has been eliminated, and it has also been transformed from a constrained problem to an unconstrained problem. > MATH \\begin aligned \\min \\beta, \\beta 0 \\frac 1 2 \\|\\beta\\| 2^2 + C \\sum i=1 ^ n \\max\\ 0, 1 - y i x i ^ T \\beta + \\beta 0 \\ \\end aligned MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_04_Partial_optimization/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_05_Transformations_and_change_of_variables",
    "title": "04-05 Transformations and change of variables",
    "chapter": "04",
    "order": 6,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "This section discusses transformations and change of variables in convex optimization problems, which are useful for simplifying or reformulating problems. The objective function or constraint functions can be modified while preserving the given optimization problem, and sometimes this can be used to discover the \"hidden convexity\" of the problem. Theorem 1 When function MATH is a monotone increasing transformation, the following relationship holds: > MATH >\\begin align > &\\text min x f x \\text subject to x \\in C \\\\ > \\Longleftrightarrow \\quad &\\text min x h f x \\text subject to x \\in C >\\end align > MATH Theorem 2 If function MATH is a one-to-one correspondence function and the image of MATH covers the feasible set MATH , then the variables of the optimization problem can be changed as follows: > MATH >\\begin align > &\\min x f x \\text subject to x \\in C \\\\\\\\ > \\Longleftrightarrow \\quad &\\min y f \\phi y \\text subject to \\phi y \\in C >\\end align > MATH Example: geometric programming A function MATH of the following form is called a monomial : > MATH Also, the sum of monomials is called a posynomial : > MATH A geometric program is defined in the following form and is a non-convex problem: > MATH \\begin align &\\min x &&f x \\\\ &\\text subject to &&g i x \\leq 1, i = 1, \\dotsc, m\\\\ &&&h j x = 1, j = 1, \\dotsc, r,\\\\\\\\ \\end align \\\\ MATH >where MATH , MATH are posynomials and MATH are monomials. Let's prove that a geometric program is equivalent to some convex problem. Proof: >For MATH , if we let MATH and MATH , then MATH can be transformed as follows, and by Theorem 2 , this preserves the given optimization problem equivalently: > MATH > >Also, a posynomial can be represented as MATH . > >At this point, by Theorem 1 , the logarithmic form MATH can also preserve the optimization problem equivalently. > >That is, the geometric program is equivalent to the following problem, which is a convex problem: > > MATH >\\begin align &\\min x \\quad && log \\big \\sum k=1 ^ p 0 e^ a 0k ^ Ty + b 0k \\big \\\\ &\\text subject to && log \\big \\sum k=1 ^ p i e^ a ik ^ Ty + b ik \\big \\leq 0 , \\quad i = 1, \\dotsc, m \\\\ &&&c j ^ Ty + d j = 0, \\quad j = 1, \\dotsc, r\\\\\\\\ \\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_05_Transformations_and_change_of_variables/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_06_Eliminating_equality_constraints",
    "title": "04-06 Eliminating equality constraints",
    "chapter": "04",
    "order": 7,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "This section explains techniques for eliminating equality constraints in convex optimization problems to simplify the feasible set or problem structure. > MATH \\begin aligned &\\min x &&f x \\\\ &\\text subject to &&g i x \\leq 0, i = 1, .., m\\\\ &&& Ax = b .\\\\ \\end aligned MATH For an arbitrary solution MATH satisfying MATH and MATH , any MATH that satisfies the equality constraint can be expressed as follows: > MATH That is, MATH . Therefore, by substituting MATH for MATH in the given problem, we can eliminate the equality constraint. Thus, the following problem is equivalent to the original problem: > MATH \\begin aligned &\\min y &&f My+x 0 \\\\ &\\text subject to &&g i My+x 0 \\leq 0, i = 1, .., m.\\\\ \\end aligned MATH However, caution is advised when using this method for the following reasons: 1. The computation of MATH is generally very expensive. 2. If MATH is sparser than MATH , the cost of computation using MATH may be higher.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_06_Eliminating_equality_constraints/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_07_Slack_variables",
    "title": "04-07 Slack variables",
    "chapter": "04",
    "order": 8,
    "owner": "YoungJae Choung",
    "lesson_type": "required",
    "content": "Slack Variables in Convex Optimization Introduction and Motivation Slack variables are auxiliary variables introduced to transform inequality constraints into equality constraints. This transformation is fundamental in optimization theory and has practical applications in linear programming, interior-point methods, and many optimization algorithms. Why do we need slack variables? - Many optimization algorithms are designed to handle equality constraints more efficiently - They provide a geometric interpretation of how \"tight\" a constraint is - They are essential in the simplex method for linear programming - They help in duality theory and sensitivity analysis Mathematical Formulation Consider the standard convex optimization problem: > MATH \\begin aligned &\\min x &&f x \\\\ &\\text subject to &&g i x \\leq 0, \\quad i = 1, \\ldots, m\\\\ &&&Ax = b \\end aligned > MATH By introducing slack variables MATH for each inequality constraint, we can reformulate this as: > MATH \\begin aligned &\\min x, s &&f x \\\\ &\\text subject to &&s i \\geq 0, \\quad i = 1, \\ldots, m\\\\ &&&g i x + s i = 0, \\quad i = 1, \\ldots, m\\\\ &&&Ax = b \\end aligned MATH Geometric Interpretation The slack variable MATH represents the \"slack\" or \"margin\" in the MATH -th constraint: - MATH : The constraint MATH is inactive not binding - The point MATH is in the interior of the feasible region defined by constraint MATH - We have MATH \\begin aligned &\\max &&3x 1 + 2x 2 \\\\ &\\text subject to &&x 1 + x 2 \\leq 4 \\quad \\text labor hours \\\\ &&&2x 1 + x 2 \\leq 6 \\quad \\text material units \\\\ &&&x 1, x 2 \\geq 0 \\end aligned > MATH With Slack Variables: > MATH \\begin aligned &\\max &&3x 1 + 2x 2 \\\\ &\\text subject to &&x 1 + x 2 + s 1 = 4\\\\ &&&2x 1 + x 2 + s 2 = 6\\\\ &&&x 1, x 2, s 1, s 2 \\geq 0 \\end aligned > MATH Interpretation: - MATH : unused labor hours - MATH : unused material units - If MATH at optimum: all labor is utilized - If MATH at optimum: some material remains unused Important Properties and Considerations 1. Equivalence of Problems The original and slack variable formulations are mathematically equivalent : - Same optimal objective value - Optimal MATH is the same in both formulations - The slack variables MATH at optimum tell us which constraints are active 2. Convexity Preservation Critical Note: The transformation preserves convexity only when MATH are affine functions . - If MATH affine : The reformulated problem remains convex - If MATH is nonlinear : The equality constraint MATH may destroy convexity 3. Dimensionality Impact - Original problem: MATH variables - With slack variables: MATH variables - Trade-off: more variables but simpler constraint structure Example: Converting a Linear Program Original Form: > MATH \\begin aligned &\\min &&c^T x \\\\ &\\text subject to &&Ax \\leq b\\\\ &&&x \\geq 0 \\end aligned > MATH Standard Form with Slack Variables: > MATH \\begin aligned &\\min &&c^T x \\\\ &\\text subject to &&Ax + s = b\\\\ &&&x, s \\geq 0 \\end aligned > MATH Where MATH is the vector of slack variables. Key Takeaways 1. Transformation Tool : Slack variables convert inequalities to equalities 2. Geometric Meaning : They measure constraint \"tightness\" 3. Algorithm Enabler : Essential for many optimization algorithms 4. Convexity Condition : Preserve convexity only for affine constraints 5. Practical Insight : Provide economic interpretation in resource problems Understanding slack variables is crucial for: - Implementing optimization algorithms - Interpreting optimization results - Analyzing constraint sensitivity - Connecting theory with computational practice",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_07_Slack_variables/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter04/04_08_Relaxation",
    "title": "04-08 Relaxation",
    "chapter": "04",
    "order": 9,
    "owner": "YoungJae Choung",
    "lesson_type": "required",
    "content": "This section discusses relaxation techniques, which are used to simplify or approximate convex optimization problems by relaxing constraints. Consider a problem of the form: > MATH The process of changing the domain set MATH to a superset MATH is known as Relaxation . > MATH Since we are optimizing over a larger domain set than MATH , the optimal value of the relaxed problem is always less than or equal to that of the original problem. Important special case: relaxing non-affine equality constraints > MATH where MATH are convex but non-affine, >are replaced with MATH By transforming equality constraints into inequality constraints, the relaxation technique loosens the constraints and effectively enlarges the domain. When the given equality constraints are convex and non-affine, this method can be used to reformulate the problem as a convex optimization problem. However, this is under the condition that the same solution is still valid even after the relaxation.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter04/04_08_Relaxation/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_00_Canonical_Problems",
    "title": "05 Canonical Problems",
    "chapter": "05",
    "order": 1,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Canonical Problems In Chapter 1 /chapter01/2021/01/07/optimization problems/ , we learned that a convex optimization problem is defined as follows: Fig1 Convex Optimization Problem in standard form 3 The domain set is convex The objective function MATH and the inequality constraint function MATH are convex The equality constraint function MATH is affine Depending on the type of objective and constraint functions, optimization problems are classified into several categories. In this chapter, we will learn about six major subclasses: - Linear Programming LP - Quadratic Programming QP - Quadratically Constrained Quadratic Programming QCQP - Second-Order Cone Programming SOCP - Semidefinite Programming SDP - Conic Programming CP These problems have the following inclusion relationships and become more general as you move down the list: MATH Fig2 Canonical Problems",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_00_Canonical_Problems/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_01_Linear_Programming_(LP)",
    "title": "05-01 Linear Programming (LP)",
    "chapter": "05",
    "order": 2,
    "owner": "Hooncheol Shin",
    "lesson_type": "required",
    "content": "Linear Programming LP is one of the most fundamental and widely-used optimization techniques in mathematics, economics, and engineering. Imagine you're a factory manager trying to maximize profit while dealing with limited resources - this is exactly the type of problem LP was designed to solve! What makes a problem \"Linear\"? A problem is linear when: - The objective function what you want to optimize is a straight line relationship - All constraints limitations are also straight line relationships - No variables are multiplied together no MATH terms - No variables appear in exponents or under square roots Simple Example: The Farmer's Problem Let's start with an intuitive example. A farmer has 100 acres of land and wants to plant corn and wheat to maximize profit: - Corn : $300 profit per acre, requires 2 hours of labor per acre - Wheat : $200 profit per acre, requires 1 hour of labor per acre - Available labor : 150 hours total Question : How many acres of each crop should the farmer plant? Interactive Exercise: Use the sliders below to explore different combinations of corn and wheat. Watch how the profit changes and whether constraints are satisfied! Corn acres: 50 Wheat acres: 50 Total land used: 100 / 100 acres Total labor used: 150 / 150 hours Total profit: $ 25000 ✓ Feasible solution Now let's formalize this intuition. If both the objective function and constraint functions are affine, the optimization problem is called a linear program LP . The general linear program is formulated as: General LP > MATH \\begin align > &\\text minimize x && c^T x + d \\\\\\\\ > &\\text subject to && Gx \\preceq h \\\\\\\\ > & && Ax = b ,\\\\\\\\ > &\\text where &&G \\in \\mathbb R ^ m \\times n \\text and A \\in \\mathbb R ^ p \\times n . >\\end align MATH The constant MATH in the objective function does not affect the optimization process or result and can be omitted. If you want to maximize MATH under the same constraints, you can equivalently minimize MATH . The above problem seeks the minimizer MATH of the affine function MATH over a polyhedral feasible set. Geometric Interpretation The power of linear programming becomes clear when we visualize it geometrically. Each constraint defines a half-space, and the feasible region is the intersection of all these half-spaces - forming a polyhedron . Key Insight: The optimal solution of a linear program always occurs at a vertex corner point of the feasible region! This is why the simplex method works by moving from vertex to vertex. Fig1 Traditional geometric interpretation of LP 1 LP in Standard form Why do we need a standard form? Many LP algorithms like the simplex method are designed to work with a specific format. Converting to standard form allows us to use these powerful algorithms consistently. Standard form LP > MATH \\begin align > &\\text minimize x && c^T x + d \\\\\\\\ > &\\text subject to && A x = b \\\\\\\\ > & && x \\succeq 0 . >\\end align MATH Key characteristics of standard form: - Objective : Always minimization maximization problems are converted by negating - Constraints : Only equality constraints inequalities converted using slack variables - Variables : All variables must be non-negative Why Standard Form? Algorithmic efficiency: Simplex method works directly on standard form Theoretical analysis: Easier to prove optimality conditions Software implementation: Most LP solvers expect standard form input All general LPs can be converted to standard form using the following steps: Converting LPs to standard form Step 1. Use slack variables MATH to convert inequality constraints into equality constraints: > MATH \\begin align > &\\text minimize x, s && c^T x + d \\\\\\\\ > &\\text subject to && Gx + s = h \\\\\\\\ > & && Ax = b ,\\\\\\\\ > & && s \\succeq 0 . > \\end align MATH Step 2. Replace each variable MATH with two nonnegative variables: MATH , where MATH . > MATH \\begin align > &\\text minimize x^ + , x^ - , s && c^Tx^ + - c^Tx^ - + d \\\\\\\\ > &\\text subject to && Gx^ + - Gx^ - + s = h \\\\\\\\ > & && Ax^ + - Ax^ - = b ,\\\\\\\\ > & && s \\succeq 0 \\\\\\\\ > & && x^ + \\succeq 0 , x^ - \\succeq 0 . > \\end align MATH Step 3. Define MATH , MATH , MATH , and MATH as follows: > MATH \\tilde x = > \\begin bmatrix > x^ + \\\\\\\\ > x^ - \\\\\\\\ > s > \\end bmatrix , > \\tilde c = > \\begin bmatrix > c \\\\\\\\ > -c \\\\\\\\ > 0 > \\end bmatrix , > \\tilde b = > \\begin bmatrix > h \\\\\\\\ > b > \\end bmatrix > MATH , > MATH > \\tilde A = > \\begin bmatrix > G & -G & I\\\\\\\\ > A & -A & O > \\end bmatrix > MATH Step 4. Substitute MATH , MATH , MATH , and MATH into the problem from Step 2: > MATH \\begin align > &\\text minimize \\tilde x && \\tilde c ^T \\tilde x + d \\\\\\\\ > &\\text subject to && \\tilde A \\tilde x = \\tilde b \\\\\\\\ > & && \\tilde x \\succeq 0 . > \\end align MATH Example - Diet program The diet problem is a classic application of linear programming, first studied during World War II to find the most economical way to feed soldiers while meeting nutritional requirements. Problem Setup: Find the cheapest combination of foods that meets all nutritional requirements. > MATH \\begin align > &\\text minimize x && c^T x \\\\\\\\ > &\\text subject to && Dx \\succeq d \\\\\\\\ > & && x \\succeq 0 . > \\end align MATH Variables and Parameters: MATH : Cost per unit of food item j $/unit MATH : Minimum recommended intake for nutrient i units/day MATH : Amount of nutrient i in food item j units of nutrient per unit of food MATH : Amount of food item j in the diet units/day Interactive Diet Optimizer: Let's solve a simplified diet problem with 3 foods and 2 nutrients! Bread Cost: $2/loaf Protein: 4g/loaf Calories: 200/loaf Milk Cost: $3/liter Protein: 8g/liter Calories: 150/liter Meat Cost: $8/kg Protein: 20g/kg Calories: 300/kg Requirements: Minimum protein: grams/day Minimum calories: calories/day Solve Diet Problem Click \"Solve Diet Problem\" to see the optimal solution! Real-world Applications: Military logistics: Feeding troops cost-effectively Hospital meal planning: Meeting patient dietary requirements Animal feed optimization: Livestock nutrition at minimum cost School lunch programs: Nutritious meals within budget constraints The Simplex Algorithm The Simplex Algorithm , developed by George Dantzig in 1947, is one of the most important algorithms in optimization history. It revolutionized linear programming by providing an efficient method to solve LP problems systematically. Why Simplex Works: The Fundamental Theorem Fundamental Theorem of Linear Programming: If a linear program has an optimal solution, then there exists an optimal solution that occurs at a vertex extreme point of the feasible region. This theorem is the key insight behind the Simplex algorithm. Instead of searching the entire feasible region which could be infinite , we only need to check the finite number of vertices! How Simplex Works: The Strategy The Simplex algorithm follows this elegant strategy: 1. Start at any vertex of the feasible region 2. Check if the current vertex is optimal 3. Move to an adjacent vertex that improves the objective function 4. Repeat until no improvement is possible optimal solution found Simplex Algorithm Steps Let's walk through the algorithm step by step using our standard form LP: > MATH \\begin align > &\\text minimize x && c^T x \\\\\\\\ > &\\text subject to && A x = b \\\\\\\\ > & && x \\succeq 0 . > \\end align MATH Step 1: Initial Setup - Convert the LP to standard form if not already - Find an initial basic feasible solution vertex - Set up the simplex tableau Step 2: Optimality Test - Check if the current solution is optimal - If all reduced costs are non-negative, we're done! Step 3: Choose Entering Variable - Select the variable with the most negative reduced cost - This determines the direction to move Step 4: Choose Leaving Variable - Use the minimum ratio test to avoid infeasibility - This determines how far to move Step 5: Pivot Operation - Update the tableau using Gaussian elimination - Move to the new vertex Step 6: Repeat - Go back to Step 2 until optimal Interactive Simplex Example Let's solve a simple 2D problem step by step to see how Simplex works in practice: Interactive Simplex Solver: Watch the algorithm move from vertex to vertex! Problem: MATH \\begin align \\text maximize & 3x 1 + 2x 2 \\\\ \\text subject to & x 1 + x 2 \\leq 4 \\\\ & 2x 1 + x 2 \\leq 6 \\\\ & x 1, x 2 \\geq 0 \\end align MATH Next Step Reset Step 0: Starting at origin 0, 0 Current objective value: 0 Click \"Next Step\" to begin the Simplex algorithm! Current Simplex Tableau: Click \"Next Step\" to see the tableau! Why Simplex is Efficient Despite having potentially exponential worst-case complexity, Simplex is remarkably efficient in practice: Simplex Efficiency: Average case: Typically visits only 2-3 times the number of constraints Practical problems: Often solves in polynomial time Warm starts: Can reuse previous solutions when problem changes slightly Degeneracy handling: Modern implementations handle degenerate cases well Simplex Variants and Modern Developments Revised Simplex Method: - More numerically stable - Better for sparse matrices - Used in most commercial solvers Dual Simplex Method: - Starts with dual feasible solution - Useful for sensitivity analysis - Better for certain problem types Interior Point Methods: - Polynomial-time complexity guarantee - Better for very large problems - Complement rather than replace Simplex Impact and Applications The Simplex algorithm has transformed numerous industries: Simplex Success Stories: Airlines: Crew scheduling, route optimization, fleet assignment Manufacturing: Production planning, supply chain optimization Finance: Portfolio optimization, risk management Telecommunications: Network flow optimization, bandwidth allocation Energy: Power grid optimization, resource allocation / Interactive Linear Programming Visualizations Implements farmer problem, geometric interpretation, diet optimizer, and norm comparisons / // Wait for DOM to be fully loaded document.addEventListener 'DOMContentLoaded', function // ==================== FARMER PROBLEM INTERACTIVE ==================== function initializeFarmerProblem const cornSlider = document.getElementById 'corn-acres' ; const wheatSlider = document.getElementById 'wheat-acres' ; if !cornSlider || !wheatSlider return; function updateFarmerResults const corn = parseFloat cornSlider.value ; const wheat = parseFloat wheatSlider.value ; // Update display values document.getElementById 'corn-value' .textContent = corn; document.getElementById 'wheat-value' .textContent = wheat; // Calculate metrics const landUsed = corn + wheat; const laborUsed = corn 2 + wheat 1; // 2 hours per corn acre, 1 hour per wheat acre const profit = corn 300 + wheat 200; // MATH 200 per wheat acre // Update results document.getElementById 'land-used' .textContent = landUsed; document.getElementById 'labor-used' .textContent = laborUsed; document.getElementById 'total-profit' .textContent = profit; // Check feasibility const feasibilityStatus = document.getElementById 'feasibility-status' ; if landUsed = minProtein && calories >= minCalories && cost Optimal Diet Solution: Bread: $ bread amt.toFixed 1 loaves Milk: $ milk amt.toFixed 1 liters Meat: $ meat amt.toFixed 1 kg --- Total cost: MATH bestCost.toFixed 2 Total protein: MATH minProtein g Total calories: MATH minCalories ; else resultsDiv.innerHTML = ' No feasible solution found! '; ; // ==================== SIMPLEX ALGORITHM VISUALIZATION ==================== function initializeSimplexVisualization const stepButton = document.getElementById 'simplex-step' ; const resetButton = document.getElementById 'simplex-reset' ; const statusDiv = document.getElementById 'simplex-status' ; const tableauDiv = document.getElementById 'tableau-content' ; const interactiveDiv = document.getElementById 'simplex-interactive' ; if !stepButton || !resetButton || !statusDiv || !tableauDiv || !interactiveDiv return; // Simplex algorithm state let currentStep = 0; let currentVertex = 0, 0 ; // Starting at origin let isOptimal = false; // Problem: maximize 3x1 + 2x2 subject to x1 + x2 = 0 // In standard form: minimize -3x1 - 2x2 subject to x1 + x2 + s1 = 4, 2x1 + x2 + s2 = 6 const vertices = 0, 0, 4, 6 , // x1, x2, s1, s2 - origin 0, 4, 0, 2 , // 0, 4, 0, 2 - intersection with x1 + x2 = 4 2, 2, 0, 0 , // 2, 2, 0, 0 - intersection of both constraints 3, 0, 1, 0 // 3, 0, 1, 0 - intersection with 2x1 + x2 = 6 ; const objectiveValues = 0, 8, 10, 9 ; // 3x1 + 2x2 at each vertex const simplexPath = 0, 3, 2 ; // Path: origin -> 3,0 -> 2,2 optimal function createVisualization const width = 400; const height = 300; const margin = top: 20, right: 20, bottom: 40, left: 40 ; // Clear previous content interactiveDiv.innerHTML = ''; const svg = d3.select ' simplex-interactive' .append 'svg' .attr 'width', width .attr 'height', height ; // Scales const xScale = d3.scaleLinear .domain 0, 4 .range margin.left, width - margin.right ; const yScale = d3.scaleLinear .domain 0, 6 .range height - margin.bottom, margin.top ; // Draw feasible region const feasibleRegion = 0, 0 , 0, 4 , 2, 2 , 3, 0 , 0, 0 ; svg.append 'path' .datum feasibleRegion .attr 'fill', 'lightblue' .attr 'fill-opacity', 0.3 .attr 'stroke', 'blue' .attr 'stroke-width', 2 .attr 'd', d3.line .x d => xScale d 0 .y d => yScale d 1 ; // Draw constraint lines // x1 + x2 = 4 svg.append 'line' .attr 'x1', xScale 0 .attr 'y1', yScale 4 .attr 'x2', xScale 4 .attr 'y2', yScale 0 .attr 'stroke', 'red' .attr 'stroke-width', 2 .attr 'stroke-dasharray', '5,5' ; // 2x1 + x2 = 6 svg.append 'line' .attr 'x1', xScale 0 .attr 'y1', yScale 6 .attr 'x2', xScale 3 .attr 'y2', yScale 0 .attr 'stroke', 'green' .attr 'stroke-width', 2 .attr 'stroke-dasharray', '5,5' ; // Draw vertices const vertexPoints = 0,0 , 0,4 , 2,2 , 3,0 ; svg.selectAll '.vertex' .data vertexPoints .enter .append 'circle' .attr 'class', 'vertex' .attr 'cx', d => xScale d 0 .attr 'cy', d => yScale d 1 .attr 'r', 6 .attr 'fill', 'orange' .attr 'stroke', 'black' .attr 'stroke-width', 2 ; // Current position indicator svg.append 'circle' .attr 'id', 'current-position' .attr 'cx', xScale currentVertex 0 .attr 'cy', yScale currentVertex 1 .attr 'r', 8 .attr 'fill', 'red' .attr 'stroke', 'darkred' .attr 'stroke-width', 3 ; // Axes svg.append 'g' .attr 'transform', translate 0,$ height - margin.bottom .call d3.axisBottom xScale ; svg.append 'g' .attr 'transform', translate $ margin.left ,0 .call d3.axisLeft yScale ; // Axis labels svg.append 'text' .attr 'x', width / 2 .attr 'y', height - 5 .attr 'text-anchor', 'middle' .text 'x₁' ; svg.append 'text' .attr 'transform', 'rotate -90 ' .attr 'x', -height / 2 .attr 'y', 15 .attr 'text-anchor', 'middle' .text 'x₂' ; // Legend const legend = svg.append 'g' .attr 'transform', translate $ width - 150 , 30 ; legend.append 'text' .attr 'x', 0 .attr 'y', 0 .text 'Constraints:' .attr 'font-weight', 'bold' ; legend.append 'line' .attr 'x1', 0 .attr 'y1', 15 .attr 'x2', 20 .attr 'y2', 15 .attr 'stroke', 'red' .attr 'stroke-width', 2 .attr 'stroke-dasharray', '5,5' ; legend.append 'text' .attr 'x', 25 .attr 'y', 19 .text 'x₁ + x₂ ≤ 4' .attr 'font-size', '12px' ; legend.append 'line' .attr 'x1', 0 .attr 'y1', 30 .attr 'x2', 20 .attr 'y2', 30 .attr 'stroke', 'green' .attr 'stroke-width', 2 .attr 'stroke-dasharray', '5,5' ; legend.append 'text' .attr 'x', 25 .attr 'y', 34 .text '2x₁ + x₂ ≤ 6' .attr 'font-size', '12px' ; function updateTableau step let tableauHTML = ''; switch step case 0: tableauHTML = Basic x₁ x₂ s₁ s₂ RHS s₁ 1 1 1 0 4 s₂ 2 1 0 1 6 z -3 -2 0 0 0 Analysis: Most negative coefficient is -3 x₁ column . x₁ enters the basis. ; break; case 1: tableauHTML = Basic x₁ x₂ s₁ s₂ RHS Ratio s₁ 1 1 1 0 4 4/1 = 4 s₂ 2 1 0 1 6 6/2 = 3 z -3 -2 0 0 0 - Minimum ratio test: min 4/1, 6/2 = min 4, 3 = 3. s₂ leaves the basis. ; break; case 2: tableauHTML = Basic x₁ x₂ s₁ s₂ RHS s₁ 0 0.5 1 -0.5 1 x₁ 1 0.5 0 0.5 3 z 0 -0.5 0 1.5 9 Current solution: x₁ = 3, x₂ = 0, objective = 9 Analysis: x₂ has negative coefficient -0.5 , so x₂ enters the basis. ; break; case 3: tableauHTML = Basic x₁ x₂ s₁ s₂ RHS x₂ 0 1 2 -1 2 x₁ 1 0 -1 1 2 z 0 0 1 1 10 OPTIMAL SOLUTION FOUND! x₁ = 2, x₂ = 2, Maximum objective value = 10 All coefficients in the objective row are non-negative. ; break; tableauDiv.innerHTML = tableauHTML; function updateStatus step let statusHTML = ''; switch step case 0: statusHTML = Step 0: Initial basic feasible solution Current vertex: 0, 0 Current objective value: 0 Basic variables: s₁ = 4, s₂ = 6 ; currentVertex = 0, 0 ; break; case 1: statusHTML = Step 1: Choose entering variable Most negative coefficient: -3 x₁ column x₁ enters the basis Performing minimum ratio test... ; break; case 2: statusHTML = Step 2: First iteration complete Current vertex: 3, 0 Current objective value: 9 Basic variables: x₁ = 3, s₁ = 1 ; currentVertex = 3, 0 ; break; case 3: statusHTML = Step 3: OPTIMAL SOLUTION FOUND! Current vertex: 2, 2 Maximum objective value: 10 Basic variables: x₁ = 2, x₂ = 2 ; currentVertex = 2, 2 ; isOptimal = true; break; statusDiv.innerHTML = statusHTML; // Update visualization const currentPos = d3.select ' current-position' ; if currentPos.node const xScale = d3.scaleLinear .domain 0, 4 .range 40, 360 ; const yScale = d3.scaleLinear .domain 0, 6 .range 260, 40 ; currentPos .transition .duration 500 .attr 'cx', xScale currentVertex 0 .attr 'cy', yScale currentVertex 1 ; function nextStep if currentStep",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_01_Linear_Programming_(LP)/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_02_Quadratic_Programming_(QP) copy",
    "title": "05-02 Quadratic Programming (QP)",
    "chapter": "05",
    "order": 3,
    "owner": "Hooncheol Shin",
    "lesson_type": "required",
    "content": "A Quadratic Program QP is a convex optimization problem where the objective function is a convex quadratic function and all constraint functions are affine. The general quadratic program is formulated as: General Quadratic Program MATH \\begin align \\text minimize x \\quad &\\frac 1 2 x^T P x + q^T x + r \\\\ \\text subject to \\quad &Gx \\preceq h \\\\ &Ax = b \\end align MATH where: - MATH positive semidefinite matrix - MATH inequality constraint matrix - MATH equality constraint matrix - MATH decision variable Key Properties of QP: - The constant MATH in the objective function does not affect the optimization process or result and can be omitted. - If MATH is not satisfied, the problem is not convex. - Even if not explicitly stated, QP assumes MATH . - The above problem seeks the minimizer MATH of the convex quadratic function MATH over a polyhedral feasible set. Understanding MATH Positive Semidefinite Cone The notation MATH represents the positive semidefinite cone , which is a fundamental concept in convex optimization: Definition: > MATH where: - MATH is the set of all MATH symmetric matrices - MATH means matrix MATH is positive semidefinite Key Properties: 1. Positive semidefinite condition : A matrix MATH is positive semidefinite if: - All eigenvalues of MATH are non-negative MATH - For any vector MATH , we have MATH 2. Convex cone property : MATH is a convex cone because if MATH and MATH , then MATH Example for MATH : For a MATH matrix MATH , the condition MATH requires: - MATH diagonal elements non-negative - MATH diagonal elements non-negative - MATH determinant non-negative Why is this important for QP? - The condition MATH ensures that the quadratic function MATH is convex - Without this condition, the problem may have multiple local minima and would not be a convex optimization problem - This guarantees that any local minimum is also a global minimum Understanding MATH Component-wise Inequality The notation MATH represents component-wise inequality constraints , which is a compact way to write multiple linear inequality constraints: Definition: MATH where: - MATH is the constraint matrix - MATH is the decision variable vector - MATH is the right-hand side vector - MATH is the number of inequality constraints Expanded Form: The single matrix inequality MATH is equivalent to the system: MATH \\begin align g 1^T x &\\leq h 1 \\\\ g 2^T x &\\leq h 2 \\\\ &\\vdots \\\\ g m^T x &\\leq h m \\end align MATH where MATH is the MATH -th row of matrix MATH . Example: Consider MATH , MATH , and MATH Then MATH means: MATH \\begin align x 1 + 2x 2 &\\leq 5 \\\\ -x 1 + 3x 2 &\\leq 2 \\\\ -x 2 &\\leq -1 \\quad \\text i.e., x 2 \\geq 1\\text \\end align MATH Geometric Interpretation: - Each inequality MATH defines a half-space in MATH - The feasible region is the intersection of all these half-spaces - This intersection forms a polyhedron or polytope if bounded - The constraint MATH defines the polyhedral feasible set for the QP Fig 1 Geometric interpretation of QP 1 QP in Standard Form The standard form of a quadratic program is: Standard Form QP MATH \\begin align \\text minimize x \\quad &\\frac 1 2 x^T P x + q^T x + r \\\\ \\text subject to \\quad &A x = b \\\\ &x \\succeq 0 \\end align MATH Any general quadratic program can be converted to standard form using the following steps: Converting QPs to Standard Form Step 1. Use slack variables MATH to convert inequality constraints into equality constraints: MATH \\begin align \\text minimize x, s \\quad &\\frac 1 2 x^T P x + q^T x + r \\\\ \\text subject to \\quad &Gx + s = h \\\\ &Ax = b \\\\ &s \\succeq 0 \\end align MATH Step 2. Replace MATH with two nonnegative variables: MATH MATH \\begin align \\text minimize x^ + , x^ - , s \\quad &\\frac 1 2 x^ + - x^ - ^T P x^ + - x^ - + q^T x^ + - q^T x^ - + r\\\\ \\text subject to \\quad &Gx^ + - Gx^ - + s = h \\\\ &Ax^ + - Ax^ - = b \\\\ &s \\succeq 0 \\\\ &x^ + \\succeq 0, \\quad x^ - \\succeq 0 \\end align MATH Step 3. Define MATH , MATH , MATH , MATH , MATH : MATH \\tilde x = \\begin bmatrix x^ + \\\\ x^ - \\\\ s \\end bmatrix , \\quad \\tilde q = \\begin bmatrix q \\\\ -q \\\\ 0 \\end bmatrix , \\quad \\tilde b = \\begin bmatrix h \\\\ b \\end bmatrix MATH MATH \\tilde A = \\begin bmatrix G & -G & I \\\\ A & -A & O \\end bmatrix , \\quad \\tilde P = \\begin bmatrix P & -P & O \\\\ -P & P & O \\\\ O & O & O \\end bmatrix MATH Step 4. Substitute the expressions from Step 3 into the formulation: MATH \\begin align \\text minimize \\tilde x \\quad &\\frac 1 2 \\tilde x ^T \\tilde P \\tilde x + \\tilde q ^T \\tilde x + r \\\\ \\text subject to \\quad &\\tilde A \\tilde x = \\tilde b \\\\ &\\tilde x \\succeq 0 \\end align MATH Linear Programming as a Special Case of QP If the quadratic term is removed from the objective function of a quadratic program, it takes the form of a linear program. Thus, LP is a special case of QP, denoted as LP MATH QP. Recall: General LP MATH \\begin align \\text minimize x \\quad &c^T x + d \\\\ \\text subject to \\quad &Gx \\preceq h \\\\ &Ax = b \\end align MATH where MATH and MATH . Example 1: Portfolio Optimization This problem involves appropriately trading off performance and risk in creating a financial portfolio. MATH \\begin align \\text maximize x \\quad &\\mu^T x - \\frac \\gamma 2 x^T P x \\\\ \\text subject to \\quad &\\mathbf 1 ^Tx = 1 \\\\ &x \\succeq 0 \\end align MATH where: - MATH : expected assets' returns - MATH : covariance matrix of assets' returns - MATH : risk aversion parameter hyperparameter - MATH : portfolio holdings percentages Example 2: Support Vector Machines Support Vector Machines SVM are an example of a quadratic program. Below is C-SVM, a variant of SVM. A detailed explanation of SVM is beyond the scope of this chapter and will therefore be omitted. MATH \\begin align \\text minimize \\beta, \\beta 0, \\xi \\quad &\\frac 1 2 \\| \\beta \\| 2^2 + C \\sum i=1 ^ n \\xi i \\\\ \\text subject to \\quad &\\xi i \\geq 0, \\quad i = 1, \\ldots, n \\\\ &y i x i^T \\beta + \\beta 0 \\geq 1 - \\xi i, \\quad i = 1, \\ldots, n \\end align MATH given: MATH and MATH having rows MATH . Example 3: Least-Squares in Regression The problem of minimizing the following convex quadratic function corresponds to an unconstrained QP: MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_02_Quadratic_Programming_(QP)-copy/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_03_Quadratically_Constrained_Quadratic_Programming_(QCQP)",
    "title": "05-03 Quadratically Constrained Quadratic Programming (QCQP)",
    "chapter": "05",
    "order": 4,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "If the inequality constraint functions in a quadratic program are replaced with convex quadratic functions, the problem is called a Quadratically Constrained Quadratic Program QCQP . Quadratically Constrained Quadratic Program > MATH >\\begin align > &\\text minimize x && 1/2 x^T P 0 x + q 0^T x + r 0 \\\\\\\\ > &\\text subject to && 1/2 x^T P i x + q i^T x + r i \\leq 0 , i = 1, \\dotsc, m\\\\\\\\ > & && Ax = b ,\\\\\\\\ > & \\text where &&P i \\in \\mathbb S + ^n \\text for i = 0, \\dotsc, m, \\text and A \\in \\mathbb R ^ p \\times n . >\\end align MATH QP and equivalent QCQP If MATH for all MATH in the QCQP constraints, the problem reduces to a QP. Thus, QP is a special case of QCQP, and MATH . Recall: Quadratic Program > MATH >\\begin align > &\\text minimize x && 1/2 x^T P x + q^T x + r \\\\\\\\ > &\\text subject to && Gx \\preceq h \\\\\\\\ > & && Ax = b ,\\\\\\\\ > & \\text where &&P \\in \\mathbb S + ^n, G \\in \\mathbb R ^ m \\times n , \\text and A \\in \\mathbb R ^ p \\times n . >\\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_03_Quadratically_Constrained_Quadratic_Programming_(QCQP)/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_04_Second_Order_Cone_Programming_(SOCP)",
    "title": "05-04 Second-Order Cone Programming (SOCP)",
    "chapter": "05",
    "order": 5,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "If the inequality constraints in a general LP are replaced with second-order cone constraints which are affine functions , the problem is called a Second-Order Cone Program SOCP . Second-Order Cone Program > MATH >\\begin align > &\\text minimize x && f^T x \\\\\\\\ > &\\text subject to && \\| A i x + b i \\| 2 \\leq c i^T x + d i, i = 1, \\dotsc, m \\\\\\\\ > & && Fx = g ,\\\\\\\\ > & \\text where &&x \\in \\mathbb R ^n \\text is the optimization variable, A i \\in \\mathbb R ^ n i \\times n , \\text and F \\in \\mathbb R ^ p \\times n . >\\end align MATH Recall: Norm cone A norm cone is a convex cone in MATH defined by all points MATH such that MATH for some norm MATH . > MATH The figure below shows the norm cone for the MATH norm MATH , also called the second-order cone or ice cream cone. Fig1 Norm Cone 1 QCQP and equivalent SOCP A QCQP can be reformulated as an SOCP in certain cases, i.e., MATH . Recall: Quadratically Constrained Quadratic Program > MATH >\\begin align > &\\text minimize x && 1/2 x^T P 0 x + q 0^T x + r 0 \\\\\\\\ > &\\text subject to && 1/2 x^T P i x + q i^T x + r i \\leq 0 , i = 1, \\dotsc, m\\\\\\\\ > & && Ax = b ,\\\\\\\\ > & \\text where &&P i \\in \\mathbb S + ^n \\text for i = 0, \\dotsc, m, \\text and A \\in \\mathbb R ^ p \\times n >\\end align MATH > Step 1. For convenience, QCQP can be reformulated in different ways to fit SOCP structure. > MATH >\\begin align > &\\text minimize x && x^T P 0 x + 2q 0^T x + r 0 \\\\\\\\ > &\\text subject to && x^T P i x + 2q i^T x + r i \\leq 0 , i = 1, \\dotsc, m\\\\\\\\ > & && Ax = b ,\\\\\\\\ > & \\text where && P i \\in \\mathbb S + ^n \\text for i = 0, \\dotsc, m \\text , and A \\in \\mathbb R ^ \\text p x n . >\\end align \\\\ > MATH > Step 2. Since MATH is a positive semidefinite matrix, any MATH satisfying MATH is also a positive semidefinite matrix. This MATH can be obtained through eigendecomposition. Using this, the objective function of the QCQP can be transformed as follows: MATH MATH > MATH > \\begin align > x^T P 0 x + 2q 0^T x + r 0 &= x^T P 0 x + q 0^T x + x^T q 0 + q 0^T P 0^ -1 q 0 - q 0^T P 0^ -1 q 0 + r 0 \\\\\\\\ > &= x^T Q 0 \\Lambda 0 \\Lambda 0 Q 0^T x + > q 0^T Q 0 \\Lambda 0^ -1 \\Lambda 0 Q 0^ -1 x + x^T Q 0 \\Lambda 0 \\Lambda 0^ -1 Q 0^ -1 q 0 + > q 0^T Q 0 \\Lambda 0^ -1 \\Lambda 0^ -1 Q 0^T q 0 - q 0^T P 0^ -1 q 0 + r 0 \\\\\\\\ > &= \\Lambda 0 Q 0^T x + \\Lambda 0^ -1 Q 0^T q 0 ^T \\Lambda 0 Q 0^T x + \\Lambda 0^ -1 Q 0^T q 0 - q 0^T P 0^ -1 q 0 + r 0 \\\\\\\\ > &=\\| \\Lambda 0 Q 0^T x + \\Lambda 0^ -1 Q 0^T q 0 \\| 2^2 - q 0^T P 0^ -1 q 0 + r 0 \\\\\\\\ > \\end align > MATH Step 3. The same procedure as in Step 2 is applied to the inequality constraint functions, and then substituted into the QCQP from Step 1. > MATH >\\begin align > &\\text minimize x && \\| \\Lambda 0 Q 0^T x + \\Lambda 0^ -1 Q 0^T q 0 \\| 2^2 - q 0^T P 0^ -1 q 0 + r 0 \\\\\\\\ > &\\text subject to && \\| \\Lambda i Q i^T x + \\Lambda i^ -1 Q i^T q i \\| 2^2 \\leq q i^T P i^ -1 q i + r i , i = 1, \\dotsc, m\\\\\\\\ > & && Ax = b .\\\\\\\\ >\\end align > MATH > Step 4. The term MATH in the objective function is a constant and can be omitted. > MATH >\\begin align > &\\text minimize x && \\| \\Lambda 0 Q 0^T x + \\Lambda 0^ -1 Q 0^T q 0 \\| 2^2 \\\\\\\\ > &\\text subject to && \\| \\Lambda i Q i^T x + \\Lambda i^ -1 Q i^T q i \\| 2^2 \\leq q i^T P i^ -1 q i + r i , i = 1, \\dotsc, m\\\\\\\\ > & && Ax = b .\\\\\\\\ >\\end align > MATH Step 5. Introducing a scalar variable MATH , the same problem as in Step 4 can be defined. > MATH >\\begin align > &\\text minimize x, t && t \\\\\\\\ > &\\text subject to &&\\lVert \\Lambda 0 Q 0^T x + \\Lambda 0^ -1 Q 0^T q 0 \\rVert 2^2 \\leq t\\\\\\\\ > & && \\| \\Lambda i Q i^T x + \\Lambda i^ -1 Q i^T q i \\| 2^2 \\leq q i^T P i^ -1 q i + r i , i = 1, \\dotsc, m\\\\\\\\ > & && Ax = b .\\\\\\\\ >\\end align > MATH The above represents a special case of SOCP. Thus, the relationship MATH holds.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_04_Second_Order_Cone_Programming_(SOCP)/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_05_Semidefinite_Programming_(SDP)",
    "title": "05-05 Semidefinite Programming (SDP)",
    "chapter": "05",
    "order": 6,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "If the inequality constraint in a general LP is replaced by a linear matrix inequality LMI , the problem is called a Semidefinite Program SDP . Semidefinite Program > MATH >\\begin align > &\\text minimize x && c^T x + d \\\\\\\\ > &\\text subject to && x 1 F 1 + \\dotsb + x n F n + G \\preceq 0 \\\\\\\\ > & && Ax = b ,\\\\\\\\ > & \\text where &&G, F 1, \\dotsb, F n \\in \\mathbb S ^ k \\text and A \\in \\mathbb R ^ p \\times n . >\\end align MATH If MATH are all diagonal matrices, the above inequality constraint is equivalent to MATH linear inequalities, and the SDP reduces to an LP. Multiple LMIs can be represented as a single LMI using block diagonal matrices: > MATH > x 1\\hat F 1 + \\dotsb + x n\\hat F n + \\hat G \\preceq 0, \\quad x 1\\tilde F 1 + \\dotsb + x n\\tilde F n + \\tilde G \\preceq 0 > MATH > is equivalent to a single LMI: > MATH > x 1 > \\begin bmatrix > \\hat F 1 & 0 \\\\\\\\ > 0 & \\tilde F 1 \\\\\\\\ > \\end bmatrix + > x 2 > \\begin bmatrix > \\hat F 2 & 0 \\\\\\\\ > 0 & \\tilde F 2 \\\\\\\\ > \\end bmatrix + > \\dotsb > + > x n > \\begin bmatrix > \\hat F n & 0 \\\\\\\\ > 0 & \\tilde F n \\\\\\\\ > \\end bmatrix + > \\begin bmatrix > \\hat G & 0 \\\\\\\\ > 0 & \\tilde G \\\\\\\\ > \\end bmatrix > \\preceq 0 > MATH SDP in Standard form When expressed as follows, it is called the standard form of a semidefinite program. Standard form SDP > MATH >\\begin align > &\\text minimize X && \\mathrm tr CX \\\\\\\\ > &\\text subject to && \\mathrm tr A i X = b i, \\quad i = 1, \\dotsc, m \\\\\\\\ > & && X \\succeq 0 ,\\\\\\\\ > & \\text where &&C, A i \\in \\mathbb S ^n, X \\in \\mathbb S ^n. >\\end align MATH Recall: MATH All SDPs can be transformed into the standard form SDP through the following process. Converting SDPs to standard form Step1. Use a slack variable S to convert the inequality constraint into an equality constraint. > MATH >\\begin align > &\\text minimize x && c^T x + d \\\\\\\\ > &\\text subject to && \\sum l=1 ^n F l x l+ S = -G \\\\\\\\ > & && Ax = b \\\\\\\\ > & && S \\succeq 0 >\\end align > MATH Step2. Transform the equality constraints derived in Step 1 into component-wise equations. > MATH >\\begin align > &\\text minimize x && c^T x + d \\\\\\\\ > &\\text subject to && \\sum l=1 ^n F l x l ij + S ij = -G ij , i,j = 1, \\dotsc, k \\\\\\\\ > & && Ax = b \\\\\\\\ > & && S \\succeq 0 >\\end align > MATH Step3. Replace x with two nonnegative variables. MATH , where MATH > MATH >\\begin align > &\\text minimize x && c^T x^ + - x^ - + d \\\\\\\\ > &\\text subject to && \\sum l=1 ^n F l x^ + l ij - \\sum l=1 ^n F l x^ - l ij + S ij = -G ij , i,j = 1, \\dotsc, k \\\\\\\\ > & && Ax^ + - Ax^ - = b \\\\\\\\ > & && S \\succeq 0 \\\\\\\\ > & && x^ + \\text , x^ - \\succeq 0 . >\\end align > MATH Step4. Define MATH . All the blanks are zero. > MATH > X = > \\begin bmatrix > diag x^ + \\\\\\\\ > & diag x^ - \\\\\\\\ > && s 11 \\\\\\\\ > &&& s 12 \\\\\\\\ > &&&&\\dotsc\\\\\\ > &&&&&s ij \\\\\\\\ > &&&&&&\\dotsc \\\\\\ > &&&&&&&s kk \\\\\\\\ > \\end bmatrix > , MATH > MATH > C = > \\begin bmatrix > diag c \\\\\\\\ > & -diag c &\\\\\\\\ > & & O k^2 \\text x k^2 \\\\\\\\ > \\end bmatrix > , MATH > MATH > P ij = > \\begin bmatrix > F 1 ij \\\\\\\\ > & F 2 ij \\\\\\\\ > &&\\dotsc\\\\\\\\ > &&& F n ij \\\\\\\\ > &&&&- F 1 ij \\\\\\\\ > &&&&&- F 2 ij \\\\\\\\ > &&&&&&\\dotsc\\\\\\\\ > &&&&&&&- F n ij \\\\\\\\ > &&&&&&&&0&\\\\\\\\ > &&&&&&&&&\\dotsc\\\\\\\\ > &&&&&&&&&&1 \\phantom 1 \\text ij th position \\\\\\\\ > &&&&&&&&&&&\\dotsc\\\\\\\\ > &&&&&&&&&&&&0\\\\\\\\ > \\end bmatrix > , MATH > > MATH > Q i = > \\begin bmatrix > diag A i \\\\\\\\ > &-diag A i \\\\\\\\ > &&O k^2 \\text x k^2 \\\\\\\\ > \\end bmatrix > MATH > MATH is ith row of A , > MATH > \\tilde A = > \\begin bmatrix > P 11 \\\\\\\\ > \\dotsc\\\\\\\\ > P kk \\\\\\\\ > Q 1 \\\\\\\\ > \\dotsc\\\\\\\\ > Q p \\\\\\\\ > \\end bmatrix > -G ij = \\mathrm tr P ij X > , MATH > > MATH > b i = \\mathrm tr Q iX > MATH , > > MATH > \\tilde b = > \\begin bmatrix > -G 11 \\\\\\\\ > \\dotsc\\\\\\\\ > -G kk \\\\\\\\ > b 1 \\\\\\\\ > \\dotsc\\\\\\\\ > b p \\\\\\\\ > \\end bmatrix > MATH . Step5. Substitute the problem from Step3 with MATH . > MATH >\\begin align > &\\text minimize X && \\mathrm tr CX \\\\\\\\ > &\\text subject to && \\mathrm tr \\tilde A iX = \\tilde b i, \\quad i=1,\\dotsc,k^2+p \\\\\\\\ > & && X \\succeq 0 . >\\end align > MATH SOCP and equivalent SDP By using the Schur complement 8 https://en.wikipedia.org/wiki/Schur complement , the inequality constraint of SOCP can be expressed in such a way that SOCP is transformed into a special case of SDP. That is, there is a relationship of inclusion: SOCP MATH SDP. Recall: Second-Order Cone Program > MATH >\\begin align > &\\text minimize x && f^T x \\\\\\\\ > &\\text subject to && \\| A i x + b i \\| 2 \\leq c i^T x + d i, i = 1, \\dotsc, m \\\\\\\\ > & && Fx = g . >\\end align > MATH SOCP to SDP by Schur complement > MATH >\\begin align > &\\text minimize x && f^T x \\\\\\\\ > &\\text subject to > && > \\begin bmatrix > c i^T x + d I & A i x + b i \\\\\\\\ > A i x + b i ^T & c i^T x + d \\\\\\\\ > \\end bmatrix \\succeq 0, i = 1, \\dotsc, m\\\\\\\\ > & && Fx = g . >\\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_05_Semidefinite_Programming_(SDP)/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_06_Conic_Programming_(CP)",
    "title": "05-06 Conic Programming (CP)",
    "chapter": "05",
    "order": 7,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "If the inequality constraint in a general LP is replaced by a generalized inequality constraint, the problem is called a Conic Program CP . Conic Program > MATH >\\begin align > &\\text minimize x && c^T x + d \\\\\\\\ > &\\text subject to && Fx + g \\preceq K 0 \\\\\\\\ > & && Ax = b ,\\\\\\\\ > & \\text where &&c, x \\in \\mathbb R ^ n , A \\in \\mathbb R ^ p \\times n , \\text and b \\in \\mathbb R ^ p . >\\end align MATH MATH is a linear map, MATH , for Euclidean space MATH . LP is the special case when MATH , i.e., LP MATH CP. SDP is the special case when MATH , i.e., SDP MATH CP.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_06_Conic_Programming_(CP)/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_02_01_Least_Square",
    "title": "05-02-01 Linear Least-Squares Problems",
    "chapter": "05",
    "order": 8,
    "owner": "Hooncheol Shin",
    "lesson_type": "required",
    "content": "Linear Least-Squares Problems A linear least-squares problem is an optimization problem without constraints where we minimize the sum of squared errors: MATH where: - MATH is a matrix with MATH - MATH are the rows of MATH - MATH is the variable we want to find - MATH is the target vector Goal: Find MATH to minimize the sum of squared residuals. Example: Linear Regression for single variable Finding the best-fit line MATH through data points. We minimize the sum of squared vertical distances from points to the line. Goal: Find MATH . Interactive Linear Regression Demonstration Instructions: Click on the canvas to add data points. The red line shows the best-fit line. Regression Parameters Slope m : 0.000 Intercept c : 0.000 R² Score: N/A MSE: N/A Equation y = 0.000x + 0.000 Clear Points Add Random Points Mathematical Formulation Objective: Minimize sum of squared residuals S m,c = Σ yᵢ - mxᵢ - c ² Solution: m = Σ xᵢ-x̄ yᵢ-ȳ / Σ xᵢ-x̄ ² c = ȳ - mx̄ // Linear Regression Interactive Demo class LinearRegressionDemo constructor this.canvas = document.getElementById 'regressionCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.points = ; this.slope = 0; this.intercept = 0; // Set up canvas this.canvas.addEventListener 'click', e => this.addPoint e ; // Initialize with some sample points this.addRandomPoints ; this.draw ; addPoint event const rect = this.canvas.getBoundingClientRect ; const x = event.clientX - rect.left; const y = event.clientY - rect.top; // Convert canvas coordinates to data coordinates const dataX = x / this.canvas.width 10; const dataY = this.canvas.height - y / this.canvas.height 10; this.points.push x: dataX, y: dataY ; this.calculateRegression ; this.draw ; this.updateDisplay ; addRandomPoints // Add some sample points with a trend const baseSlope = 0.8; const baseIntercept = 2; for let i = 0; i sum + p.x, 0 ; const sumY = this.points.reduce sum, p => sum + p.y, 0 ; const sumXY = this.points.reduce sum, p => sum + p.x p.y, 0 ; const sumXX = this.points.reduce sum, p => sum + p.x p.x, 0 ; const meanX = sumX / n; const meanY = sumY / n; const numerator = sumXY - n meanX meanY; const denominator = sumXX - n meanX meanX; if Math.abs denominator sum + p.y, 0 / this.points.length; let ssRes = 0; let ssTot = 0; for const point of this.points const predicted = this.slope point.x + this.intercept; ssRes += Math.pow point.y - predicted, 2 ; ssTot += Math.pow point.y - meanY, 2 ; return ssTot === 0 ? 1 : 1 - ssRes / ssTot ; calculateMSE if this.points.length === 0 return 0; let mse = 0; for const point of this.points const predicted = this.slope point.x + this.intercept; mse += Math.pow point.y - predicted, 2 ; return mse / this.points.length; draw // Clear canvas this.ctx.clearRect 0, 0, this.canvas.width, this.canvas.height ; // Draw grid this.drawGrid ; // Draw regression line if this.points.length >= 2 this.drawRegressionLine ; // Draw points and residuals this.drawPoints ; // Draw axes labels this.drawLabels ; drawGrid this.ctx.strokeStyle = ' f0f0f0'; this.ctx.lineWidth = 1; // Vertical lines for let i = 0; i = 2 const predictedY = this.slope point.x + this.intercept; const predictedCanvasY = this.canvas.height - predictedY / 10 this.canvas.height; this.ctx.strokeStyle = ' ff6b6b'; this.ctx.lineWidth = 1; this.ctx.setLineDash 2, 2 ; this.ctx.beginPath ; this.ctx.moveTo canvasX, canvasY ; this.ctx.lineTo canvasX, predictedCanvasY ; this.ctx.stroke ; this.ctx.setLineDash ; // Draw point this.ctx.fillStyle = ' 2f3542'; this.ctx.beginPath ; this.ctx.arc canvasX, canvasY, 4, 0, 2 Math.PI ; this.ctx.fill ; drawLabels this.ctx.fillStyle = ' 666'; this.ctx.font = '12px Arial'; // X-axis labels for let i = 0; i = 2 document.getElementById 'r2-value' .textContent = this.calculateR2 .toFixed 3 ; document.getElementById 'mse-value' .textContent = this.calculateMSE .toFixed 3 ; else document.getElementById 'r2-value' .textContent = 'N/A'; document.getElementById 'mse-value' .textContent = 'N/A'; // Global functions for buttons function clearPoints if window.regressionDemo window.regressionDemo.clearPoints ; function addRandomPoints if window.regressionDemo window.regressionDemo.clearPoints ; window.regressionDemo.addRandomPoints ; // Initialize when DOM is loaded document.addEventListener 'DOMContentLoaded', function if document.getElementById 'regressionCanvas' window.regressionDemo = new LinearRegressionDemo ; ; // Initialize immediately if DOM is already loaded if document.readyState === 'loading' document.addEventListener 'DOMContentLoaded', function if document.getElementById 'regressionCanvas' window.regressionDemo = new LinearRegressionDemo ; ; else if document.getElementById 'regressionCanvas' window.regressionDemo = new LinearRegressionDemo ; Problem: Given MATH data points MATH , find the line MATH that minimizes the sum of squared vertical distances from the points to the line. Objective Function: We want to minimize MATH Solution: To find the minimum, we take partial derivatives and set them equal to zero. Taking the partial derivative with respect to MATH : > MATH This gives us: MATH Therefore: > MATH where MATH and MATH are the means of MATH and MATH values. Taking the partial derivative with respect to MATH : > MATH Substituting MATH : MATH Rearranging: MATH Since MATH and MATH : > MATH Note that MATH Therefore: > MATH Final Result: The best-fit line has parameters: > MATH This is the classical least-squares solution for linear regression, also known as the Normal Equations . --- The Optimal Solution of Linear Regression with multiple variables Problem Statement: In Linear Regression, we aim to find a vector of coefficients MATH that best fits a linear model to a given dataset. We have MATH data points, each with MATH features. Let MATH be the design matrix of size MATH , where each row represents a data point and each column represents a feature. Let MATH be the vector of target values of size MATH . Our linear model predicts the target values MATH as: > MATH where MATH is the vector of unknown coefficients of size MATH . Objective Function Cost Function : The goal is to minimize the sum of squared errors residuals between the actual target values MATH and the predicted values MATH . This is known as the Ordinary Least Squares OLS objective function: MATH > Note: MATH is the cost function also called loss function or objective function that measures how well our linear model fits the data. It represents the total squared error between our predictions and the actual values. The smaller MATH , the better our model fits the data. Our goal is to find the optimal weights MATH that minimize this function. We can express this in matrix form by expanding the squared Euclidean norm: MATH Expanding this expression: MATH Using the property that MATH , we have MATH . Also, since MATH is a scalar, its transpose is itself: MATH . Thus, the two middle terms are identical: MATH Minimization MATH using Calculus: To find the optimal MATH that minimizes MATH , we take the derivative of MATH with respect to MATH and set it to zero. We use the following matrix calculus rules: 1. MATH 2. MATH If MATH is symmetric, this simplifies to MATH Applying these rules to MATH : MATH Let's evaluate each term: - MATH since MATH is a scalar constant with respect to MATH - MATH using rule 1, with MATH - For the third term, let MATH . Note that MATH is a symmetric matrix because MATH . So, MATH using rule 2 for a symmetric matrix MATH Combining these, the derivative is: > MATH Finding the Optimal Solution: To find the minimum of MATH , we set the derivative equal to zero: MATH Dividing by 2 and rearranging: MATH This is known as the Normal Equation . If MATH is invertible which happens when MATH has full column rank , we can solve for MATH : > MATH This is the closed-form solution for linear least-squares regression, also known as the Normal Equation solution . Performance: - Time complexity: roughly MATH operations - A standard computer solves problems with hundreds of variables and thousands of terms in seconds - Sparse matrices many zero entries can be solved much faster Example: A sparse matrix for image processing might have only 5 non-zero entries per row in a 10,000 × 10,000 matrix.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_02_01_Least_Square/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_02_02_Geometric_Programming",
    "title": "05-02-02 Geometric Programming",
    "chapter": "05",
    "order": 9,
    "owner": "Copilot",
    "lesson_type": "",
    "content": "In this section, we will see a class of problems that appear non-convex when looking at the objective function and constraint functions, but can be transformed into convex form through several techniques. First, we need some definitions: 5.2.1. Monomials and Posynomials A function MATH with domain MATH all elements are positive has the form: MATH where MATH and MATH , is called a monomial function . A sum of monomials: MATH where MATH , is called a posynomial function , or simply a posynomial . 5.2.2. Geometric Programming An optimization problem of the form: MATH \\begin align x &= \\arg\\min x f 0 x \\\\ \\text subject to: &f i x \\leq 1, \\quad i = 1, 2, \\ldots, m \\quad \\quad 26 \\\\ &h j x = 1, \\quad j = 1, 2, \\ldots, p \\end align MATH where MATH are posynomials and MATH are monomials, is called Geometric Programming GP . The condition MATH is implicit. Note that if MATH is a posynomial and MATH is a monomial, then MATH is a posynomial. Example: MATH \\begin align x, y, z &= \\arg\\min x,y,z x/y \\\\ \\text subject to: &1 \\leq x \\leq 2 \\\\ &x^3 + 2y/z \\leq \\sqrt y \\\\ &x/y = z \\end align MATH This can be rewritten in GP form: MATH \\begin align x, y, z &= \\arg\\min x,y,z xy^ -1 \\\\ \\text subject to: &x^ -1 \\leq 1 \\\\ & 1/2 x \\leq 1 \\\\ &x^3 y^ -1/2 + 2y^ 1/2 z^ -1 \\leq 1 \\\\ &xy^ -1 z^ -1 = 1 \\end align MATH This problem is clearly nonconvex since both the objective function and constraint functions are not convex. 5.2.3. Transforming GP to Convex Form GP can be transformed to convex form as follows: Let MATH , i.e., MATH . If MATH is a monomial function of MATH , then: MATH where MATH . Now, the function MATH is a convex function in MATH . The reader can prove by definition that the composition of two convex functions is a convex function. In this case, both the MATH function and the affine function are convex functions. Similarly, the posynomial in equation 25 can be written as: MATH where MATH and MATH . Now, the posynomial has been written as a sum of MATH functions of affine functions and is therefore a convex function, recall that the sum of convex functions is convex . The GP problem 26 is rewritten as: MATH \\begin align y &= \\arg\\min y \\sum k=1 ^ K 0 \\exp a 0k ^T y + b 0k \\\\ \\text subject to: &\\sum k=1 ^ K i \\exp a ik ^T y + b ik \\leq 1, \\quad i = 1, \\ldots, m \\quad \\quad 27 \\\\ &\\exp g j^T y + h j = 1, \\quad j = 1, \\ldots, p \\end align MATH where MATH , MATH and MATH . With the observation that the function MATH is a convex function if MATH are convex functions we omit the proof , we can rewrite problem 27 in convex form by taking the MATH of the functions as follows: GP in convex form: MATH \\begin align \\text minimize y \\quad &\\tilde f 0 y = \\log\\left \\sum k=1 ^ K 0 \\exp a 0k ^T y + b 0k \\right \\\\ \\text subject to: &\\tilde f i y = \\log\\left \\sum k=1 ^ K i \\exp a ik ^T y + b ik \\right \\leq 0, \\quad i = 1, \\ldots, m \\quad \\quad 28 \\\\ &\\tilde h j y = g j^T y + h j = 0, \\quad j = 1, \\ldots, p \\end align MATH Now we can say that GP is equivalent to a convex optimization problem because the objective function and inequality constraint functions in 28 are all convex functions, while the equality constraints are in affine form. This form is often called a geometric program in convex form to distinguish it from the original definition of GP .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_02_02_Geometric_Programming/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_01_01_LP_Simple_Algorithm",
    "title": "05-01-01 Linear Programming - Simplex Algorithm",
    "chapter": "05",
    "order": 9,
    "owner": "Hooncheol Shin",
    "lesson_type": "required",
    "content": "",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_01_01_LP_Simple_Algorithm/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter05/05_02_03_Linear_Regression_Statistical_View",
    "title": "05-02-03 Linear Regression from Statistical Perspective",
    "chapter": "05",
    "order": 10,
    "owner": "AI Assistant",
    "lesson_type": "",
    "content": "Linear Regression from Statistical Perspective In this lesson, we explore linear regression from a probabilistic and statistical viewpoint, demonstrating why minimizing the sum of squared errors MSE is not only intuitive but also theoretically justified through maximum likelihood estimation. 1. Probabilistic Interpretation of Linear Regression From a probabilistic perspective, we can prove that the estimates obtained from linear regression based on minimizing the sum of squared errors from the MSE function are completely natural and reasonable. Indeed, we assume that the target variable and input variables are related through the equation: MATH where MATH represents the random error that any equation has. These are factors that cannot be explained by the model. Since our estimate is unbiased, this random error is assumed to satisfy the properties according to the Gauss-Markov hypothesis: Assumption 1: Zero Mean Error The errors MATH are random variables with zero expectation: MATH Assumption 2: Uncorrelated Errors The random errors have no correlation: MATH Assumption 3: Constant Variance Homoscedasticity The variance of the random error is invariant: MATH Assumption 4: Independence of Errors and Features The random error MATH and the input variables MATH have no correlation: MATH 2. Gaussian Distribution of Errors Under these assumptions, the random errors MATH form a Gaussian normal distribution with mean 0 and variance MATH , denoted as MATH . The probability density function at each point MATH is: MATH Substituting MATH into the probability density function, we get: MATH The notation MATH indicates the probability of MATH corresponding to MATH , parameterized by MATH . Here, MATH is known and is not considered as a condition of MATH , hence we use ; instead of ,. 3. Maximum Likelihood Estimation From a probabilistic perspective, MATH is a function dependent on the input data MATH when the weight MATH is known. When viewing probability from the perspective of a function with respect to MATH , we call it the likelihood function: MATH According to condition 2 of the Gauss-Markov hypothesis, the errors are independent, so the joint probability of the data equals the product of the probability densities of each data point: MATH \\begin align L \\mathbf w &= \\prod i=1 ^ n p y i \\mid \\mathbf x i; \\mathbf w \\\\ &= \\prod i=1 ^ n \\frac 1 \\sqrt 2\\pi \\sigma^2 \\exp \\left -\\frac \\epsilon i^2 2\\sigma^2 \\right \\end align MATH The likelihood function reflects the probabilistic relationship between MATH and MATH . To find MATH such that this relationship is most appropriate, according to Maximum Likelihood Estimation, we choose MATH such that MATH is maximized. 4. Log-Likelihood Optimization Taking the logarithm of both sides to simplify the optimization problem: MATH \\begin align \\hat \\mathbf w &= \\arg \\max \\log L \\mathbf w \\\\ &= \\arg \\max \\log \\left \\prod i=1 ^ n \\frac 1 \\sqrt 2\\pi \\sigma^2 \\exp \\left -\\frac \\epsilon i^2 2\\sigma^2 \\right \\right \\\\ &= \\arg \\max \\sum i=1 ^ n \\log \\left \\frac 1 \\sqrt 2\\pi \\sigma^2 \\exp \\left -\\frac \\epsilon i^2 2\\sigma^2 \\right \\right \\\\ &= \\arg \\max \\sum i=1 ^ n \\left -\\frac \\epsilon i^2 2\\sigma^2 - \\log \\sqrt 2\\pi \\sigma^2 \\right \\\\ &= \\arg \\max \\left -\\frac 1 2\\sigma^2 \\sum i=1 ^ n \\epsilon i^2 - n \\log \\sqrt 2\\pi \\sigma^2 \\right \\end align MATH Since MATH and MATH are constants, optimizing the above function is equivalent to minimizing: MATH This is equivalent to minimizing the MSE function: MATH 5. Theoretical Justification Thus, from a probabilistic perspective, we have proven that linear regression based on minimizing the sum of squared errors is equivalent to optimizing the likelihood function. When the conditions of the Gauss-Markov hypothesis are satisfied, our estimate is the best linear unbiased estimator BLUE . Assumptions about confidence intervals for predicted values and evaluation of the significance of weights through P-values can be made based on the normal distribution. Maximum Likelihood Estimation Visualization Visualization: Blue dots are data points, red line is the fitted line, and the curves show the Gaussian distributions of errors. Parameters Noise Level σ : 0.5 True Slope: 1.0 True Intercept: 0.0 Generate New Data MLE Estimates: Slope: 0.000 Intercept: 0.000 Log-Likelihood: 0.000 6. Training Linear Regression Models with sklearn Sklearn is a comprehensive Python library for data science, supporting training of most machine learning models, building pipelines, data normalization, and performing cross-validation. In this section, we will learn how to train linear regression models on sklearn. Returning to the previous problem, if we add information about distance to city center: MATH then the problem becomes multivariate regression. The process of building and training the model includes the steps: 1. Data collection 2. Data cleaning 3. Feature selection 4. Data normalization 5. Train/test split 6. Model training and evaluation In this problem, we focus on step 6 to understand how to train the model. Sklearn Linear Regression Example import numpy as np from sklearn.linear model import LinearRegression from sklearn.model selection import train test split from sklearn.metrics import mean squared error, r2 score import matplotlib.pyplot as plt Sample data: house prices Features: area, distance to center X = np.array 50, 20 , 60, 18 , 70, 17 , 80, 16 , 90, 15 , 100, 14 , 110, 12 , 120, 10 , 130, 8 , 140, 7 , 150, 5 , 160, 2 , 170, 1 Target: prices in thousands y = np.array 150, 180, 210, 240, 270, 300, 330, 360, 390, 420, 450, 480, 510 Split data X train, X test, y train, y test = train test split X, y, test size=0.3, random state=42 Create and train model model = LinearRegression model.fit X train, y train Make predictions y pred = model.predict X test Evaluate model mse = mean squared error y test, y pred r2 = r2 score y test, y pred print f\"Coefficients: model.coef \" print f\"Intercept: model.intercept \" print f\"MSE: mse:.2f \" print f\"R²: r2:.3f \" Model Results Run Sklearn Example Model Parameters: Area coeff: -- Distance coeff: -- Intercept: -- Performance: MSE: -- R²: -- Prediction Calculator Area m² : Distance to center km : Predict Price Predicted Price: -- Key Insights 1. Theoretical Foundation : Maximum likelihood estimation provides a solid theoretical foundation for why we minimize squared errors in linear regression. 2. Gaussian Assumptions : The effectiveness of linear regression relies on the Gauss-Markov assumptions, particularly that errors are normally distributed. 3. Best Linear Unbiased Estimator : Under the right conditions, OLS provides the BLUE - the most efficient unbiased linear estimator. 4. Practical Implementation : Modern tools like sklearn make it easy to implement these theoretical concepts in practice. 5. Model Evaluation : Understanding the statistical foundation helps in proper model evaluation using metrics like R², confidence intervals, and p-values. // MLE Visualization class MLEVisualization constructor this.canvas = document.getElementById 'mleCanvas' ; this.ctx = this.canvas.getContext '2d' ; this.width = this.canvas.width; this.height = this.canvas.height; // Parameters this.trueSlope = 1.0; this.trueIntercept = 0.0; this.noiseLevel = 0.5; this.dataPoints = ; this.setupControls ; this.generateData ; this.draw ; setupControls const noiseSlider = document.getElementById 'noise-slider' ; const slopeSlider = document.getElementById 'slope-slider' ; const interceptSlider = document.getElementById 'intercept-slider' ; const generateBtn = document.getElementById 'generate-data' ; noiseSlider.addEventListener 'input', e => this.noiseLevel = parseFloat e.target.value ; document.getElementById 'noise-value' .textContent = this.noiseLevel.toFixed 1 ; this.generateData ; this.draw ; ; slopeSlider.addEventListener 'input', e => this.trueSlope = parseFloat e.target.value ; document.getElementById 'true-slope-value' .textContent = this.trueSlope.toFixed 1 ; this.generateData ; this.draw ; ; interceptSlider.addEventListener 'input', e => this.trueIntercept = parseFloat e.target.value ; document.getElementById 'true-intercept-value' .textContent = this.trueIntercept.toFixed 1 ; this.generateData ; this.draw ; ; generateBtn.addEventListener 'click', => this.generateData ; this.draw ; ; generateData this.dataPoints = ; const n = 20; for let i = 0; i x: x + 2.5 this.width / 5, y: this.height - y + 2.5 this.height / 5 ; // Draw axes this.ctx.strokeStyle = ' ddd'; this.ctx.lineWidth = 1; this.ctx.beginPath ; this.ctx.moveTo 0, this.height / 2 ; this.ctx.lineTo this.width, this.height / 2 ; this.ctx.moveTo this.width / 2, 0 ; this.ctx.lineTo this.width / 2, this.height ; this.ctx.stroke ; // Draw fitted line this.ctx.strokeStyle = ' ff4444'; this.ctx.lineWidth = 2; this.ctx.beginPath ; const start = transform -2, mle.slope -2 + mle.intercept ; const end = transform 2, mle.slope 2 + mle.intercept ; this.ctx.moveTo start.x, start.y ; this.ctx.lineTo end.x, end.y ; this.ctx.stroke ; // Draw data points and error distributions for const point of this.dataPoints const pos = transform point.x, point.y ; // Draw Gaussian error distribution const predicted = mle.slope point.x + mle.intercept; const errorCenter = transform point.x, predicted ; this.ctx.strokeStyle = ' cccccc'; this.ctx.lineWidth = 1; this.ctx.beginPath ; for let i = 0; i this.runSklearnExample ; predBtn.addEventListener 'click', => this.makePrediction ; runSklearnExample // Simulate sklearn linear regression const X, y = this.data; // Calculate means const meanX1 = X.reduce sum, row => sum + row 0 , 0 / X.length; const meanX2 = X.reduce sum, row => sum + row 1 , 0 / X.length; const meanY = y.reduce sum, val => sum + val, 0 / y.length; // Calculate coefficients using normal equation let sumX1X1 = 0, sumX2X2 = 0, sumX1X2 = 0; let sumX1Y = 0, sumX2Y = 0; for let i = 0; i MATH prediction.toFixed 0 k ; // Initialize when DOM is loaded document.addEventListener 'DOMContentLoaded', function new MLEVisualization ; new SklearnDemo ; ; mle-demo canvas border-radius: 5px; sklearn-demo pre max-height: 400px; overflow-y: auto; .demo-container margin: 20px 0; input type=\"range\" -webkit-appearance: none; appearance: none; height: 5px; background: ddd; outline: none; border-radius: 5px; input type=\"range\" ::-webkit-slider-thumb -webkit-appearance: none; appearance: none; width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; input type=\"range\" ::-moz-range-thumb width: 15px; height: 15px; background: 007bff; cursor: pointer; border-radius: 50%; border: none;",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_02_03_Linear_Regression_Statistical_View/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_00_gradient_descent",
    "title": "06 Gradient Descent",
    "chapter": "06",
    "order": 1,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "In this chapter, we explore Gradient Descent , one of the most fundamental and important methods in optimization. In optimization algorithms, choosing the search direction and step size is crucial for convergence speed and success. Gradient descent moves in the direction of the negative gradient. The step size can be fixed or adaptively chosen, and we will discuss both approaches in this chapter. For gradient descent to converge, certain preconditions must be met. If these conditions are satisfied, we can analyze how quickly gradient descent converges. If strong convexity holds, convergence is even faster, and we will examine the convergence rate in such cases. We will also look at applications of gradient descent, including gradient boosting and stochastic gradient descent.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_00_gradient_descent/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_01_gradient_descent",
    "title": "06-01 Gradient Descent",
    "chapter": "06",
    "order": 2,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "Gradient descent is the simplest algorithm for solving unconstrained convex and differentiable optimization problems. > MATH > where MATH is differentiable and MATH . The optimal value is MATH , and the optimizer is MATH . Why Gradient Descent Matters in Data Science Gradient descent is the workhorse of machine learning! It's the algorithm behind: - Training neural networks : Backpropagation uses gradient descent to update weights - Linear regression : Finding optimal coefficients to minimize MSE - Logistic regression : Optimizing parameters for classification - Deep learning : Training complex models with millions of parameters - Recommendation systems : Learning user preferences and item features Key insight : Every time you see \"training\" or \"learning\" in ML, gradient descent or its variants is likely involved! Gradient Descent for Single Variable Functions For functions of a single variable MATH , gradient descent simplifies significantly. The gradient becomes the derivative, and the update rule becomes: > MATH where MATH is the derivative of MATH at point MATH . Geometric Interpretation In the single variable case, the derivative MATH represents the slope of the tangent line at point MATH : - If MATH , the function is increasing, so we move left subtract a positive value - If MATH x^ k = x^ k-1 - t x^ k-1 - 2 MATH Starting from MATH with step size MATH : > MATH > MATH > MATH > ... The sequence converges to MATH , which is the global minimum. Step Size Selection The choice of step size MATH is crucial: - Too small : Convergence is very slow - Too large : The algorithm may overshoot and diverge - Optimal : For quadratic functions MATH with MATH , the optimal step size is MATH Interactive Visualization Step Size t : 0.1 Starting Point: -3 Start Animation Reset Single Step Iteration: 0, x = -3.000, f x = 13.500, f' x = -5.000 class SingleVarGradientDescent constructor this.canvas = document.getElementById 'gradient-canvas' ; this.ctx = this.canvas.getContext '2d' ; this.stepSizeSlider = document.getElementById 'step-size-slider' ; this.startPointSlider = document.getElementById 'start-point-slider' ; this.stepSizeValue = document.getElementById 'step-size-value' ; this.startPointValue = document.getElementById 'start-point-value' ; this.iterationInfo = document.getElementById 'iteration-info' ; // Animation state this.isAnimating = false; this.currentX = -3; this.iteration = 0; this.history = ; this.animationId = null; // Function parameters: f x = 0.5 x - 2 ^2 + 1 this.a = 0.5; this.b = 2; this.c = 1; this.setupEventListeners ; this.reset ; setupEventListeners this.stepSizeSlider.addEventListener 'input', e => this.stepSizeValue.textContent = e.target.value; ; this.startPointSlider.addEventListener 'input', e => this.startPointValue.textContent = e.target.value; if !this.isAnimating this.reset ; ; document.getElementById 'start-animation' .addEventListener 'click', => this.startAnimation ; ; document.getElementById 'reset-animation' .addEventListener 'click', => this.reset ; ; document.getElementById 'step-once' .addEventListener 'click', => this.singleStep ; ; // Function: f x = 0.5 x - 2 ^2 + 1 f x return this.a Math.pow x - this.b, 2 + this.c; // Derivative: f' x = x - 2 fprime x return 2 this.a x - this.b ; // Convert x coordinate to canvas coordinate xToCanvas x const xMin = -5, xMax = 5; return x - xMin / xMax - xMin this.canvas.width; // Convert y coordinate to canvas coordinate yToCanvas y const yMin = 0, yMax = 15; return this.canvas.height - y - yMin / yMax - yMin this.canvas.height; // Convert canvas x to actual x canvasToX canvasX const xMin = -5, xMax = 5; return xMin + canvasX / this.canvas.width xMax - xMin ; drawFunction this.ctx.strokeStyle = ' 2196F3'; this.ctx.lineWidth = 2; this.ctx.beginPath ; for let canvasX = 0; canvasX if !this.isAnimating return; if Math.abs this.fprime this.currentX > 1e-6 && this.iteration Gradient Descent Method for Multivariables function Gradient descent starts from an initial point MATH and iteratively updates as follows until a stopping criterion is met: > MATH , MATH Pseudocode: > Given a starting point MATH > Repeat > 1. Determine descent direction MATH . > 2. Line search: choose step size MATH . > 3. Update MATH . > Until stopping criterion is satisfied Examples The figure below shows gradient descent on a convex function. In this case, the local minimum is also the global minimum. Fig 1 Gradient descent in convex functions 3 The next figure shows gradient descent on a non-convex function. Here, the initial point determines which local minimum is reached. Fig 2 Gradient descent in non-convex functions 3 Gradient Descent Interpretation Gradient descent can be interpreted as choosing the next point by minimizing a quadratic approximation of the function. For a function MATH , the second-order Taylor expansion around MATH is: > MATH > MATH If we approximate the Hessian MATH by MATH , then: > MATH where MATH is the step size. Thus, in gradient descent, the function is approximated by a quadratic function whose Hessian matrix has eigenvalues equal to the reciprocal of the step size. The term MATH represents a linear approximation of MATH , and MATH serves as a proximity term indicating how close MATH is to MATH . The next position is chosen as the minimum of this approximated quadratic function. Setting the gradient of MATH to zero to find the next position MATH leads to: > MATH In the illustration below, the blue dot represents the current position MATH , and the red dot represents the next position MATH . The curve below is the actual function MATH , and the curve above is the quadratic approximation of MATH . Hence, the red dot indicates the minimum of the quadratic approximation. MATH The proximity of the next position MATH to the current position MATH is influenced by the weight of the proximity term MATH . A smaller MATH results in a larger weight for the proximity term, leading to smaller steps. This process can be expressed as: > \\begin align x^+ = \\underset y \\arg \\min \\ f x + \\nabla f x ^T y - x + \\frac 1 2t \\parallel y - x \\parallel 2^2 \\end align",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_01_gradient_descent/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_02_how_to_choose_step_sizes",
    "title": "06-02 How to choose step sizes",
    "chapter": "06",
    "order": 3,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "When performing gradient descent, the step size determines how the variable MATH is updated and affects the speed and success of finding the optimal value. This section introduces three main methods for choosing the step size to help gradient descent find the optimum more quickly: - Fixed step size - Backtracking line search - Exact line search",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_02_how_to_choose_step_sizes/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_02_01_fixed_step_size",
    "title": "06-02-01 Fixed step size",
    "chapter": "06",
    "order": 4,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The simplest way to choose the step size in gradient descent is to use a fixed value for all iterations: MATH for MATH . However, the convergence and behavior depend heavily on the choice of MATH . For example, in the figure below, gradient descent is applied to MATH with different step sizes: Fig 1 Step size scenarios 3 In case A, the step size MATH is too large, causing divergence after 8 steps. The minimum cannot be reached. In case B, the step size MATH is too small, so convergence is very slow and the minimum is not reached even after 100 steps. In case C, the step size is \"appropriate,\" and convergence is achieved in about 40 steps. How to find this \"appropriate\" value is discussed later in this chapter.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_02_01_fixed_step_size/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_02_02_backtracking_line_search",
    "title": "06-02-02 Backtracking line search",
    "chapter": "06",
    "order": 5,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Introduction: The Step Size Problem In gradient descent, we use the update rule: MATH where: - MATH is our current position at iteration MATH - MATH is the gradient tells us the direction of steepest ascent - MATH is the step size how far we move in that direction Why Do We Need Adaptive Step Sizes? The Problem with Fixed Step Sizes: - If MATH is too small → we make slow progress many tiny steps - If MATH is too large → we might overshoot the minimum or even diverge - A fixed step size can't adapt to different regions of the function The Solution: Use an adaptive step size that automatically adjusts based on the function's behavior. One of the most popular methods is backtracking line search . What is Backtracking Line Search? The Core Idea: Start with a large step size and progressively make it smaller until we find a \"good enough\" step. Think of it like this: 1. Try a big step - Maybe we can make fast progress! 2. Check if it's too big - Did we overshoot or not improve enough? 3. If yes, backtrack - Make the step smaller and try again 4. Repeat until satisfied - Stop when we find an acceptable step Fig1 Backtracking Line Search: The red line shows the \"acceptance threshold\" - if our function value blue curve goes below this line, we accept the step size 3 The Mathematical Condition We accept a step size MATH if it provides sufficient decrease in the function value: MATH where: - Left side: The actual function value after taking the step - Right side: The current function value minus a threshold for \"sufficient decrease\" - MATH is a parameter that controls how much decrease we require typically MATH Intuition: The right side represents a linear approximation of how much the function should decrease. If the actual decrease left side is at least this much, we accept the step. The Backtracking Line Search Algorithm Parameters - MATH : Controls how much decrease we require typically MATH - MATH : Controls how much we shrink the step size when backtracking typically MATH or MATH Algorithm Steps Input: Current point MATH , gradient MATH Output: Good step size MATH 1. Initialize: Set MATH start with a full step 2. Check the condition: While the sufficient decrease condition is NOT satisfied: MATH 3. Backtrack: Set MATH shrink the step size 4. Update: Use the final MATH to update: MATH Step-by-Step Example Let's say we're at point MATH with gradient MATH : 1. Try MATH : Check if MATH - If YES → Accept MATH - If NO → Continue to step 2 2. Try MATH : Check if MATH - If YES → Accept MATH - If NO → Continue to step 3 3. Try MATH : Check if MATH - And so on... Why Does This Work? The Mathematical Intuition The Sufficient Decrease Condition Explained The condition MATH can be understood as: Left side: MATH = The actual function value after taking the step Right side: MATH = Current value minus expected decrease The expected decrease MATH comes from the linear approximation : MATH We require the actual decrease to be at least a fraction MATH of this predicted decrease. Why This Guarantees Progress 1. Prevents tiny steps: The condition ensures we don't accept arbitrarily small step sizes 2. Prevents overshooting: If we overshoot, the function value won't decrease enough 3. Balances speed and stability: We get reasonably large steps while maintaining convergence Performance Comparison Fig2 Convergence comparison: Backtracking line search adaptive vs Fixed step size. Notice how the adaptive method converges much faster! 3 Practical Example: Minimizing a Quadratic Function Let's work through a detailed example to see backtracking line search in action! Example 1: Simple Quadratic Lucky Case Problem: Minimize MATH starting from MATH Setup: - Parameters: MATH , MATH - Gradient: MATH - Optimal solution: MATH Iteration 1: - Current: MATH , MATH , MATH - Try MATH : - New point: MATH - Function value: MATH - Check sufficient decrease: MATH - Left side: MATH - Right side: MATH - MATH ✓ Accept MATH - Update: MATH Result: We reached the optimum in one step! This is because the quadratic function is \"well-behaved.\" Example 2: More Realistic Scenario Problem: Minimize MATH starting from MATH This function has local minima and is more challenging. Let's see how backtracking handles it: Setup: - Parameters: MATH , MATH - Gradient: MATH Iteration 1: - Current: MATH , MATH , MATH Step 1: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Too big step! Step 2: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Still too big! Step 3: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Still too big! Step 4: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Still too big! Step 5: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Getting closer! Step 6: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Almost there! Step 7: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Very close! Step 8: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ So close! Step 9: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Almost! Step 10: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Getting very small steps now... Step 11: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 12: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 13: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 14: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 15: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 16: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 17: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 18: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 19: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Step 20: Try MATH - New point: MATH - Function value: MATH - Check: MATH ✗ Wait, let me recalculate this more carefully. I think there might be an error in my calculations. Let me redo this with a simpler but more accurate approach. Example 2 Corrected : A More Realistic Function Problem: Minimize MATH starting from MATH Setup: - Parameters: MATH , MATH - Gradient: MATH - At MATH : MATH , MATH Iteration 1: Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✗ Way too big! Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✗ Still too big! Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✗ Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✗ Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✗ Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✗ Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✗ Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✗ Try MATH : - New point: MATH - Function value: MATH - Check: MATH ✓ Accept! Result: After 8 backtracking steps, we accept MATH and update to MATH Key Insights from These Examples 1. Simple functions like pure quadratics may converge in one step 2. Complex functions require multiple backtracking steps to find safe step sizes 3. The algorithm is robust - it automatically finds appropriate step sizes without manual tuning 4. Each backtracking iteration makes the step size smaller by factor MATH 5. The sufficient decrease condition prevents both overshooting and accepting tiny progress Why This Beats Fixed Step Sizes Let's see a head-to-head comparison! We'll minimize MATH starting from MATH . Function details: - Gradient: MATH - Optimal solution: MATH with MATH - Starting point: MATH , MATH Comparison: Fixed Step Size vs Backtracking Line Search Method 1: Fixed Small Step Size MATH | Iteration | MATH | MATH | MATH | MATH | |-----------|---------|-------------------|---------------------------------------|----------------| | 0 | 4.0 | 6.0 | 3.4 | 8.96 | | 1 | 3.4 | 4.8 | 2.92 | 6.4864 | | 2 | 2.92 | 3.84 | 2.536 | 5.548896 | | 3 | 2.536 | 3.072 | 2.2288 | 5.103194 | | 4 | 2.2288 | 2.4576 | 1.98304 | 4.835481 | | 5 | 1.98304 | 1.96608 | 1.786432 | 4.693904 | | ... | ... | ... | ... | ... | | 20 | 1.24537 | 0.49074 | 1.196296 | 4.192934 | | ... | ... | ... | ... | ... | | 50 | 1.049 | 0.098 | 1.0392 | 4.0038 | Result: After 50 iterations, we're still not very close to the optimum! Method 2: Fixed Large Step Size MATH | Iteration | MATH | MATH | MATH | MATH | |-----------|---------|-------------------|---------------------------------------|----------------| | 0 | 4.0 | 6.0 | -5.0 | 40 | | 1 | -5.0 | -12.0 | 13.0 | 148 | | 2 | 13.0 | 24.0 | -23.0 | 568 | | 3 | -23.0 | -48.0 | 49.0 | 2308 | | 4 | 49.0 | 96.0 | -95.0 | 9028 | Result: Diverges catastrophically! The algorithm explodes and never converges. Method 3: Fixed Medium Step Size MATH | Iteration | MATH | MATH | MATH | MATH | |-----------|---------|-------------------|---------------------------------------|----------------| | 0 | 4.0 | 6.0 | 1.0 | 4.0 | | 1 | 1.0 | 0.0 | 1.0 | 4.0 | Result: Lucky guess! Converges in 2 steps, but this only works because we happened to choose exactly the right step size for this particular function. Method 4: Backtracking Line Search MATH , MATH Iteration 1: - Current: MATH , MATH , MATH - Try MATH : MATH , MATH - Check: MATH ✗ Too big! - Try MATH : MATH , MATH - Check: MATH ✗ Still too big! - Try MATH : MATH , MATH - Check: MATH ✗ Still too big! - Try MATH : MATH , MATH - Check: MATH ✓ Accept! - Update: MATH Iteration 2: - Current: MATH , MATH , MATH - Try MATH : MATH , MATH - Check: MATH ✗ Very close! - Try MATH : MATH , MATH - Check: MATH ✗ - Try MATH : MATH , MATH - Check: MATH ✗ - Try MATH : MATH , MATH - Check: MATH ✗ - Try MATH : MATH , MATH - Check: MATH ✓ Accept! - Update: MATH Iteration 3: - Current: MATH , MATH , MATH - The gradient is very small, so we're very close to the optimum - After one more step: MATH essentially converged! Result: Converges to the optimum in just 3 iterations ! Performance Summary | Method | Iterations to Convergence | Final Error | Comments | |--------|---------------------------|-------------|----------| | Fixed Small MATH | 50+ | High | Very slow, conservative | | Fixed Large MATH | ∞ | ∞ | Diverges completely | | Fixed Medium MATH | 2 | Very Low | Lucky guess only! | | Backtracking | 3 | Very Low | Robust and fast | Key Insights 1. Fixed small steps: Safe but painfully slow 2. Fixed large steps: Fast when they work, catastrophic when they don't 3. Fixed \"perfect\" steps: Impossible to find without knowing the answer 4. Backtracking: Automatically finds good step sizes, robust across different functions The Real Power: Robustness Across Functions The magic of backtracking line search is that it works well for ANY function: - Gentle functions: Takes appropriately large steps for fast progress - Steep functions: Automatically reduces step size to prevent overshooting - Varying functions: Adapts step size throughout the optimization process - Unknown functions: No need to guess the \"right\" step size beforehand Bottom line: Backtracking line search gives you the speed of large steps when safe, and the stability of small steps when necessary - automatically! Advantages and Practical Considerations Advantages: 1. Automatic adaptation: No need to manually tune step sizes 2. Convergence guarantees: Theoretical guarantees for convex functions 3. Simple implementation: Easy to code and understand 4. Robust: Works well across different types of problems Practical Tips: - Typical parameter values: MATH , MATH or MATH - Computational cost: Each backtracking step requires one function evaluation - When to use: Especially useful when the function's \"landscape\" varies significantly Summary Backtracking line search solves the fundamental problem of choosing good step sizes in gradient descent: 1. Start optimistic: Try a large step size MATH 2. Test rigorously: Check if the step provides sufficient decrease 3. Adapt intelligently: If not, shrink the step size and try again 4. Guarantee progress: The mathematical condition ensures we always make meaningful progress This simple yet powerful technique transforms gradient descent from a method that requires careful tuning into a robust, adaptive algorithm that works well out of the box. Next time you implement gradient descent, consider adding backtracking line search - your algorithm will converge faster and be much more reliable!",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_02_02_backtracking_line_search/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_02_03_exact_line_search",
    "title": "06-02-03 Exact line search",
    "chapter": "06",
    "order": 6,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Another way to adapt the step size in gradient descent is exact line search . What is Exact Line Search? In exact line search, we move in the direction of the negative gradient and choose the best possible step size. For the following expression, if MATH , the next step MATH moves away from the current position. By varying MATH , we find the step size MATH that minimizes MATH : > MATH Exact line search is efficient for single-variable minimization problems, but for multivariable problems, searching exhaustively for the optimal step size is often impractical. In practice, backtracking is more efficient and commonly used.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_02_03_exact_line_search/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_03_convergence_analysis",
    "title": "06-03 Convergence analysis",
    "chapter": "06",
    "order": 7,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "In this section, we analyze the convergence of Gradient Descent. We will examine the error bounds for convergence in the case of a fixed step size and in the case of backtracking. In addition, we will analyze the error bounds when the strong convexity condition is satisfied.\"",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_03_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_03_01_convex_function_quadratic_upper_bound",
    "title": "06-03-01 Convex function quadratic upper bound",
    "chapter": "06",
    "order": 8,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Introduction The quadratic upper bound is a fundamental property of smooth convex functions that provides a crucial tool for analyzing and designing optimization algorithms. This property establishes that any smooth convex function can be bounded above by a quadratic function, which has important implications for convergence analysis of gradient-based methods. Motivation Why do we care about quadratic upper bounds? 1. Algorithm Design : Many optimization algorithms like gradient descent rely on local approximations of the objective function. The quadratic upper bound provides a systematic way to construct these approximations. 2. Convergence Analysis : The quadratic upper bound allows us to prove convergence rates for optimization algorithms by bounding how much the function can change. 3. Step Size Selection : The Lipschitz constant MATH in the bound directly determines safe step sizes for gradient descent. Quadratic Upper Bound Property Theorem : If MATH is convex and MATH is Lipschitz continuous with constant MATH , then MATH satisfies the quadratic upper bound: > MATH \\begin align f y & \\le f x + \\nabla f x ^T y-x + \\frac L 2 \\| y - x \\|^2 2 \\quad \\forall x, y \\end align MATH Geometric Interpretation This inequality states that: - The linear approximation MATH first-order Taylor expansion underestimates MATH due to convexity - The quadratic upper bound MATH overestimates MATH - The function MATH is \"sandwiched\" between these two bounds Equivalent Characterization Corollary : For any smooth convex function MATH , the following function is convex: > MATH \\begin align g x & = \\frac L 2 \\| x \\|^2 2 - f x \\quad \\text where dom g = dom f \\end align MATH This means that MATH can be written as the difference between a quadratic function and a convex function. Proof We will prove both the quadratic upper bound and the equivalent characterization. The proof relies on two key properties of smooth convex functions. Prerequisites Definition 1 Monotone Operator : In vector space MATH , an operator MATH is monotone if: > MATH Property 1 Monotonicity of Gradient : If MATH is convex and differentiable, then MATH is a monotone operator: > MATH Property 2 Lipschitz Continuity : MATH is Lipschitz continuous with constant MATH : > MATH Proof of Quadratic Upper Bound Step 1 : Consider the function MATH for MATH . By the fundamental theorem of calculus: > MATH \\begin align f y - f x &= h 1 - h 0 = \\int 0^1 h' t dt \\\\ &= \\int 0^1 \\nabla f x + t y-x ^T y-x dt \\end align MATH Step 2 : We can rewrite this as: > MATH \\begin align f y - f x &= \\nabla f x ^T y-x + \\int 0^1 \\nabla f x + t y-x - \\nabla f x ^T y-x dt \\end align MATH Step 3 : Using the Cauchy-Schwarz inequality and Lipschitz continuity: > MATH \\begin align & \\nabla f x + t y-x - \\nabla f x ^T y-x \\\\ &\\le \\|\\nabla f x + t y-x - \\nabla f x \\| \\cdot \\|y-x\\| \\\\ &\\le L \\cdot t\\|y-x\\| \\cdot \\|y-x\\| = Lt\\|y-x\\|^2 \\end align MATH Step 4 : Integrating over MATH : > MATH \\begin align f y - f x &\\le \\nabla f x ^T y-x + \\int 0^1 Lt\\|y-x\\|^2 dt \\\\ &= \\nabla f x ^T y-x + L\\|y-x\\|^2 \\int 0^1 t dt \\\\ &= \\nabla f x ^T y-x + \\frac L 2 \\|y-x\\|^2 \\end align MATH Therefore: MATH Proof of Equivalent Characterization Theorem : The function MATH is convex. Proof : We need to show that MATH positive semidefinite . Since MATH , we have: - MATH - MATH For any vector MATH , we need to show MATH : > MATH \\begin align v^T \\nabla^2 g x v &= v^T LI - \\nabla^2 f x v \\\\ &= L\\|v\\|^2 - v^T \\nabla^2 f x v \\end align MATH From the Lipschitz continuity of MATH , we can show that MATH , which means: MATH Therefore: MATH This proves that MATH is convex. MATH Examples Example 1: Quadratic Function Consider MATH where MATH positive semidefinite . - MATH - MATH - If MATH , then MATH The quadratic upper bound becomes: MATH For quadratic functions, this bound is tight when MATH . Example 2: Logistic Loss Consider the logistic loss MATH where MATH . - MATH - MATH Since MATH for all MATH , we have: MATH Therefore, MATH and the quadratic upper bound is: MATH Example 3: Least Squares For MATH where MATH : - MATH - MATH - MATH largest singular value squared The quadratic upper bound is: MATH Applications in Optimization 1. Gradient Descent Step Size The quadratic upper bound directly determines the maximum safe step size for gradient descent. If we use step size MATH , then: MATH When MATH , the right-hand side simplifies to: MATH This guarantees sufficient decrease at each iteration. 2. Convergence Rate Analysis For gradient descent with step size MATH , the quadratic upper bound enables us to prove: MATH where MATH is the strong convexity parameter. This gives linear convergence with rate MATH . 3. Proximal Gradient Methods In composite optimization MATH where MATH is smooth and MATH is non-smooth, the quadratic upper bound of MATH leads to the proximal gradient update: MATH where MATH ensures convergence. 4. Accelerated Methods Advanced methods like Nesterov's accelerated gradient and FISTA also rely on the quadratic upper bound to achieve optimal MATH convergence rates for smooth convex functions. Key Takeaways 1. Fundamental Property : The quadratic upper bound is a cornerstone of smooth convex optimization theory. 2. Algorithm Design : It provides the theoretical foundation for choosing step sizes in gradient-based methods. 3. Convergence Analysis : It enables rigorous proofs of convergence rates for optimization algorithms. 4. Practical Impact : Understanding MATH helps practitioners tune algorithms effectively. 5. Geometric Intuition : It formalizes the idea that smooth convex functions don't \"curve too much\" - they're bounded above by parabolas. Interactive Visualization The following diagram illustrates the quadratic upper bound property: f y | | Quadratic Upper Bound | f x + ∇f x ᵀ y-x + L/2 ||y-x||² | / | / | / True function f y |/ / /| / / | / / | / Linear Approximation / | / f x + ∇f x ᵀ y-x / | / / |/ / x y / | → | Key Observations : - The linear approximation tangent line lies below MATH due to convexity - The quadratic upper bound lies above MATH - The true function MATH is sandwiched between these bounds - The gap between bounds depends on MATH Lipschitz constant and MATH Computational Considerations Estimating the Lipschitz Constant In practice, finding the exact Lipschitz constant MATH can be challenging. Common approaches: 1. Theoretical Analysis : For specific function classes quadratic, logistic, etc. 2. Spectral Methods : MATH when Hessian is available 3. Adaptive Methods : Start with estimate and adjust based on sufficient decrease condition 4. Line Search : Use backtracking to find appropriate step sizes Practical Algorithm Implementation python def gradient descent with lipschitz f, grad f, x0, L estimate=1.0, max iter=1000 : \"\"\" Gradient descent with adaptive Lipschitz constant estimation \"\"\" x = x0 L = L estimate for k in range max iter : grad = grad f x Try step with current L estimate alpha = 1.0 / L x new = x - alpha grad Check sufficient decrease condition if f x new <= f x - 0.5 alpha np.linalg.norm grad 2: x = x new Accept step else: L = 2 Increase L estimate and retry continue Optional: decrease L if step was very conservative if k % 10 == 0: L = 0.9 return x This adaptive approach ensures convergence while automatically tuning the Lipschitz constant estimate.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_03_01_convex_function_quadratic_upper_bound/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_03_02_convergence_analysis_and_proof",
    "title": "06-03-02 Convergence analysis & Proof",
    "chapter": "06",
    "order": 9,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Suppose MATH is convex and differentiable with MATH , and MATH is Lipschitz continuous with constant MATH : > MATH for any MATH Reference: Wikipedia: Lipschitz continuity https://en.wikipedia.org/wiki/Lipschitz continuity Convergence Theorem Gradient descent with fixed step size MATH satisfies: > MATH \\begin align f x^ k - f^ \\le \\frac \\| x^ 0 - x^ \\|^2 2 2tk \\end align MATH With fixed step size, the convergence rate is MATH . To achieve MATH , MATH iterations are needed. Proof If MATH is Lipschitz continuous and MATH is convex, then MATH has a quadratic upper bound see 06-03-02 % multilang post url contents/chapter06/21-03-20-06 03 02 convex function quadratic upper bound % . > MATH \\begin align f y \\le f x + \\nabla f x ^T y-x + \\frac L 2 \\| y - x \\|^2 2 \\quad \\forall x, y \\end align MATH For gradient descent MATH , substitute MATH : > MATH \\begin align f x^+ & \\le f x - t 1 - \\frac Lt 2 \\| \\nabla f x \\|^2 2 \\\\\\ & \\le f x - \\frac t 2 \\| \\nabla f x \\|^2 2 \\quad \\text if t \\le 1/L \\end align MATH Thus, MATH and gradient descent converges.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_03_02_convergence_analysis_and_proof/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_03_03_convergence_analysis_for_backtracking",
    "title": "06-03-03 Convergence analysis for backtracking",
    "chapter": "06",
    "order": 10,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "Suppose MATH is convex and differentiable with MATH , and MATH is Lipschitz continuous with constant MATH : > MATH for any MATH Reference: Wikipedia: Lipschitz continuity https://en.wikipedia.org/wiki/Lipschitz continuity Convergence Theorem Gradient descent with backtracking line search satisfies: > MATH \\begin align f x^ k - f^ \\le \\frac \\| x^ 0 - x^ \\| 2^2 2 t min k , \\quad t min = \\min \\ 1, \\beta/L \\ \\end align MATH The convergence rate for backtracking line search is similar to that for fixed step size, with the step size MATH replaced by MATH . If MATH is not too small, the performance is comparable to fixed step size MATH vs. MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_03_03_convergence_analysis_for_backtracking/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_03_04_convergence_analysis_under_strong_convexity",
    "title": "06-03-04 Convergence analysis under strong convexity",
    "chapter": "06",
    "order": 11,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "If MATH satisfies the following condition, it is strongly convex assuming MATH is twice differentiable and MATH : > MATH \\begin align f y \\ge f x + \\nabla f x ^T y-x + \\frac m 2 \\| y-x \\| 2^2, \\quad \\forall x, y \\end align MATH Here, MATH has a quadratic lower bound, and the constant MATH is determined by the minimum eigenvalue of the Hessian of MATH . For any convex function MATH , MATH is strongly convex if: > MATH \\begin align g x = f x - \\frac m 2 \\| x \\| 2^2 \\quad \\text is convex for all x \\text and m > 0 \\end align MATH Convergence Theorem Given Lipschitz continuity and strong convexity, the following theorem holds where MATH is the Lipschitz constant and MATH is the strong convexity constant : Gradient descent with fixed step size MATH or backtracking line search satisfies: > MATH \\begin align f x^ k - f^ \\le c^k \\frac L 2 \\| x^ 0 - x^ \\| 2^2, \\quad c = 1 - \\frac m L , \\quad 0 MATH \\begin align f x^+ \\le f x - \\frac 1 2L \\| \\nabla f x \\| 2^2 \\end align MATH Subtracting MATH from both sides: > MATH \\begin align f x^+ - f x^ \\le f x - f x^ - \\frac 1 2L \\| \\nabla f x \\| 2^2 \\end align MATH Since Gradient Descent satisfies the condition: > MATH \\begin align f x - f x^ \\le \\frac 1 2m \\| \\nabla f x \\| 2^2 \\end align MATH We can substitute to get: > MATH \\begin align f x^+ - f x^ & \\le f x - f x^ - \\frac m L f x - f x^ \\\\ & = 1 - \\frac m L f x - f x^ \\\\ & = c f x - f x^ \\\\ \\end align MATH Iterating this process gives: > MATH \\begin align f x^ k - f x^ \\le c^k f x^ 0 - f x^ \\\\ \\end align MATH From the Taylor expansion of the function with MATH and MATH : > MATH \\begin align f y \\le f x + \\nabla f x ^T y-x + \\frac L 2 \\lVert y - x \\rVert^2 2 \\space \\space \\forall x, y \\end align MATH And since the function is convex, we have: > MATH \\begin align f x^ 0 & \\le f x^ + \\nabla f x^ ^T x^ 0 - x^ + \\frac L 2 \\lVert x^ 0 - x^ \\rVert^2 2 \\\\\\\\ & = f x^ + \\frac L 2 \\lVert x^ 0 - x^ \\rVert^2 2 \\\\\\\\ \\end align MATH Rearranging gives: > MATH \\begin align f x^ 0 - f x^ & \\le \\frac L 2 \\lVert x^ 0 - x^ \\rVert^2 2 \\\\\\\\ \\end align MATH Substituting this into the previous inequality results in: > MATH \\begin align f x^ k - f x^ & \\le c^k \\frac L 2 \\lVert x^ 0 - x^ \\rVert^2 2 \\\\\\ \\end align MATH This proves the Convergence Theorem for Gradient Descent under Strong Convexity. Linear convergence When MATH is strongly convex, the convergence rate becomes MATH , which is exponential. To achieve MATH , it requires MATH iterations. Without strong convexity, it would require MATH iterations. The convergence rate MATH appears linear on a semi-log plot, as shown below. Fig 1 Linear convergence 1 Here, the constant MATH in MATH is given by MATH and depends on the condition number MATH . A larger condition number results in slower convergence where the condition number is the ratio of the largest eigenvalue to the smallest eigenvalue .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_03_04_convergence_analysis_under_strong_convexity/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_03_05_look_at_the_conditions_and_practicalities",
    "title": "06-03-05 A look at the conditions & Practicalities",
    "chapter": "06",
    "order": 12,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Lipschitz continuity & Strong convexity conditions Let's examine the conditions for Lipschitz continuity and Strong convexity using MATH as an example. Lipschitz continuity of MATH : This means MATH . Since MATH , we have MATH . Strong convexity of MATH : This means MATH . Since MATH , we have MATH . If MATH is an MATH matrix and MATH , then MATH , so MATH cannot be strongly convex. Even if MATH , the condition number MATH can be very large. If function MATH is strongly convex and has a Lipschitz gradient, then it satisfies the following. You can think of MATH as being sandwiched between two quadratic functions. > MATH and MATH Satisfying both conditions for all MATH can be very strong. However, if we think more carefully, we can see that this condition is only needed for the following sublevel set. > MATH Practicalities Stopping Criteria for Optimization Algorithms In practice, optimization algorithms need well-defined stopping criteria to determine when to terminate the iterative process. Here are the most common stopping conditions: 1. Gradient Magnitude Near Zero: This is the ideal stopping condition for unconstrained optimization problems, based on the fact that the gradient of the objective function equals zero at local extrema. MATH \\|\\nabla f \\mathbf x k \\| \\leq \\epsilon 1 MATH where MATH is the current point at iteration MATH , and MATH is a small positive threshold. If MATH is the solution, then MATH . If MATH is strongly convex, then: > MATH 2. Small Change in Objective Function Value: The algorithm stops when the objective function value no longer changes significantly between consecutive iterations. MATH |f \\mathbf x k+1 - f \\mathbf x k | \\leq \\epsilon 2 MATH where MATH is a small positive threshold. 3. Small Change in Variables Parameters : The algorithm stops when the model parameters no longer change significantly between iterations. MATH \\|\\mathbf x k+1 - \\mathbf x k\\| \\leq \\epsilon 3 MATH where MATH is a small positive threshold. 4. Maximum Number of Iterations: To prevent the algorithm from running indefinitely or for too long, an upper limit on the number of iterations is set. MATH k \\geq \\text MaxIterations MATH This is a safety stopping condition that ensures the algorithm will terminate within a reasonable time, even if it hasn't achieved perfect convergence. 5. Maximum Runtime: Similar to the maximum iteration limit, the algorithm can be stopped if it has been running for more than an allowed time period. MATH \\text ElapsedTime \\geq \\text MaxTime MATH 6. Combined Conditions: In practice, multiple stopping conditions are often used in combination. For example, the algorithm stops if any of the above conditions is satisfied. This helps balance between accuracy and computational efficiency. Reference Derivation Process The derivation process for the above equation is as follows. Since MATH satisfies Strong Convexity, there exists a constant MATH such that: > MATH \\begin align \\nabla^2 f x \\succeq mI \\\\ \\end align MATH Let's expand function MATH using the second-order Taylor series. > MATH \\begin align f y = f x + \\nabla f x ^T y−x + \\frac 1 2 y−x ^T \\nabla^2 f x y−x , \\space \\forall x, y \\end align MATH Then, according to the above MATH , we can arrange the last term as a lower bound condition. > MATH \\begin align f y & \\ge f x + \\nabla f x ^T y−x + \\frac m 2 \\lVert y−x \\rVert 2^2, \\space \\forall x, y \\end align MATH Differentiating MATH with respect to MATH gives MATH . Substituting MATH into the Taylor expansion gives: > MATH \\begin align f y & \\ge f x + \\nabla f x ^T \\tilde y −x + \\frac m 2 \\lVert \\tilde y −x \\rVert 2^2 \\\\ & = f x - \\frac 1 2m \\lVert \\nabla f x \\rVert 2^2 \\end align MATH Therefore, replacing MATH with MATH gives: > MATH \\begin align f^ \\ge f x - \\frac 1 2m \\lVert \\nabla f x \\rVert 2^2 \\end align MATH The above stopping rule is derived as follows: > MATH \\begin align f x - f^ \\le \\frac 1 2m \\lVert \\nabla f x \\rVert^2 2 & \\le \\epsilon \\\\ \\lVert \\nabla f x \\rVert^2 2 & \\le 2m\\epsilon \\\\ \\lVert \\nabla f x \\rVert 2 & \\le \\sqrt 2m\\epsilon \\\\ \\end align MATH Advantages and Disadvantages of Gradient Descent Pros The algorithm is simple and the cost per iteration is low. Very fast for well-conditioned, strongly convex problems. Cons Generally slow because many problems are not strongly convex or well-conditioned. Cannot handle non-differentiable functions.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_03_05_look_at_the_conditions_and_practicalities/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_03_06_can_we_do_better",
    "title": "06-03-06 Can we do better?",
    "chapter": "06",
    "order": 13,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Gradient descent has a convergence rate of MATH for problems represented by functions that have Lipschitz gradients and are convex and differentiable. Are there first-order methods that are faster than gradient descent? First-order method A first-order method can express changes at the MATH -th iteration as follows. Therefore, the change at the MATH -th iteration is expressed as the span of gradients from the initial position MATH to MATH . > MATH span MATH Theorem Nesterov Nesterov's theorem provides a lower bound for the convergence of first-order methods. > Nesterov Theorem For any MATH and starting point MATH , there exists a function MATH such that any first-order method satisfies the following condition where MATH denotes the dimension : \\begin align f x^ k −f^ \\star ≥ \\frac 3L \\lVert x^ 0 −x^ \\star \\rVert 2^2 32 k + 1 ^2 \\\\\\ \\end align Since Nesterov's theorem has MATH in the denominator of the lower bound, the convergence rate becomes MATH . Furthermore, the number of iterations becomes MATH . We will examine this content in detail later.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_03_06_can_we_do_better/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_04_gradient_boosting",
    "title": "06-04 Gradient boosting",
    "chapter": "06",
    "order": 14,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Gradient boosting Gradient boosting is a method that uses gradient descent to sequentially create trees while compensating for the errors of previous trees when trying to predict results with an ensemble model composed of multiple trees. Gradient Boosting can be used for both regression and classification. For detailed information, refer to the Gradient Boosting from scratch https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d blog Reference Functional gradient descent algorithm Gradient boosting was introduced as a functional gradient descent algorithm by Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean. The functional gradient descent algorithm optimizes the loss function over function space by repeatedly selecting functions that have the negative direction of the gradient, thus performing gradient descent. For detailed information, refer to Gradient Boosting https://en.wikipedia.org/wiki/Gradient boosting Reference Boosting vs Bagging Boosting is an ensemble technique that sequentially generates weak learners to predict results. The next stage learner learns from the data that the previous stage learner predicted incorrectly, and the results of sequentially generated learners are combined to create the final result. Bagging is an ensemble technique that generates weak learners independently from each other to predict results. Therefore, each learner runs in parallel and their results are combined to create the final result. For detailed information, refer to the What is the difference between Bagging and Boosting? https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/ blog Gradient Boosting Let's examine the background of how Gradient Boosting came to be developed. Suppose there is an ensemble model composed of trees that is used for classification. This model will want to predict results that minimize the error with the observed values. Let the observed values be MATH , MATH , the input data be MATH , and the predicted values be MATH , MATH . As shown in the figure below, each tree belonging to the ensemble receives MATH as input and outputs results according to the branching conditions in the tree's nodes. MATH The predicted value MATH of the ensemble model can be calculated by weighted summation of each tree's results. Here, MATH is the result output by tree MATH when it receives MATH as input. > MATH \\begin equation u i = \\sum j=1 ^M \\beta j T j x i \\end equation MATH For the loss function, it can be defined as MATH in the form of sum of squared errors to minimize the error between observed and predicted values. > MATH \\begin equation \\min \\beta \\sum i=1 ^n L\\left y i, \\sum j=1 ^M \\beta j T j x i \\right \\end equation MATH Generally, when constructing trees in ensemble models, many small trees with fixed depth are created. This is because making trees smaller uses less memory and enables faster prediction, and as the number of trees increases, the performance of the ensemble improves. Generally, the depth of trees is fixed at 5 or less. Therefore, in this problem, the node conditions defined in each tree are very diverse and the results of very many trees are linearly combined, making the tree space quite large. Thus, it can be said that this is a very difficult problem to optimize. To solve this problem, the optimization problem must be transformed into an easier one. The original optimization problem is to find MATH weights MATH that minimize the loss function. Let's think of this problem as a minimization problem MATH of function MATH with respect to predicted values MATH . If function MATH is the loss function MATH , then finding MATH that minimizes the loss function can be said to be an easily redefined problem. Here, MATH is the number of data points. Gradient boosting refers to the technique of solving the redefined minimization problem MATH using gradient descent. Algorithm The Gradient boosting algorithm performs gradient descent in the following way to find the optimal solution MATH of MATH . 1. Set the initial value as the result of an arbitrary tree: MATH . Then repeat the following steps 2~4. 2. Calculate the negative gradient for MATH , which is the most recent predicted value for MATH data points. > MATH \\begin equation d i = - \\left . \\left \\frac \\partial L y i,u i \\partial u i \\right \\right| u i = u i^ k-1 , i=1,\\dots,n \\end equation MATH 3. Find the tree MATH whose results MATH are most similar to the gradients MATH for MATH data points. > MATH \\begin equation \\min \\text trees T \\sum i=1 ^n d i-T x i ^2 \\end equation MATH 4. Calculate the step size MATH and update the predicted values using the MATH found above. > MATH This algorithm finds the gradient MATH with respect to MATH to find the optimal solution MATH through gradient descent, finds MATH closest to MATH , and substitutes MATH instead of gradient in the update equation to find the next position. The final predicted value MATH obtained in this way can be seen to be identical to the weighted summation of tree results defined earlier. That is, if we expand the recursive update equation MATH back to MATH , we get MATH , which can be made into the form of weighted summation of tree results.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_04_gradient_boosting/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_06_gradent_descent_with_momentum",
    "title": "06-06 Gradient descent with momentum",
    "chapter": "06",
    "order": 15,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "The Problem with Vanilla Gradient Descent Imagine you're rolling a ball down a valley. Standard gradient descent is like a ball with no memory - at each step, it only considers the current slope and moves accordingly. This can lead to several problems: 1. Slow convergence in ravines : When the function has steep gradients in some directions and shallow gradients in others 2. Oscillations : The algorithm may zigzag back and forth across the valley instead of making steady progress 3. Getting stuck in poor local minima : Without momentum, the algorithm may settle in suboptimal solutions Question : What if our ball could \"remember\" its previous direction and maintain some velocity? Momentum: Adding Memory to Gradient Descent Gradient descent with momentum is inspired by physics - specifically, the motion of a ball rolling down a hill with friction. The key insight is to accumulate a velocity vector that combines the current gradient with the previous momentum. The Momentum Algorithm Update rules: MATH \\begin align v^ k &= \\beta v^ k-1 + 1-\\beta \\nabla f x^ k-1 \\\\ x^ k &= x^ k-1 - t v^ k \\end align MATH where: - MATH is the momentum velocity at iteration MATH - MATH is the momentum coefficient typically 0.9 or 0.99 - MATH is the learning rate - MATH initial velocity is zero Alternative Formulation Nesterov-style Some implementations use a slightly different form: MATH \\begin align v^ k &= \\beta v^ k-1 + \\nabla f x^ k-1 \\\\ x^ k &= x^ k-1 - t v^ k \\end align MATH Key insight : The momentum term MATH is an exponentially weighted moving average of past gradients. Understanding the Momentum Coefficient MATH The momentum coefficient MATH controls how much \"memory\" the algorithm has: - MATH : No momentum, reduces to standard gradient descent - MATH : Moderate momentum, commonly used in practice - MATH : High momentum, used in some deep learning applications - MATH : Maximum momentum, but may cause instability Exponentially Weighted Moving Average The momentum MATH can be expanded as: MATH This shows that momentum gives exponentially decreasing weights to older gradients. Interactive Visualization: Gradient Descent vs Momentum Learning Rate: 0.1 Momentum β : 0.9 Animation Speed: 5 Start Vanilla GD Start Momentum GD Compare Both Reset Vanilla Gradient Descent Ready to start Momentum Gradient Descent Ready to start document.addEventListener 'DOMContentLoaded', function // Momentum visualization implementation const svg = d3.select \" momentum-svg\" ; const width = 800, height = 500; const margin = top: 20, right: 20, bottom: 40, left: 40 ; // Function to optimize: f x,y = 0.5 x^2 + 5 y^2 elongated bowl function f x, y return 0.5 x x + 5 y y; function gradient x, y return x, 10 y ; // Scale setup const xScale = d3.scaleLinear .domain -6, 6 .range margin.left, width - margin.right ; const yScale = d3.scaleLinear .domain -3, 3 .range height - margin.bottom, margin.top ; // Create contour plot function createContours // Create a simple grid-based contour visualization const gridSize = 20; const contourLevels = 1, 4, 9, 16, 25, 36, 49 ; svg.selectAll \".contour\" .remove ; // Draw contour ellipses for the function f x,y = 0.5 x^2 + 5 y^2 contourLevels.forEach level => // For f x,y = 0.5 x^2 + 5 y^2 = level // This is an ellipse: x^2/ 2 level + y^2/ level/5 = 1 const a = Math.sqrt 2 level ; // semi-major axis in x direction const b = Math.sqrt level / 5 ; // semi-minor axis in y direction if a const grad = gradient x, y ; x -= lr grad 0 ; y -= lr grad 1 ; vanillaPath.push x: x, y: y ; iteration++; updateVisualization ; updateStatus ; // Stop if converged or max iterations reached if Math.abs grad 0 = maxIterations clearInterval vanillaInterval ; vanillaRunning = false; , 1000 / parseFloat document.getElementById 'momentum-speed' .value ; function runMomentumGD const lr = parseFloat document.getElementById 'momentum-lr' .value ; const beta = parseFloat document.getElementById 'momentum-beta' .value ; let x = -4, y = 2; // Starting point let vx = 0, vy = 0; // Initial velocity let iteration = 0; const maxIterations = 500; momentumPath = x: x, y: y ; momentumRunning = true; momentumInterval = setInterval => const grad = gradient x, y ; vx = beta vx + 1 - beta grad 0 ; vy = beta vy + 1 - beta grad 1 ; x -= lr vx; y -= lr vy; momentumPath.push x: x, y: y ; iteration++; updateVisualization ; updateStatus ; // Stop if converged or max iterations reached if Math.abs grad 0 = maxIterations clearInterval momentumInterval ; momentumRunning = false; , 1000 / parseFloat document.getElementById 'momentum-speed' .value ; function updateVisualization // Remove existing paths svg.selectAll \".vanilla-path\" .remove ; svg.selectAll \".momentum-path\" .remove ; svg.selectAll \".vanilla-point\" .remove ; svg.selectAll \".momentum-point\" .remove ; // Draw vanilla GD path if vanillaPath.length > 1 const line = d3.line .x d => xScale d.x .y d => yScale d.y ; svg.append \"path\" .datum vanillaPath .attr \"class\", \"vanilla-path\" .attr \"d\", line .attr \"fill\", \"none\" .attr \"stroke\", \" 4CAF50\" .attr \"stroke-width\", 2 ; // Current point const current = vanillaPath vanillaPath.length - 1 ; svg.append \"circle\" .attr \"class\", \"vanilla-point\" .attr \"cx\", xScale current.x .attr \"cy\", yScale current.y .attr \"r\", 5 .attr \"fill\", \" 4CAF50\" ; // Draw momentum GD path if momentumPath.length > 1 const line = d3.line .x d => xScale d.x .y d => yScale d.y ; svg.append \"path\" .datum momentumPath .attr \"class\", \"momentum-path\" .attr \"d\", line .attr \"fill\", \"none\" .attr \"stroke\", \" FF9800\" .attr \"stroke-width\", 2 ; // Current point const current = momentumPath momentumPath.length - 1 ; svg.append \"circle\" .attr \"class\", \"momentum-point\" .attr \"cx\", xScale current.x .attr \"cy\", yScale current.y .attr \"r\", 5 .attr \"fill\", \" FF9800\" ; function updateStatus // Update vanilla GD status if vanillaPath.length > 0 const current = vanillaPath vanillaPath.length - 1 ; const fValue = f current.x, current.y ; document.getElementById 'vanilla-gd-info' .innerHTML = Iteration: MATH current.x.toFixed 3 , MATH fValue.toFixed 4 ; // Update momentum GD status if momentumPath.length > 0 const current = momentumPath momentumPath.length - 1 ; const fValue = f current.x, current.y ; document.getElementById 'momentum-gd-info' .innerHTML = Iteration: MATH current.x.toFixed 3 , MATH fValue.toFixed 4 ; // Event listeners document.getElementById 'start-vanilla-gd' .addEventListener 'click', => if !vanillaRunning vanillaRunning = true; runVanillaGD ; ; document.getElementById 'start-momentum-gd' .addEventListener 'click', => if !momentumRunning momentumRunning = true; runMomentumGD ; ; document.getElementById 'start-both' .addEventListener 'click', => if !vanillaRunning && !momentumRunning vanillaRunning = true; momentumRunning = true; runVanillaGD ; runMomentumGD ; ; document.getElementById 'reset-momentum' .addEventListener 'click', => clearInterval vanillaInterval ; clearInterval momentumInterval ; vanillaRunning = false; momentumRunning = false; vanillaPath = ; momentumPath = ; updateVisualization ; document.getElementById 'vanilla-gd-info' .innerHTML = 'Ready to start'; document.getElementById 'momentum-gd-info' .innerHTML = 'Ready to start'; ; // Update display values document.getElementById 'momentum-lr' .addEventListener 'input', function document.getElementById 'momentum-lr-value' .textContent = this.value; ; document.getElementById 'momentum-beta' .addEventListener 'input', function document.getElementById 'momentum-beta-value' .textContent = this.value; ; document.getElementById 'momentum-speed' .addEventListener 'input', function document.getElementById 'momentum-speed-value' .textContent = this.value; ; ; Advantages of Momentum 1. Faster Convergence Momentum helps the algorithm build up speed in consistent directions, leading to faster convergence especially in: - Functions with valleys or ravines - Ill-conditioned problems high condition number - Functions with many local minima 2. Reduced Oscillations In directions where the gradient changes sign frequently, momentum helps smooth out the oscillations by averaging past gradients. 3. Escape from Local Minima The accumulated momentum can help the algorithm \"roll through\" small local minima and continue toward better solutions. Variants and Extensions 1. Nesterov Accelerated Gradient NAG Instead of computing the gradient at the current position, NAG computes it at the \"look-ahead\" position: MATH \\begin align v^ k &= \\beta v^ k-1 + \\nabla f x^ k-1 - \\beta v^ k-1 \\\\ x^ k &= x^ k-1 - t v^ k \\end align MATH Intuition : \"Look before you leap\" - check the gradient at where momentum would take you. The Problem with Regular Momentum While momentum helps the ball overcome local minima, there's a limitation we can observe: when approaching the target, momentum still takes considerable time before stopping. The reason is precisely because of the accumulated velocity. The Key Insight The fundamental idea is to predict the future direction - essentially looking ahead one step! Specifically, if we use the momentum term MATH for updating, we can approximate the next position as MATH we don't include the gradient term here as we'll use it in the final step . Instead of using the gradient at the current position, NAG takes a step forward and uses the gradient at the anticipated next position. Visual Comparison With regular momentum : The update is the sum of two vectors: - Momentum vector from previous step - Gradient at the current position With Nesterov momentum : The update is the sum of two vectors: - Momentum vector from previous step - Gradient at the look-ahead position where momentum would take us This \"look-ahead\" approach allows NAG to make more informed corrections and often leads to faster convergence. 2. Adaptive Moment Estimation Adam Adam combines momentum with adaptive learning rates for each parameter: MATH \\begin align m^ k &= \\beta 1 m^ k-1 + 1-\\beta 1 \\nabla f x^ k-1 \\\\ v^ k &= \\beta 2 v^ k-1 + 1-\\beta 2 \\nabla f x^ k-1 ^2 \\\\ x^ k &= x^ k-1 - t \\frac m^ k \\sqrt v^ k + \\epsilon \\end align MATH Practical Implementation Tips 1. Choosing the Momentum Coefficient - Start with MATH : A good default for most problems - Increase to MATH : For very smooth optimization landscapes - Decrease to MATH : For noisy or non-smooth functions 2. Learning Rate Adjustment When using momentum, you may need to reduce the learning rate compared to vanilla gradient descent, as momentum amplifies the effective step size. 3. Warm-up Period Consider starting with lower momentum and gradually increasing it, as momentum needs time to build up effective velocity. Mathematical Analysis Convergence Properties For strongly convex functions with Lipschitz gradients, momentum gradient descent achieves: - Linear convergence : MATH for some MATH - Improved condition number : Effective condition number can be improved from MATH to MATH Heavy Ball Method Connection Momentum gradient descent is closely related to the heavy ball method from classical mechanics: MATH This differential equation, when discretized, leads to the momentum update rules. Comparison Summary | Aspect | Vanilla GD | Momentum GD | |--------|------------|-------------| | Memory | No | Yes exponential decay | | Convergence | Can be slow | Often faster | | Oscillations | More prone | Reduced | | Local minima | May get stuck | Better escape | | Hyperparameters | Learning rate | Learning rate + momentum | | Computational cost | Low | Slightly higher | Key Takeaways 1. Momentum adds memory : It remembers the direction of previous steps 2. Faster convergence : Especially effective for functions with valleys or ravines 3. Reduced oscillations : Smooths out zigzag behavior 4. Widely used : Foundation for many modern optimization algorithms 5. Tunable : The momentum coefficient MATH allows fine-tuning for different problems Bottom line : Momentum is a simple yet powerful enhancement to gradient descent that has stood the test of time and remains relevant in modern machine learning applications.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_06_gradent_descent_with_momentum/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_05_stochastic_gradient_descent",
    "title": "06-05 Stochastic gradient descent",
    "chapter": "06",
    "order": 15,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "Motivation: The Big Data Challenge Imagine you're training a machine learning model on millions of data points. Traditional gradient descent requires computing gradients for all data points before making a single update. This becomes computationally expensive and memory-intensive for large datasets. Question : What if we could make progress by looking at just one data point at a time? The Mathematical Foundation Consider the optimization problem of minimizing a sum of functions: MATH \\begin equation \\min x f x = \\min x \\sum i=1 ^m f i x \\end equation MATH Real-world interpretation : In machine learning, MATH often represents the loss on the MATH -th training example, where MATH are the model parameters. Batch Gradient Descent Traditional Approach The gradient of the sum equals the sum of gradients: MATH The update rule becomes: MATH \\begin equation x^ k = x^ k-1 - t k \\cdot \\sum i=1 ^ m \\nabla f i x^ k-1 , \\, k=1,2,3,\\dots \\end equation MATH Computational cost : MATH gradient evaluations per iteration, where MATH is the number of functions or data points . Stochastic Gradient Descent: The Efficient Alternative Key Insight : Instead of computing gradients for all MATH functions, SGD uses only one function at each iteration. MATH \\begin equation x^ k = x^ k-1 - t k \\cdot \\nabla f i k x^ k-1 , \\quad i k \\in \\ 1,2,\\dots,m\\ \\end equation MATH Computational cost : MATH gradient evaluation per iteration - a massive improvement! Interactive Visualization: GD vs SGD with Step Control Learning Rate: 0.1 Number of Functions m : 5 Animation Speed: 5 Execution Mode: Auto Run Manual Step-by-Step Start Gradient Descent Start SGD Pause Step Forward Reset Gradient Descent Status Ready to start SGD Status Ready to start Gradient Descent Batch Iterations: 0, Cost: 0 Gradient: 0, 0 Stochastic Gradient Descent Iterations: 0, Cost: 0 Selected Function: -, Gradient: 0, 0 Convergence Progress GD Progress: SGD Progress: Selection Strategies for SGD The function index MATH can be selected using different strategies: 1. Cyclic Rule Pattern : MATH - Advantages : Deterministic, ensures all functions are visited - Disadvantages : May get stuck in periodic patterns 2. Randomized Rule Pattern : MATH chosen uniformly at random from MATH - Advantages : Avoids periodic patterns, better theoretical guarantees - Disadvantages : Some functions may be visited more frequently than others In practice : Randomized selection is preferred due to better convergence properties and ability to escape local patterns. Convergence Analysis: Theory vs Practice Mathematical Comparison Batch GD one epoch : MATH SGD one epoch with cyclic rule : MATH Understanding the SGD Cyclic Rule Formula Let's break down this formula step by step to understand what happens during one complete epoch of SGD with cyclic rule: What is an epoch? One epoch means we've processed all MATH functions exactly once. The cyclic rule process: - Start at position MATH - Step 1 : Use function MATH , compute MATH , update to MATH - Step 2 : Use function MATH , compute MATH , update to MATH - Step 3 : Use function MATH , compute MATH , update to MATH - ... - Step m : Use function MATH , compute MATH , update to MATH Individual SGD updates: MATH \\begin align x^ k+1 &= x^ k - t k \\nabla f 1 x^ k \\\\ x^ k+2 &= x^ k+1 - t k \\nabla f 2 x^ k+1 \\\\ x^ k+3 &= x^ k+2 - t k \\nabla f 3 x^ k+2 \\\\ &\\vdots \\\\ x^ k+m &= x^ k+m-1 - t k \\nabla f m x^ k+m-1 \\end align MATH Telescoping the updates: If we substitute recursively and collect all terms, we get: MATH This can be written compactly as: MATH Key insight: Each gradient MATH is evaluated at a different position MATH , not at the same starting position MATH . Key difference in update directions : MATH This difference represents how much the SGD path deviates from what batch GD would do. If the functions don't change much locally Lipschitz continuous gradients , this difference is small and SGD behaves similarly to batch GD. Concrete Example with m = 3 Functions Let's illustrate with MATH functions to make this crystal clear: Starting position: MATH SGD Cyclic Rule Process: 1. Use MATH : Compute MATH , update: MATH 2. Use MATH : Compute MATH at the new position, update: MATH 3. Use MATH : Compute MATH at the newest position, update: MATH Final SGD result after one epoch: MATH Compare with Batch GD: MATH The crucial difference: - Batch GD : All gradients evaluated at the same starting point MATH - SGD : Each gradient evaluated at a different point along the optimization path This is why SGD can make faster initial progress it's already \"exploring\" the landscape but can be noisier near the optimum. Convergence Properties Batch Gradient Descent Direction : Always in steepest descent direction Convergence : Smooth, monotonic decrease Speed : Slower per epoch, but stable Memory : Requires full dataset in memory Stochastic Gradient Descent Direction : Noisy, approximate descent direction Convergence : Oscillatory, but faster initial progress Speed : Faster per epoch, especially for large datasets Memory : Processes one sample at a time Theoretical Guarantees Lipschitz Continuity Condition : If MATH is Lipschitz continuous with constant MATH : MATH Then SGD converges to the same optimal solution as batch GD, provided the learning rate satisfies appropriate conditions. Practical Observation : - SGD excels in the exploration phase far from optimum - SGD struggles in the exploitation phase near optimum due to noise Mini-Batch Gradient Descent: The Best of Both Worlds A compromise between batch GD and SGD uses mini-batches of size MATH : MATH where MATH is a mini-batch of MATH randomly selected indices. Comparison: Batch vs Mini-batch vs SGD Method Batch Size Computation/Update Convergence Memory Usage Batch GD MATH full dataset MATH Smooth, stable High Mini-batch GD MATH typically 32-256 MATH Balanced Medium SGD 1 MATH Fast but noisy Low Key Takeaways and Practical Insights When to Use SGD 1. Large datasets millions of samples 2. Online learning scenarios 3. Limited memory environments 4. Early training phases for quick progress When to Use Batch GD 1. Small to medium datasets 2. High precision requirements 3. Stable convergence needed 4. Final fine-tuning phases Best Practices - Learning rate scheduling : Start high, decay over time - Shuffling : Randomize data order each epoch - Mini-batches : Often the best practical choice - Momentum : Helps SGD overcome noise and accelerate convergence Modern reality : Most deep learning frameworks use mini-batch SGD with sophisticated optimizers Adam, RMSprop that adapt the learning rate automatically. document.addEventListener 'DOMContentLoaded', function // Global variables for demos let gdData = , sgdData = ; let gdPosition = 2, 2 , sgdPosition = 2, 2 ; let isRunning = false; let isPaused = false; let isManualMode = false; let animationId; let currentAlgorithm = null; // 'gd' or 'sgd' let algorithmState = gd: iteration: 0, position: 2, 2 , path: , functions: , totalCost: 0, gradient: 0, 0 , sgd: iteration: 0, position: 2, 2 , path: , functions: , totalCost: 0, gradient: 0, 0 , selectedFunc: -1 ; // Initialize sliders const learningRateSlider = document.getElementById 'learning-rate' ; const numFunctionsSlider = document.getElementById 'num-functions' ; const animationSpeedSlider = document.getElementById 'animation-speed' ; const lrValue = document.getElementById 'lr-value' ; const mValue = document.getElementById 'm-value' ; const speedValue = document.getElementById 'speed-value' ; if learningRateSlider learningRateSlider.addEventListener 'input', function lrValue.textContent = this.value; ; if numFunctionsSlider numFunctionsSlider.addEventListener 'input', function mValue.textContent = this.value; ; if animationSpeedSlider animationSpeedSlider.addEventListener 'input', function speedValue.textContent = this.value; ; // Execution mode radio buttons const execModeRadios = document.querySelectorAll 'input name=\"exec-mode\" ' ; execModeRadios.forEach radio => radio.addEventListener 'change', function isManualMode = this.value === 'manual'; updateControlButtons ; ; ; // Simple quadratic functions for demonstration function createFunctions m const functions = ; for let i = 0; i animationId = requestAnimationFrame step ; , delay ; else isRunning = false; updateControlButtons ; const detailedInfoId = algorithm === 'gd' ? 'gd-detailed-info' : 'sgd-detailed-info'; document.getElementById detailedInfoId .textContent = 'Converged!'; step ; // Perform a single step of the algorithm function performSingleStep algorithm const state = algorithmState algorithm ; const lr = parseFloat learningRateSlider.value ; if algorithm === 'gd' return performGDStep state, lr ; else return performSGDStep state, lr ; function performGDStep state, lr // Compute full gradient sum of all function gradients let gradX = 0, gradY = 0; for const func of state.functions const grad = func.gradient state.position 0 , state.position 1 ; gradX += grad 0 ; gradY += grad 1 ; // Average the gradients gradX /= state.functions.length; gradY /= state.functions.length; state.gradient = gradX, gradY ; // Update position state.position 0 -= lr gradX; state.position 1 -= lr gradY; state.path.push state.position.slice ; // Compute total cost state.totalCost = 0; for const func of state.functions state.totalCost += func.value state.position 0 , state.position 1 ; state.totalCost /= state.functions.length; state.iteration++; // Update visualization updateAlgorithmDisplay 'gd' ; // Check convergence const gradientMagnitude = Math.sqrt gradX gradX + gradY gradY ; return state.iteration 0.01; function performSGDStep state, lr // Select random function const funcIndex = Math.floor Math.random state.functions.length ; const func = state.functions funcIndex ; state.selectedFunc = funcIndex; // Compute gradient for selected function only const grad = func.gradient state.position 0 , state.position 1 ; state.gradient = grad; // Update position state.position 0 -= lr grad 0 ; state.position 1 -= lr grad 1 ; state.path.push state.position.slice ; // Compute total cost state.totalCost = 0; for const func of state.functions state.totalCost += func.value state.position 0 , state.position 1 ; state.totalCost /= state.functions.length; state.iteration++; // Update visualization updateAlgorithmDisplay 'sgd' ; // Check convergence SGD needs more iterations return state.iteration 0.01; function updateAlgorithmDisplay algorithm const state = algorithmState algorithm ; const plotId = algorithm === 'gd' ? 'gd-plot' : 'sgd-plot'; const color = algorithm === 'gd' ? 'steelblue' : 'orange'; // Update path visualization const plot = svg: d3.select $ plotId , g: d3.select $ plotId g , xScale: d3.scaleLinear .domain -3, 3 .range 0, 270 , yScale: d3.scaleLinear .domain -3, 3 .range 270, 0 ; if !plot.g.empty updatePath plot, state.path, color ; // Update info displays const infoId = algorithm === 'gd' ? 'gd-info' : 'sgd-info'; const gradientInfoId = algorithm === 'gd' ? 'gd-gradient-info' : 'sgd-gradient-info'; document.getElementById infoId .textContent = Iterations: MATH state.totalCost.toFixed 4 ; if algorithm === 'gd' document.getElementById gradientInfoId .textContent = Gradient: MATH state.gradient 1 .toFixed 3 ; else document.getElementById gradientInfoId .textContent = Selected Function: f MATH state.gradient 0 .toFixed 3 , $ state.gradient 1 .toFixed 3 ; // Update progress bar const progressId = algorithm === 'gd' ? 'gd-progress' : 'sgd-progress'; const maxIterations = algorithm === 'gd' ? 100 : 300; const progress = Math.min state.iteration / maxIterations 100, 100 ; document.getElementById progressId .style.width = $ progress % ; // Update detailed status const detailedInfoId = algorithm === 'gd' ? 'gd-detailed-info' : 'sgd-detailed-info'; const gradMagnitude = Math.sqrt state.gradient 0 2 + state.gradient 1 2 ; document.getElementById detailedInfoId .textContent = Step MATH state.position 0 .toFixed 3 , MATH gradMagnitude.toFixed 4 ; function updatePath plot, path, color const line = d3.line .x d => plot.xScale d 0 .y d => plot.yScale d 1 ; plot.g.selectAll '.path' .remove ; plot.g.append 'path' .datum path .attr 'class', 'path' .attr 'fill', 'none' .attr 'stroke', color .attr 'stroke-width', 2 .attr 'd', line ; // Add current position const current = path path.length - 1 ; plot.g.selectAll '.current-pos' .remove ; plot.g.append 'circle' .attr 'class', 'current-pos' .attr 'cx', plot.xScale current 0 .attr 'cy', plot.yScale current 1 .attr 'r', 5 .attr 'fill', color ; function resetDemo if animationId cancelAnimationFrame animationId ; isRunning = false; isPaused = false; currentAlgorithm = null; // Reset algorithm states algorithmState = gd: iteration: 0, position: 2, 2 , path: , functions: , totalCost: 0, gradient: 0, 0 , sgd: iteration: 0, position: 2, 2 , path: , functions: , totalCost: 0, gradient: 0, 0 , selectedFunc: -1 ; // Clear plots d3.select ' gd-plot' .selectAll \" \" .remove ; d3.select ' sgd-plot' .selectAll \" \" .remove ; // Reset info displays document.getElementById 'gd-info' .textContent = 'Iterations: 0, Cost: 0'; document.getElementById 'sgd-info' .textContent = 'Iterations: 0, Cost: 0'; document.getElementById 'gd-gradient-info' .textContent = 'Gradient: 0, 0 '; document.getElementById 'sgd-gradient-info' .textContent = 'Selected Function: -, Gradient: 0, 0 '; // Reset detailed info document.getElementById 'gd-detailed-info' .textContent = 'Ready to start'; document.getElementById 'sgd-detailed-info' .textContent = 'Ready to start'; // Reset progress bars document.getElementById 'gd-progress' .style.width = '0%'; document.getElementById 'sgd-progress' .style.width = '0%'; // Update control buttons updateControlButtons ; ;",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_05_stochastic_gradient_descent/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter06/06_07_regularization_and_loss_functions",
    "title": "06-07 Regularization and Loss Functions",
    "chapter": "06",
    "order": 16,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Introduction: The Overfitting Problem in Machine Learning In machine learning, one of the biggest challenges is overfitting - when a model learns too much detail from training data, leading to poor performance on new data. Regularization is a crucial technique to address this problem. Why do we need Regularization? Consider the basic linear regression problem: MATH Problem : When the number of features is large or data is scarce, the model can find solutions with training error = 0 but poor generalization. Solution : Add a regularization term to \"penalize\" large weights: MATH where: - MATH is the regularization parameter - MATH is the regularization function Ridge Regression L2 Regularization Mathematical Definition Ridge regression uses the L2 norm as the regularization term: MATH Component explanation: - Loss term : MATH - measures deviation between predictions and actual values - Regularization term : MATH - penalizes large weights - Regularization strength : MATH controls the amount of regularization Analytical Solution Ridge regression has a closed-form solution: MATH Advantages of this solution: - The matrix MATH is always invertible when MATH - Solves multicollinearity problems - Computationally efficient Effects of Ridge Regularization Ridge Regularization Controls Lambda λ : 1.0 Noise Level: 0.1 Number of Features: 10 Generate New Data Coefficients Path L2 Norm of coefficients: 0 Training vs Validation Error Optimal λ: 0 Lasso Regression L1 Regularization Mathematical Definition Lasso Least Absolute Shrinkage and Selection Operator uses the L1 norm: MATH where MATH Key Properties of Lasso 1. Sparse Solutions: - Lasso can set some coefficients to exactly zero - Automatically performs feature selection - Creates simple, interpretable models 2. No closed-form solution: - Requires optimization algorithms coordinate descent, proximal gradient - More computationally complex than Ridge Ridge vs Lasso Comparison Ridge vs Lasso Regularization Comparison Lambda λ : 1.0 Feature Correlation: 0.5 Update Comparison Ridge Coefficients Shrinkage: Uniform, never zero Lasso Coefficients Sparsity: Some exactly zero Regularization Geometry L1: Diamond, L2: Circle Geometry of Regularization Intuitive understanding of why Lasso creates sparse solutions: 1. L1 constraint : MATH creates a diamond shape in 2D 2. L2 constraint : MATH creates a circular shape in 2D 3. Intersection with loss contours : - L1: High probability of intersecting contours at corners sparse solutions - L2: Usually intersects at smooth points non-sparse solutions Elastic Net: Combining Ridge and Lasso Elastic Net combines both L1 and L2 regularization: MATH Parameters: - MATH : mixing parameter - MATH : Pure Lasso - MATH : Pure Ridge - MATH Loss Functions in Machine Learning 1. Regression Loss Functions Mean Squared Error MSE MATH Properties: - Smooth, differentiable everywhere - Sensitive to outliers - Convex optimization problem Mean Absolute Error MAE MATH Properties: - Robust to outliers - Non-differentiable at zero - Convex but requires subgradient methods Huber Loss MATH L Huber \\mathbf w = \\frac 1 n \\sum i=1 ^n \\begin cases \\frac 1 2 y i - \\mathbf w ^T \\mathbf x i ^2 & \\text if |y i - \\mathbf w ^T \\mathbf x i| \\leq \\delta \\\\ \\delta |y i - \\mathbf w ^T \\mathbf x i| - \\frac 1 2 \\delta^2 & \\text otherwise \\end cases MATH Properties: - Combines MSE small errors and MAE large errors - Smooth and robust - Parameter MATH controls transition point 2. Classification Loss Functions Logistic Loss Cross-entropy MATH Hinge Loss SVM MATH Interactive Loss Functions Comparison Loss Functions Comparison Huber Delta δ : 1.0 Error Range: 4.0 Show Derivatives Regression Loss Functions ■ MSE ■ MAE ■ Huber Classification Loss Functions ■ Logistic ■ Hinge ■ 0-1 Loss Regularization Path and Model Selection Cross-Validation for Regularization Choosing optimal MATH is typically done through cross-validation: 1. Split data : Training, validation, test sets 2. Grid search : Try multiple MATH values 3. Evaluate : Compute validation error for each MATH 4. Select : Choose MATH with lowest validation error Regularization Path Visualization Regularization Path Analysis Method: Ridge Lasso Elastic Net Elastic Net α: 0.5 Animate Path Coefficient Path Current λ: 0, Active features: 0 Cross-Validation Curve Optimal λ: 0, CV Score: 0 Practical Guidelines and Best Practices 1. When to use Ridge vs Lasso? Use Ridge when: All features are meaningful High multicollinearity Need stability in predictions Small dataset, many features Don't need automatic feature selection Use Lasso when: Need automatic feature selection Many irrelevant features Want simple, interpretable models Sparse solutions are preferred High-dimensional data 2. Hyperparameter Tuning Strategy python Pseudo-code for regularization tuning def tune regularization X train, y train, X val, y val : lambda values = np.logspace -4, 2, 50 Log-spaced values best lambda = None best score = float 'inf' for lambda val in lambda values: model = fit regularized model X train, y train, lambda val val score = evaluate model model, X val, y val if val score document.addEventListener 'DOMContentLoaded', function // Ridge Effect Demo initializeRidgeEffectDemo ; // Ridge vs Lasso Comparison initializeRidgeLassoComparison ; // Loss Functions Demo initializeLossFunctionsDemo ; // Regularization Path Demo initializeRegularizationPathDemo ; function initializeRidgeEffectDemo const lambdaSlider = document.getElementById 'ridge-lambda' ; const noiseSlider = document.getElementById 'ridge-noise' ; const featuresSlider = document.getElementById 'ridge-features' ; const regenerateBtn = document.getElementById 'ridge-regenerate' ; let currentData = generateRidgeData ; function generateRidgeData const n = 50; // samples const p = parseInt featuresSlider.value ; // features const noise = parseFloat noiseSlider.value ; // Generate true coefficients sparse const trueCoeffs = Array p .fill 0 .map , i => i Array p .fill .map => Math.random 2 - 1 ; // Generate y with noise const y = X.map row => row.reduce sum, x, i => sum + x trueCoeffs i , 0 + Math.random - 0.5 noise 2 ; return X, y, trueCoeffs, n, p ; function updateRidgeDemo const lambda = parseFloat lambdaSlider.value ; document.getElementById 'ridge-lambda-value' .textContent = lambda.toFixed 1 ; // Compute Ridge coefficients for different lambda values const lambdaRange = Array 50 .fill .map , i => i 0.2 ; const coeffPaths = computeRidgeCoefficients currentData, lambdaRange ; // Update visualizations updateRidgeCoefficientsPlot coeffPaths, lambdaRange, lambda ; updateRidgeErrorPlot currentData, lambdaRange ; function computeRidgeCoefficients data, lambdaRange const X, y, p = data; const paths = Array p .fill .map => ; lambdaRange.forEach lambda => const coeffs = solveRidge X, y, lambda ; coeffs.forEach coeff, i => paths i .push coeff ; ; return paths; function solveRidge X, y, lambda // Simplified Ridge solution: w = X'X + λI ^ -1 X'y const p = X 0 .length; const n = X.length; // Compute X'X const XtX = Array p .fill .map => Array p .fill 0 ; for let i = 0; i xScale lambdaRange i .y d => yScale d ; // Draw coefficient paths const colors = d3.schemeCategory10; coeffPaths.forEach path, i => g.append 'path' .datum path .attr 'fill', 'none' .attr 'stroke', colors i % colors.length .attr 'stroke-width', 2 .attr 'd', line ; ; // Current lambda line g.append 'line' .attr 'x1', xScale currentLambda .attr 'x2', xScale currentLambda .attr 'y1', 0 .attr 'y2', height .attr 'stroke', 'red' .attr 'stroke-width', 2 .attr 'stroke-dasharray', '5,5' ; // Update info const currentIndex = Math.round currentLambda / 0.2 ; const currentCoeffs = coeffPaths.map path => path currentIndex || 0 ; const l2Norm = Math.sqrt currentCoeffs.reduce sum, c => sum + c c, 0 ; document.getElementById 'ridge-coeff-info' .textContent = L2 Norm of coefficients: $ l2Norm.toFixed 3 ; function updateRidgeErrorPlot data, lambdaRange // Simplified error computation for demo const svg = d3.select ' ridge-error' ; svg.selectAll ' ' .remove ; const margin = top: 20, right: 20, bottom: 40, left: 50 ; const width = 400 - margin.left - margin.right; const height = 300 - margin.top - margin.bottom; const g = svg.append 'g' .attr 'transform', translate MATH margin.top ; // Generate synthetic error curves const trainErrors = lambdaRange.map lambda => 0.1 + 0.5 lambda + Math.random 0.1 ; const valErrors = lambdaRange.map lambda => 0.3 + 0.2 Math.abs lambda - 2 + Math.random 0.1 ; // Scales const xScale = d3.scaleLinear .domain d3.extent lambdaRange .range 0, width ; const yScale = d3.scaleLinear .domain 0, Math.max ...trainErrors, ...valErrors .nice .range height, 0 ; // Axes g.append 'g' .attr 'transform', translate 0,$ height .call d3.axisBottom xScale ; g.append 'g' .call d3.axisLeft yScale ; // Axis labels g.append 'text' .attr 'transform', translate MATH height + 35 .style 'text-anchor', 'middle' .text 'Lambda λ ' ; g.append 'text' .attr 'transform', 'rotate -90 ' .attr 'y', 0 - margin.left .attr 'x', 0 - height/2 .attr 'dy', '1em' .style 'text-anchor', 'middle' .text 'Error' ; // Line generator const line = d3.line .x d, i => xScale lambdaRange i .y d => yScale d ; // Draw error curves g.append 'path' .datum trainErrors .attr 'fill', 'none' .attr 'stroke', 'blue' .attr 'stroke-width', 2 .attr 'd', line ; g.append 'path' .datum valErrors .attr 'fill', 'none' .attr 'stroke', 'red' .attr 'stroke-width', 2 .attr 'd', line ; // Legend const legend = g.append 'g' .attr 'transform', translate $ width - 100 , 20 ; legend.append 'line' .attr 'x1', 0 .attr 'x2', 20 .attr 'y1', 0 .attr 'y2', 0 .attr 'stroke', 'blue' .attr 'stroke-width', 2 ; legend.append 'text' .attr 'x', 25 .attr 'y', 5 .text 'Training' ; legend.append 'line' .attr 'x1', 0 .attr 'x2', 20 .attr 'y1', 15 .attr 'y2', 15 .attr 'stroke', 'red' .attr 'stroke-width', 2 ; legend.append 'text' .attr 'x', 25 .attr 'y', 20 .text 'Validation' ; // Find optimal lambda const optimalIndex = valErrors.indexOf Math.min ...valErrors ; const optimalLambda = lambdaRange optimalIndex ; document.getElementById 'ridge-error-info' .textContent = Optimal λ: $ optimalLambda.toFixed 2 ; // Event listeners lambdaSlider.addEventListener 'input', updateRidgeDemo ; noiseSlider.addEventListener 'input', function document.getElementById 'ridge-noise-value' .textContent = this.value; ; featuresSlider.addEventListener 'input', function document.getElementById 'ridge-features-value' .textContent = this.value; ; regenerateBtn.addEventListener 'click', function currentData = generateRidgeData ; updateRidgeDemo ; ; // Initial update updateRidgeDemo ; function initializeRidgeLassoComparison const lambdaSlider = document.getElementById 'comparison-lambda' ; const correlationSlider = document.getElementById 'comparison-correlation' ; const updateBtn = document.getElementById 'comparison-update' ; function updateComparison const lambda = parseFloat lambdaSlider.value ; const correlation = parseFloat correlationSlider.value ; document.getElementById 'comparison-lambda-value' .textContent = lambda.toFixed 1 ; document.getElementById 'comparison-correlation-value' .textContent = correlation.toFixed 1 ; // Generate synthetic data with controlled correlation const p = 8; // features const trueCoeffs = 2, -1.5, 1, 0.5, -0.8, 0, 0, 0 ; // Compute Ridge and Lasso coefficients const ridgeCoeffs = computeRidgeCoeffs trueCoeffs, lambda, correlation ; const lassoCoeffs = computeLassoCoeffs trueCoeffs, lambda, correlation ; // Update visualizations updateComparisonPlot 'ridge-comparison', ridgeCoeffs, 'Ridge' ; updateComparisonPlot 'lasso-comparison', lassoCoeffs, 'Lasso' ; updateGeometryPlot lambda ; function computeRidgeCoeffs trueCoeffs, lambda, correlation // Simplified Ridge shrinkage const shrinkageFactor = 1 / 1 + lambda ; return trueCoeffs.map coeff => coeff shrinkageFactor ; function computeLassoCoeffs trueCoeffs, lambda, correlation // Simplified Lasso soft thresholding return trueCoeffs.map coeff => const absCoeff = Math.abs coeff ; if absCoeff w$ i+1 .range 0, width .padding 0.1 ; const yScale = d3.scaleLinear .domain d3.extent coeffs .nice .range height, 0 ; // Axes g.append 'g' .attr 'transform', translate 0,$ height .call d3.axisBottom xScale ; g.append 'g' .call d3.axisLeft yScale ; // Bars const color = title === 'Ridge' ? ' 4CAF50' : ' FF9800'; g.selectAll '.bar' .data coeffs .enter .append 'rect' .attr 'class', 'bar' .attr 'x', d, i => xScale w$ i+1 .attr 'width', xScale.bandwidth .attr 'y', d => d >= 0 ? yScale d : yScale 0 .attr 'height', d => Math.abs yScale d - yScale 0 .attr 'fill', color ; // Zero line g.append 'line' .attr 'x1', 0 .attr 'x2', width .attr 'y1', yScale 0 .attr 'y2', yScale 0 .attr 'stroke', 'black' .attr 'stroke-width', 1 ; function updateGeometryPlot lambda const svg = d3.select ' regularization-geometry' ; svg.selectAll ' ' .remove ; const margin = top: 20, right: 20, bottom: 40, left: 40 ; const width = 300 - margin.left - margin.right; const height = 250 - margin.top - margin.bottom; const g = svg.append 'g' .attr 'transform', translate MATH margin.top ; const centerX = width / 2; const centerY = height / 2; const radius = Math.min width, height / 3; // L2 constraint circle g.append 'circle' .attr 'cx', centerX .attr 'cy', centerY .attr 'r', radius .attr 'fill', 'none' .attr 'stroke', ' 4CAF50' .attr 'stroke-width', 3 .attr 'opacity', 0.7 ; // L1 constraint diamond const diamondPoints = centerX, centerY - radius , centerX + radius, centerY , centerX, centerY + radius , centerX - radius, centerY ; const line = d3.line .x d => d 0 .y d => d 1 ; g.append 'path' .datum ...diamondPoints, diamondPoints 0 .attr 'd', line .attr 'fill', 'none' .attr 'stroke', ' FF9800' .attr 'stroke-width', 3 .attr 'opacity', 0.7 ; // Contour lines ellipses for let i = 1; i 0.5 e e ; const maeLoss = errors.map e => Math.abs e ; const huberLoss = errors.map e => Math.abs e xScale errors i .y d => yScale d ; // Draw loss functions const colors = ' 1f77b4', ' ff7f0e', ' 2ca02c' ; const losses = mseLoss, maeLoss, huberLoss ; const names = 'MSE', 'MAE', 'Huber' ; losses.forEach loss, i => g.append 'path' .datum loss .attr 'fill', 'none' .attr 'stroke', colors i .attr 'stroke-width', 3 .attr 'd', line ; ; // Legend const legend = g.append 'g' .attr 'transform', translate $ width - 80 , 20 ; names.forEach name, i => legend.append 'line' .attr 'x1', 0 .attr 'x2', 20 .attr 'y1', i 20 .attr 'y2', i 20 .attr 'stroke', colors i .attr 'stroke-width', 3 ; legend.append 'text' .attr 'x', 25 .attr 'y', i 20 + 5 .text name ; ; function updateClassificationLosses range, showDerivatives const svg = d3.select ' classification-losses' ; svg.selectAll ' ' .remove ; const margin = top: 20, right: 20, bottom: 40, left: 50 ; const width = 400 - margin.left - margin.right; const height = 300 - margin.top - margin.bottom; const g = svg.append 'g' .attr 'transform', translate MATH margin.top ; // Generate margin values y f x const margins = d3.range -range, range, 0.1 ; // Loss functions const logisticLoss = margins.map m => Math.log 1 + Math.exp -m ; const hingeLoss = margins.map m => Math.max 0, 1 - m ; const zeroOneLoss = margins.map m => m xScale margins i .y d => yScale d ; // Draw loss functions const colors = ' d62728', ' 9467bd', ' 8c564b' ; const losses = logisticLoss, hingeLoss, zeroOneLoss ; const names = 'Logistic', 'Hinge', '0-1 Loss' ; losses.forEach loss, i => g.append 'path' .datum loss .attr 'fill', 'none' .attr 'stroke', colors i .attr 'stroke-width', 3 .attr 'd', line ; ; // Legend const legend = g.append 'g' .attr 'transform', translate $ width - 80 , 20 ; names.forEach name, i => legend.append 'line' .attr 'x1', 0 .attr 'x2', 20 .attr 'y1', i 20 .attr 'y2', i 20 .attr 'stroke', colors i .attr 'stroke-width', 3 ; legend.append 'text' .attr 'x', 25 .attr 'y', i 20 + 5 .text name ; ; // Event listeners deltaSlider.addEventListener 'input', updateLossFunctions ; rangeSlider.addEventListener 'input', updateLossFunctions ; derivativesCheckbox.addEventListener 'change', updateLossFunctions ; // Initial update updateLossFunctions ; function initializeRegularizationPathDemo const methodSelect = document.getElementById 'path-method' ; const alphaSlider = document.getElementById 'path-alpha' ; const animateBtn = document.getElementById 'path-animate' ; let isAnimating = false; let animationId = null; function updateRegularizationPath const method = methodSelect.value; const alpha = parseFloat alphaSlider.value ; document.getElementById 'path-alpha-value' .textContent = alpha.toFixed 1 ; // Generate regularization path const lambdaRange = d3.range 0, 5, 0.1 ; const coeffPaths = generateRegularizationPath method, alpha, lambdaRange ; updateCoefficientPathPlot coeffPaths, lambdaRange, method ; updateCVCurvePlot lambdaRange, method ; function generateRegularizationPath method, alpha, lambdaRange const p = 6; // features const trueCoeffs = 2, -1.5, 1, 0.5, -0.8, 0.3 ; const paths = Array p .fill .map => ; lambdaRange.forEach lambda => let coeffs; if method === 'ridge' coeffs = trueCoeffs.map c => c / 1 + lambda ; else if method === 'lasso' coeffs = trueCoeffs.map c => const abs c = Math.abs c ; return abs c > lambda ? Math.sign c abs c - lambda : 0; ; else // elastic net coeffs = trueCoeffs.map c => const abs c = Math.abs c ; const l1 part = abs c > alpha lambda ? Math.sign c abs c - alpha lambda : 0; return l1 part / 1 + 1 - alpha lambda ; ; coeffs.forEach coeff, i => paths i .push coeff ; ; return paths; function updateCoefficientPathPlot coeffPaths, lambdaRange, method const svg = d3.select ' coefficient-path' ; svg.selectAll ' ' .remove ; const margin = top: 20, right: 20, bottom: 40, left: 50 ; const width = 400 - margin.left - margin.right; const height = 300 - margin.top - margin.bottom; const g = svg.append 'g' .attr 'transform', translate MATH margin.top ; // Scales const xScale = d3.scaleLinear .domain d3.extent lambdaRange .range 0, width ; const allCoeffs = coeffPaths.flat ; const yScale = d3.scaleLinear .domain d3.extent allCoeffs .nice .range height, 0 ; // Axes g.append 'g' .attr 'transform', translate 0,$ height .call d3.axisBottom xScale ; g.append 'g' .call d3.axisLeft yScale ; // Axis labels g.append 'text' .attr 'transform', translate MATH height + 35 .style 'text-anchor', 'middle' .text 'Lambda λ ' ; g.append 'text' .attr 'transform', 'rotate -90 ' .attr 'y', 0 - margin.left .attr 'x', 0 - height/2 .attr 'dy', '1em' .style 'text-anchor', 'middle' .text 'Coefficient Value' ; // Line generator const line = d3.line .x d, i => xScale lambdaRange i .y d => yScale d ; // Draw coefficient paths const colors = d3.schemeCategory10; coeffPaths.forEach path, i => g.append 'path' .datum path .attr 'fill', 'none' .attr 'stroke', colors i % colors.length .attr 'stroke-width', 2 .attr 'd', line ; // Add feature label at the end const lastValue = path path.length - 1 ; if Math.abs lastValue > 0.01 g.append 'text' .attr 'x', width + 5 .attr 'y', yScale lastValue + 3 .attr 'font-size', '10px' .text f$ i+1 ; ; // Update info const currentLambda = 1.0; // example const activeFeatures = coeffPaths.filter path => Math.abs path Math.floor currentLambda 10 > 0.01 .length; document.getElementById 'path-info' .textContent = Current λ: MATH activeFeatures ; function updateCVCurvePlot lambdaRange, method const svg = d3.select ' cv-curve' ; svg.selectAll ' ' .remove ; const margin = top: 20, right: 20, bottom: 40, left: 50 ; const width = 400 - margin.left - margin.right; const height = 300 - margin.top - margin.bottom; const g = svg.append 'g' .attr 'transform', translate MATH margin.top ; // Generate synthetic CV scores const cvScores = lambdaRange.map lambda => const base = 0.2 + 0.1 Math.abs lambda - 1.5 ; return base + Math.random - 0.5 0.05; ; // Scales const xScale = d3.scaleLinear .domain d3.extent lambdaRange .range 0, width ; const yScale = d3.scaleLinear .domain d3.extent cvScores .nice .range height, 0 ; // Axes g.append 'g' .attr 'transform', translate 0,$ height .call d3.axisBottom xScale ; g.append 'g' .call d3.axisLeft yScale ; // Axis labels g.append 'text' .attr 'transform', translate MATH height + 35 .style 'text-anchor', 'middle' .text 'Lambda λ ' ; g.append 'text' .attr 'transform', 'rotate -90 ' .attr 'y', 0 - margin.left .attr 'x', 0 - height/2 .attr 'dy', '1em' .style 'text-anchor', 'middle' .text 'CV Score' ; // Line generator const line = d3.line .x d, i => xScale lambdaRange i .y d => yScale d ; // Draw CV curve g.append 'path' .datum cvScores .attr 'fill', 'none' .attr 'stroke', ' 2196F3' .attr 'stroke-width', 3 .attr 'd', line ; // Find and mark optimal lambda const minIndex = cvScores.indexOf Math.min ...cvScores ; const optimalLambda = lambdaRange minIndex ; g.append 'circle' .attr 'cx', xScale optimalLambda .attr 'cy', yScale cvScores minIndex .attr 'r', 5 .attr 'fill', 'red' ; // Update info document.getElementById 'cv-info' .textContent = Optimal λ: MATH cvScores minIndex .toFixed 3 ; function animatePath if isAnimating // Stop animation isAnimating = false; if animationId clearInterval animationId ; animateBtn.textContent = 'Animate Path'; return; // Start animation isAnimating = true; animateBtn.textContent = 'Stop Animation'; const lambdaRange = d3.range 0, 5, 0.1 ; let currentIndex = 0; animationId = setInterval => if currentIndex >= lambdaRange.length currentIndex = 0; // Update visualization for current lambda const currentLambda = lambdaRange currentIndex ; // Add animation logic here currentIndex++; , 100 ; // Event listeners methodSelect.addEventListener 'change', updateRegularizationPath ; alphaSlider.addEventListener 'input', updateRegularizationPath ; animateBtn.addEventListener 'click', animatePath ; // Initial update updateRegularizationPath ; ;",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_07_regularization_and_loss_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_00_subgradient",
    "title": "07 Subgradient",
    "chapter": "07",
    "order": 1,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "In this chapter, we introduce the concept of subgradient and subgradient optimality conditions, which generalize the gradient and first-order optimality conditions discussed previously. We also explore several applications and examples related to these concepts.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_00_subgradient/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_01_subgradient",
    "title": "07-01 Subgradient",
    "chapter": "07",
    "order": 2,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "Subgradient For a convex function MATH , a subgradient at MATH is any vector MATH that satisfies: > MATH \\begin equation \\label subgradient f y \\geq f x + g^T y-x , \\text for all y \\end equation MATH The subgradient defined above: - Generalizes the gradient for convex functions to cases where the function is not differentiable. - For convex functions, a subgradient always exists. If MATH is differentiable at MATH , then MATH is the unique subgradient. - For non-convex functions, a subgradient may be defined similarly, but it may not always exist depending on the function. Below are examples of subgradients for some functions. Example 1 MATH MATH - For MATH , MATH must hold. That is, MATH . If MATH , i.e., MATH , then the condition holds for all MATH . Thus, MATH Wikipedia: Sign function https://en.wikipedia.org/wiki/Sign function . - For MATH , MATH must hold. Therefore, MATH . Example 2 MATH MATH At a point MATH , - For MATH , since it is differentiable at MATH , we have MATH - For MATH , we have MATH Example3 MATH MATH - For MATH , since it is differentiable, we have MATH - For MATH , we have MATH . Therefore, MATH Example4 MATH , where MATH are both convex and differentiable functions. MATH - For MATH , we have MATH - For MATH , we have MATH - For MATH , we have MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_01_subgradient/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_02_subdifferentials",
    "title": "07-02 Sub-differentials",
    "chapter": "07",
    "order": 3,
    "owner": "Kyeongmin Woo",
    "lesson_type": "required",
    "content": "The subdifferential MATH of a convex function MATH at a point MATH is the set of all subgradients at MATH : > \\begin equation \\partial f x = \\ g \\in \\mathbb R ^n | \\text g is a subgradient of f at x \\ \\end equation The subdifferential has the following properties: - MATH is always a closed convex set, whether MATH is convex or not. - If MATH is convex, MATH always contains at least one element; if MATH is not convex, it may be empty. - If MATH is differentiable and convex at MATH , then MATH . - If MATH , then MATH is differentiable at MATH and MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_02_subdifferentials/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_02_01_connection_to_a_convexity_geometry",
    "title": "07-02-01 Connection to a Convexity Geometry",
    "chapter": "07",
    "order": 4,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "For a convex set MATH , consider the indicator function MATH defined as: > MATH I C x = I\\ x \\in C \\ = \\begin cases 0 &\\text if x \\in C \\\\ \\infty &\\text if x \\notin C \\end cases MATH The subdifferential of this function has the following geometric meaning: Lemma For MATH , the subdifferential MATH coincides with the normal cone MATH to the set MATH at MATH : > \\begin equation \\mathcal N C x = \\ g \\in \\mathbb R ^n | g^Tx \\geq g^Ty \\text for all y \\in C \\ \\end equation Proof By definition, the subgradient must satisfy: > \\begin equation I C y \\geq I C x + g^T y-x \\text for all y \\end equation Here, MATH and MATH , so: > \\begin equation I C y \\geq g^T y-x \\text for all y \\end equation First, for all MATH , we have: > \\begin equation I C y = 0 \\geq g^T y-x \\end equation Thus, the subgradient MATH must satisfy MATH . Second, for all MATH , MATH , so the inequality holds for any value of MATH : > MATH which is always true. Since the subgradient must satisfy both conditions above, the subgradient for this function becomes: > MATH This completes the proof. Fig 1 Normal cone 1",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_02_01_connection_to_a_convexity_geometry/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_02_02_subgradient_calculus",
    "title": "07-02-02 Subgradient Calculus",
    "chapter": "07",
    "order": 5,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The following basic rules hold for the subdifferential of convex functions: Scaling > MATH \\eqalign ext if & a>0, \\\\ ext then &\\partial af = a\\cdot \\partial f MATH Addition > MATH Here, the sum of two sets MATH is defined as the set of all possible sums. Affine composition > MATH \\eqalign ext if & g x =f Ax+b , \\\\ ext then & \\partial g x = A^T \\partial f Ax+b MATH Finite pointwise maximum > MATH \\eqalign ext if & f x =\\max i=1,\\dots,m f i x , \\\\ ext then & \\partial f x = \\text conv \\left \\bigcup i:f i x =f x \\partial f i x \\right MATH That is, MATH is defined as the convex hull of the union of the subdifferentials of the functions attaining the value MATH at MATH . General pointwise maximum > MATH \\eqalign ext if & f x = \\max s \\in S f s x ,\\\\ ext then & \\partial f x \\supseteq cl \\left \\ \\text conv \\left \\bigcup s:f s x =f x \\partial f s x \\right \\right\\ MATH Here, MATH may be an infinite set, so the union of infinitely many sets may not be closed. Therefore, we take the closure to ensure the subdifferential is a closed set. On the other hand, if the set MATH is compact closed and bounded and the functions MATH are continuous with respect to MATH , then the equality holds. For example, consider the following p-norm function MATH : > \\begin equation f x = \\vert \\vert x \\vert \\vert \\ p = \\max \\vert \\vert z \\vert \\vert q \\leq 1 z^Tx, \\qquad 1/p + 1/q =1 \\end equation If we let MATH , then MATH such that MATH belongs to MATH . Since MATH , the union MATH is the union of all MATH , which gives us MATH . Here, MATH is a compact set, and MATH is linear. By the general pointwise maximum rule, taking the convex hull and then the closure of MATH does not add any additional elements to the set. Therefore, the subgradient of the function MATH is as follows: > \\begin equation \\partial f x = \\arg\\max \\vert \\vert z \\vert \\vert q \\leq 1 z^T x \\end equation",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_02_02_subgradient_calculus/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_03_subgradient_optimality_condition",
    "title": "07-03 Subgradient Optimality Condition",
    "chapter": "07",
    "order": 6,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "In this section, we examine optimality conditions using subgradients, and provide several examples to illustrate their application and usefulness.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_03_subgradient_optimality_condition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_03_01_subgradient_optimality_condition",
    "title": "07-03-01 Subgradient Optimality Condition",
    "chapter": "07",
    "order": 7,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Lemma For any function MATH , the condition that MATH is a minimizer of MATH and that MATH is a subgradient at MATH are equivalent: > MATH \\begin equation f x^ = \\min x f x \\Longleftrightarrow 0 \\in \\partial f x^ \\end equation MATH Proof > MATH \\begin align &f x^ = \\min x f x \\\\ \\Longleftrightarrow &f y \\geq f x^ \\text for all y\\\\ \\Longleftrightarrow &f y \\geq f x^ + 0^T y-x^ \\\\ \\Longleftrightarrow &0 \\in \\partial f x^ \\end align MATH Note that convexity of MATH is not used in this proof, so this optimality condition applies even to non-convex functions.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_03_01_subgradient_optimality_condition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_03_02_derivation_of_first-order_optimality_condition",
    "title": "07-03-02 Derivation of First-Order Optimality Condition",
    "chapter": "07",
    "order": 8,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "First-order optimality condition: If MATH is convex and differentiable, the subgradient optimality condition coincides with the first-order optimality condition, as shown below. MATH Proof > MATH \\begin alignat 2 f x^ = \\min x\\in C f x \\quad & \\Longleftrightarrow & & \\quad f x^ = \\min x f x + I C x \\\\ \\quad & \\Longleftrightarrow & &\\quad 0 \\in \\partial f x^ + I C x^ \\\\ \\quad & \\Longleftrightarrow & &\\quad 0 \\in \\ \\nabla f x^ \\ + \\mathcal N C x^ \\\\ \\quad & \\Longleftrightarrow & &\\quad - \\nabla f x^ \\in \\mathcal N C x^ \\\\ \\quad & \\Longleftrightarrow & &\\quad - \\nabla f x^ ^Tx^ \\geq -\\nabla f x^ ^Ty, \\text for all y \\in C \\\\ \\quad & \\Longleftrightarrow & &\\quad \\nabla f x^ ^T y-x^ \\geq 0, \\text for all y \\in C \\end alignat MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_03_02_derivation_of_first-order_optimality_condition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_03_03_example_lasso_optimality_condition",
    "title": "07-03-03 Example: Lasso Optimality Condition",
    "chapter": "07",
    "order": 9,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "For the lasso problem given below, > \\begin equation \\min \\beta \\frac 1 2 \\| y-X\\beta \\| 2^2 + \\lambda \\| \\beta \\| 1 \\end equation where MATH , MATH , MATH . The subgradient optimality condition for this problem can be expressed as: > MATH \\eqalign 0 \\in \\partial\\left \\frac 1 2 \\| y-X\\beta \\| 2^2 + \\lambda \\| \\beta \\| 1\\right &\\quad \\Longleftrightarrow \\quad 0 \\in - X^T y-X\\beta + \\lambda \\partial \\| \\beta \\| 1 \\\\ &\\quad \\Longleftrightarrow \\quad X^T y-X\\beta = \\lambda v \\\\ & \\quad \\text for some v \\in \\partial \\| \\beta \\| 1 \\\\ MATH Here, for a point MATH , the subgradient MATH is given by: MATH v i = \\begin cases 1 &\\text if \\beta i > 0 \\\\ -1 &\\text if \\beta i \\begin equation X^T y-X\\beta = \\lambda v \\end equation That is, for the optimal MATH , the following conditions hold: > MATH \\begin cases X i^T y-X\\beta = \\lambda \\cdot \\text sign \\beta i &\\text if \\beta i \\neq 0 \\\\ |X i^T y-X\\beta | \\leq \\lambda &\\text if \\beta i = 0 \\end cases MATH Here, MATH denotes the MATH th column of the matrix MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_03_03_example_lasso_optimality_condition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_03_04_example_soft-thresholding",
    "title": "07-03-04 Example: Soft-Thresholding",
    "chapter": "07",
    "order": 10,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "For the simpler lasso problem with MATH : > \\begin equation \\min \\beta \\frac 1 2 \\| y-\\beta \\| 2^2 + \\lambda \\| \\beta \\| 1 \\end equation From the previous example, the subgradient optimality condition is: > MATH \\begin cases y i-\\beta i = \\lambda \\cdot \\text sign \\beta i &\\text if \\beta i \\neq 0 \\\\ |y i-\\beta i| \\leq \\lambda &\\text if \\beta i = 0 \\end cases MATH From this condition, the solution MATH can be found, where > MATH S \\lambda y i = \\begin cases y i - \\lambda &\\text if y i > \\lambda \\\\ 0 &\\text if -\\lambda \\leq y i \\leq \\lambda, \\quad \\quad i \\in \\ 1,2,\\dots,n \\ \\\\ y i + \\lambda &\\text if y i MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_03_04_example_soft-thresholding/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter07/07_03_05_example_distance_to_convex_set",
    "title": "07-03-05 Example: Distance to a Convex Set",
    "chapter": "07",
    "order": 11,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The distance function to a closed convex set MATH is defined as: > \\begin alignat 1 dist x,C & = \\min y \\in C \\| y-x \\| 2 \\\\ & = \\| x-P C x \\| 2 \\\\ & \\geq 0 \\end alignat Here, MATH is the projection of the point MATH onto the set MATH , i.e., the closest point in MATH to MATH . The subgradient of the distance function is: > \\begin equation \\partial dist x,C = \\ \\frac x-P C x \\| x-P C x \\| 2 \\ \\end equation Proof If MATH , then by the first-order optimality condition, > \\begin equation x-u ^T y-u \\leq 0 \\ \\text for all y \\in C \\end equation Thus, > \\begin equation C \\subseteq H = \\ y: x-u ^T y-u \\leq 0 \\ \\end equation i For MATH , > \\begin equation x-u ^T y-u \\leq 0 \\end equation On the other hand, since MATH , > \\begin equation dist y,C \\geq \\frac x-u ^T y-u \\| x-u \\| 2 \\text for all y \\in H \\end equation ii For MATH , > \\begin equation x-u ^T y-u = \\| x-u \\| 2 \\| y-u \\| 2 \\cos\\theta, \\end equation where MATH is the angle between MATH and MATH . Then, > MATH \\eqalign dist y,C &\\geq dist y,H \\\\ &= \\| y-u \\| 2 \\cos \\theta \\\\ &= \\frac x-u ^T y-u \\| x-u \\| 2 \\text for all y \\notin H MATH Therefore, from i and ii , for all MATH , > MATH \\eqalign dist y,C &\\geq \\frac x-u ^T y-u \\| x-u \\| 2 \\\\ &= \\frac x-u ^T y-x+x-u \\| x-u \\| 2 \\\\ & = \\| x-u \\| 2 + \\left \\frac x-u \\| x-u \\| 2 \\right ^T y-x MATH In conclusion, MATH has the following subgradient at MATH : > MATH Moreover, the subdifferential MATH contains only one element, so MATH is differentiable and its derivative coincides with the subgradient.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter07/07_03_05_example_distance_to_convex_set/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_00_subgradient_method",
    "title": "08 Subgradient Method",
    "chapter": "08",
    "order": 1,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "In this section, we will look at the subgradient method, which can be applied to convex functions that may not be differentiable, using the concept of subgradients. We will also explore the convergence properties and rate of the subgradient method with examples.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_00_subgradient_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_subgradient_method",
    "title": "08-01 Subgradient Method",
    "chapter": "08",
    "order": 2,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Assume the domain of the function is MATH and there is a convex function MATH that may not be differentiable everywhere. The subgradient method is defined by replacing the gradient in gradient descent with a subgradient. MATH > MATH Here, MATH , i.e., MATH is a subgradient of MATH at MATH . Subgradient method not subgradient \"descent\" Unlike gradient descent, the subgradient method does not always guarantee descent at each step hence the name is not subgradient \"descent\" . Therefore, when using the subgradient method, it is important to track the best result at each iteration. > MATH MATH denotes the minimum value of the function MATH obtained over MATH iterations of the subgradient method.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_subgradient_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_01_step_size_choices",
    "title": "08-01-01 Step size choices",
    "chapter": "08",
    "order": 3,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "There are various ways to choose the step size in the subgradient method. Let’s take a closer look at the following two approaches: - Fixed step sizes : MATH , where MATH - Diminishing step sizes : MATH that satisfy the following conditions: >\\begin align > \\sum k=1 ^ \\infty t k = \\infty, \\quad \\sum k=1 ^ \\infty t k^ 2 \\end align Example of Diminishing step sizes > MATH \\begin align & t k = \\frac 1 k , k = 1,2,3,... \\\\ & \\sum k=1 ^ \\infty t k = \\infty \\quad \\text Harmonic series \\\\ & \\sum k=1 ^ \\infty t^2 k \\approx 1.644934 < \\infty \\quad \\text Basel problem \\\\ \\end align MATH A key feature of the step size in the subgradient method is that it must be set in advance, unlike in gradient descent. In other words, unlike the backtracking line search in gradient descent, the step size in the subgradient method does not adapt to the curvature of the function.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_01_step_size_choices/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_02_basic_inequality",
    "title": "08-01-02 Basic Inequality",
    "chapter": "08",
    "order": 4,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The convergence theorem and convergence rate of the subgradient method can be proved using the following basic inequality. Basic Inequality > MATH \\begin align f best ^ k - f^ \\quad \\le \\quad \\frac R^ 2 +G^ 2 \\sum i=1 ^ k \\alpha i ^ 2 2\\sum i=1 ^ k \\alpha i \\end align MATH Proof If MATH is the optimal point of the function MATH , then the following equation holds: > MATH \\begin alignat 1 \\Vert x^ k+1 -x^ \\Vert 2^ 2 & \\quad = \\quad \\Vert x^ k -\\alpha k g^ k -x^ \\Vert 2^ 2 \\\\ & \\quad = \\quad \\Vert x^ k -x^ -\\alpha k g^ k \\Vert 2^ 2 \\\\ & \\quad = \\quad \\Vert x^ k -x^ \\Vert 2^2 - 2 \\alpha k g^ k T x^ k -x^ +\\alpha k^2 \\Vert g^ k \\Vert 2^2 \\\\ \\end alignat MATH From the definition of subgradient, the following inequality holds: > MATH \\begin alignat 1 f x^ \\ge f x^ k + g^ k T x^ -x^ k & \\quad \\Longleftrightarrow \\quad f x^ -f x^ k \\ge g^ k T x^ -x^ k \\\\ & \\quad \\Longleftrightarrow \\quad f x^ k - f x^ \\le g^ k T x^ k -x^ \\\\ & \\quad \\Longleftrightarrow \\quad -2\\alpha k f x^ k - f x^ \\ge -2\\alpha k g^ k T x^ k -x^ \\\\ & \\quad \\Longleftrightarrow \\quad -2\\alpha k g^ k T x^ k -x^ \\le -2\\alpha k f x^ k -f x^ \\\\ \\end alignat MATH Using the above equations and inequalities, the following inequality can be derived:",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_02_basic_inequality/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_03_convergence_analysis",
    "title": "08-01-03 Convergence analysis",
    "chapter": "08",
    "order": 5,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "In gradient descent, it is assumed that MATH is Lipschitz continuous, but in the subgradient method, we assume that MATH itself is Lipschitz continuous. See the convergence theorem for gradient descent 06-03-01 % multilang post url contents/chapter06/21-03-20-06 03 01 convergence analysis and proof % for reference. Assume MATH is convex, dom MATH , and MATH satisfies the Lipschitz condition: >\\begin align > | f x - f y | \\le G \\lVert x - y \\rVert 2 \\text for all x, y \\end align Given these assumptions, the convergence formulas for fixed and diminishing step sizes are as follows: Convergence theorem for fixed step sizes Fixed step sizes have the following convergence property: >\\begin align > \\lim k\\to\\infty f x^ k best \\le f^ + \\frac G^ 2 t 2 \\end align Convergence theorem for diminishing step sizes The diminishing step sizes method has the following convergence property: >\\begin align \\lim k\\to\\infty f x^ k best = f^ \\end align Proofs The proofs for fixed and diminishing step sizes are as follows. Proof of convergence theorem for fixed step sizes The fixed step size method uses MATH in its proof. > MATH \\begin align & f best ^ k - f^ \\le \\frac R^ 2 +G^ 2 \\sum i=1 ^ k t i ^ 2 2\\sum i=1 ^ k t i = \\frac R^ 2 +G^ 2 k t^ 2 2kt = \\frac R^ 2 2tk + \\frac G^ 2 t 2 \\\\ & \\lim k→\\infty f^ k best - f^ \\le 0 + \\frac G^ 2 t 2 = \\frac G^ 2 t 2 \\\\ & \\lim k→\\infty f^ k best \\le f^ + \\frac G^ 2 t 2 \\end align MATH Proof of convergence theorem for diminishing step sizes The proof for the diminishing step sizes uses the following properties 1 and 2 : > MATH \\begin align \\text 1 \\sum i=1 ^ \\infty t i = \\infty, \\quad \\text 2 \\sum i=1 ^ \\infty t i^ 2 = \\beta MATH \\begin align & f best ^ k - f^ \\le \\frac R^ 2 +G^ 2 \\sum i=1 ^ k t i ^ 2 2\\sum i=1 ^ k t i \\\\ & \\lim k→\\infty f^ k best - f^ \\le \\frac R^ 2 +G^ 2 \\beta 2\\infty = 0 \\\\ & \\lim k→\\infty f^ k best = f^ \\\\ \\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_03_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_04_convergence_rate",
    "title": "08-01-04 Convergence rate",
    "chapter": "08",
    "order": 6,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The convergence rate describes how the number of iterations required to reach an MATH -suboptimal point depends on MATH , using Big-O notation https://en.wikipedia.org/wiki/Big O notation . For example, if MATH and the convergence rate is MATH , then about MATH iterations are needed. Let’s use 08-01-02 Basic inequality % multilang post url contents/chapter08/20-03-29-08 01 02 basic inequality % to derive the convergence rate for the subgradient method with fixed step sizes. > MATH >\\begin align > f^ k best - f^ \\quad \\le \\quad \\frac R^ 2 2kt + \\frac G^ 2 t 2 \\end align Suppose MATH satisfies MATH and MATH where MATH is the suboptimality gap, MATH is the Lipschitz constant, and MATH is the distance between the starting point and the optimal point . Then MATH . If MATH , then MATH , and MATH leads to MATH . This means the number of iterations required is at least MATH to achieve MATH . The convergence rate of this algorithm is MATH , which means it requires significantly more iterations than the gradient descent method, which has a rate of MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_04_convergence_rate/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_05_example_regularized_logistic_regression",
    "title": "08-01-05 Example: Regularized Logistic Regression",
    "chapter": "08",
    "order": 7,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Suppose MATH for MATH . The logistic regression loss is defined as: > \\begin align f \\beta = \\sum i=1 ^n\\big -y ix i^T\\beta + \\log 1+\\exp x i^T\\beta \\big \\end align This function is a finite sum of a linear function and a log-sum-exp function, so it is a differentiable convex function. Now, the regularization problem for MATH is formulated as: > \\begin align \\min \\beta \\text f \\beta + \\lambda \\cdot P \\beta \\end align Here, MATH can be defined as MATH ridge penalty or MATH lasso penalty . The loss function with ridge penalty remains a differentiable convex function, but the loss function with lasso penalty becomes a nondifferentiable convex function. For these two loss functions, we can apply gradient descent for ridge and the subgradient method for lasso, and by plotting the objective function value at iteration MATH , we can observe the convergence characteristics of both methods. Fig 1 Gradient descent vs Subgradient method 3 This experiment shows that gradient descent converges much faster than the subgradient method.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_05_example_regularized_logistic_regression/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_06_polyak_step_sizes",
    "title": "08-01-06 Polyak step sizes",
    "chapter": "08",
    "order": 8,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Polyak step sizes are a way to set the step size when the optimal value is known. If MATH is known, Polyak step sizes can be defined as follows. Convergence theorem for Polyak step-sizes > MATH \\begin align t k = \\frac f^ k-1 -f^ \\Vert g^ k-1 \\Vert 2^ 2 , \\quad k = 1,2,3... \\end align MATH Proof of convergence theorem for Polyak step-sizes The proof can be derived from the basic inequality % multilang post url contents/chapter08/20-03-29-08 01 02 basic inequality % and the sequence of inequalities used there. > MATH \\begin align \\Vert x^ k -x^ \\Vert 2^ 2 \\quad \\le \\quad \\Vert x^ k-1 -x^ \\Vert 2^ 2 -2t k f x^ k-1 -f^ +t k^ 2 \\Vert g^ k-1 \\Vert 2^ 2 \\\\ \\end align MATH By differentiating the right-hand side above with respect to MATH and setting it to zero, we obtain the Polyak step size that minimizes the right-hand side. > MATH \\begin align & \\frac \\partial \\partial t k \\Vert x^ k-1 -x^ \\Vert 2^ 2 -2t k f x^ k-1 -f^ +t k^ 2 \\Vert g^ k-1 \\Vert 2^ 2 = 0 \\\\ \\Longleftrightarrow & -2 f x^ k-1 -f^ +2t k \\Vert g^ k-1 \\Vert 2^ 2 = 0 \\\\ \\Longleftrightarrow & f x^ k-1 -f^ = t k \\Vert g^ k-1 \\Vert 2^ 2 \\\\ \\Longleftrightarrow & t k = \\frac f x^ k-1 -f^ \\Vert g^ k-1 \\Vert 2^ 2 \\quad \\text Polyak step size at k \\end align MATH The convergence rate of the Polyak step size can also be derived from the basic inequality % multilang post url contents/chapter08/20-03-29-08 01 02 basic inequality % and the sequence of inequalities used there. Convergence rate for Polyak step-sizes Let’s substitute the Polyak step size MATH into the basic inequality derived in the basic inequality % multilang post url contents/chapter08/20-03-29-08 01 02 basic inequality % . > MATH \\begin align & 2\\sum i=1 ^ k t i f x^ i -f^ \\le R^2 + \\sum i=1 ^kt i^2 \\Vert g^ i \\Vert 2^2 \\\\ \\Longleftrightarrow \\quad & 2\\sum i=1 ^ k \\frac f x^ i -f^ ^2 \\Vert g^ i \\Vert 2^2 \\le R^2 + \\sum i=1 ^k\\frac f x^ i -f^ ^2 \\Vert g^ i \\Vert 2^2 \\\\ \\Longleftrightarrow \\quad & \\sum i=1 ^ k \\frac f x^ i -f^ ^2 \\Vert g^ i \\Vert 2^2 \\le R^2 \\\\ \\end align MATH Assuming that the Lipschitz condition MATH always holds, the above inequality can be rearranged as follows: > MATH \\begin align & \\sum i=1 ^ k f x^ i -f^ ^2 \\le R^2G^2 \\\\ \\Longleftrightarrow \\quad & k ⋅ f x^ i -f^ ^2 \\le R^2G^2 \\\\ \\Longleftrightarrow \\quad & \\sqrt k ⋅ f x^ i -f^ \\le RG \\\\ \\Longleftrightarrow \\quad & f x^ i -f^ \\le \\frac RG \\sqrt k \\\\ \\end align MATH If we let MATH , then MATH , so the number of trials required to reach a suboptimal point with respect to MATH is on the order of MATH . In other words, the convergence rate is MATH , which is the same as other subgradient methods.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_06_polyak_step_sizes/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_07_example_intersection_of_sets",
    "title": "08-01-07 Example: Intersection of sets",
    "chapter": "08",
    "order": 9,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Suppose we want to find the intersection point of several closed convex sets. First, let’s define MATH as the distance from a point MATH to the set MATH , and MATH as the maximum distance from MATH to all sets MATH : > MATH \\begin align f i x & = \\mathbb dist x, C i , i=1,...,m \\\\ f x & = \\max 1,...,m \\text f i x \\end align MATH Using these two functions, the problem of finding the intersection of convex sets can be formulated as the following minimization problem: > MATH \\begin align min x \\text f x \\end align MATH The problem of finding the intersection point of convex sets is equivalent to finding the point MATH that minimizes the maximum distance MATH to the sets MATH . In this case, the objective function MATH is convex. If all sets have a common intersection point, then MATH and the optimal point is MATH . Gradient of distance function In the previous section % multilang post url contents/chapter07/21-03-25-07 03 05 example distance to convex set % , we defined the distance to a convex set as MATH , and saw that the gradient of this function is: > MATH \\begin align \\partial dist x,C = \\frac x-P C x \\Vert x-P C x \\Vert 2 \\end align MATH Here, MATH is the projection of the point MATH onto the set MATH . Subdifferential of finite pointwise maximum Finite pointwise maximum 함수 MATH 에 대한 subdifferential은 다음과 같이 정의 된다. > MATH \\begin align \\partial f x = \\text conv \\left \\bigcup i:f i x =f x \\partial f i x \\right \\end align MATH 즉, MATH 의 subdifferential은 그 지점의 모든 subdifferential MATH 의 합집합에 대한 convex hull로 정의된다. 만약 MATH 이고 MATH 이라면 MATH 이다. Deriving subgradient updating steps 이전 장 % multilang post url contents/chapter07/21-03-25-07 03 05 example distance to convex set % 에서 보았던 MATH 는 다음과 같은 subgradient를 가진다. > MATH MATH \\begin align g i = \\nabla f i x = \\frac x-P C i x \\Vert x-P C i x \\Vert 2 \\end align MATH 만약 컨벡스 집합의 교차점이 있다면 우리는 MATH 임을 바로 알 수 있기에 Polyak step sizes를 사용할 수 있다. 위 subgradient 수식을 보면 MATH 가 정규화된 형태이므로 MATH 이다. 결국 Polyak step size MATH 에 우리가 알고 있는 값을 대입하면 다음과 같은 subgradient method 공식을 도출할 수 있다. > MATH \\begin align x^ k & = x^ k-1 - t k ⋅g k-1 \\\\ & = x^ k-1 - \\frac f^ k-1 -f^ \\Vert g^ k-1 \\Vert 2^ 2 \\frac x^ k-1 -P C i x \\Vert x^ k-1 -P C i x \\Vert 2 \\\\ & = x^ k-1 - f x^ k-1 \\frac x^ k-1 -P C i x \\Vert x^ k-1 -P C i x \\Vert 2 \\end align MATH 여기서 Polyak size인 MATH 는 MATH 이므로 subgradient method는 아래와 같이 정리된다. > MATH \\begin align x^ k = P C i x^ k-1 \\end align MATH 이 문제는 그림으로 표현하면 각 스텝에서 가장 가까운 컨벡스 함수에 projection을 반복하는 형태이다. Fig 2 Alternating Projection Algorithm 10",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_07_example_intersection_of_sets/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_01_08_projected_subgradient_method",
    "title": "08-01-08 Projected Subgradient Method",
    "chapter": "08",
    "order": 10,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The method described in the previous example is called the projected subgradient method. This algorithm can be used for convex problems with constraints. If we denote the domain that satisfies the constraints as the set MATH , then a constrained convex problem is defined as follows: > MATH \\begin align \\min x \\text f x \\quad \\text subject to x \\in C \\end align MATH By using the projected subgradient method, such problems can be solved relatively easily. The projected subgradient method is similar to the standard subgradient method, but at each iteration, the result is projected onto the set MATH . > MATH \\begin align x^ k = P c x^ k-1 - t k \\cdot g^ k-1 , \\quad k = 1,2,3,... \\end align MATH If projection is possible, this method has the same convergence properties and rate as the subgradient method. One thing to note about the projected subgradient method is that even if MATH is a simple convex set, if the projection operation MATH is difficult, the overall problem also becomes hard to solve. Typically, the following sets MATH are relatively easy to project onto: - Affine images: MATH - Solution set of linear system: MATH - Nonnegative orthant: MATH MATH - Some norm balls: MATH for MATH - Some simple polyhedra and simple cones",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_01_08_projected_subgradient_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_02_stochastic_subgradient_method",
    "title": "08-02 Stochastic Subgradient Method",
    "chapter": "08",
    "order": 11,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The stochastic subgradient method is similar to stochastic gradient descent, but replaces the gradient with a subgradient.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_02_stochastic_subgradient_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_02_01_stochastic_subgradient_method",
    "title": "08-02-01 Stochastic Subgradient Method",
    "chapter": "08",
    "order": 12,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Let’s consider the problem of minimizing the sum of functions as follows: > MATH \\begin equation \\min x \\sum i=1 ^m f i x \\end equation MATH If we apply the subgradient method to this problem, we need to compute the subgradient for each function MATH and sum them. This is similar to the method introduced in stochastic gradient descent % multilang post url contents/chapter06/21-03-20-06 05 stochastic gradient descent % . In summary, the stochastic subgradient method takes the following form: > MATH \\begin align x^ k = x^ k-1 - t k \\cdot g i k ^ k-1 , \\quad k = 1, 2, 3, . . . \\end align MATH Here, MATH is the index chosen at the MATH -th iteration. As will be discussed in the next section on the convergence rate of the stochastic subgradient method, the choice of cyclic or random method affects the result. MATH , and this update direction is different from the usual subgradient method % multilang post url contents/chapter08/20-03-29-08 01 subgradient method % also called batch subgradient method or full batch subgradient method , where MATH is used. If each MATH is differentiable, this algorithm becomes stochastic gradient descent. The stochastic subgradient method is a more general form",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_02_01_stochastic_subgradient_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_02_02_convergence_of_stochastic_methods",
    "title": "08-02-02 Convergence of Stochastic Methods",
    "chapter": "08",
    "order": 13,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Assume each function MATH is convex and Lipschitz continuous with constant G. For the stochastic subgradient method, the following properties hold for fixed and diminishing step sizes: - Fixed step sizes for MATH , MATH > MATH \\text For cyclic and randomized methods with fixed step sizes, the following holds: \\\\ \\begin align \\lim k\\to\\infty f x best ^ k \\le f^ + 5m^ 2 G^ 2 t/2 \\end align MATH Here, MATH is the Lipschitz constant of MATH . - Diminishing step sizes > MATH \\text For cyclic and randomized methods with diminishing step sizes, the following holds: \\\\ \\begin align \\lim k\\to\\infty f x best ^ k = f^ \\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_02_02_convergence_of_stochastic_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_02_03_convergence_rate_of_stochastic_method",
    "title": "08-02-03 Convergence Rate of Stochastic Method",
    "chapter": "08",
    "order": 14,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "There are differences in the convergence rates between cyclic and randomized methods. The convergence rate % multilang post url contents/chapter08/20-03-29-08 01 04 convergence rate % of the batch subgradient method is MATH , where MATH is the Lipschitz constant of MATH . - Cyclic method : The iteration complexity of the cyclic method is MATH . If one cycle of the cyclic stochastic subgradient method is considered equivalent to one batch subgradient method, then each cycle requires MATH iterations. MATH is the Lipschitz constant of a single function MATH - Randomized method : The iteration complexity of the randomized method is MATH . That is, the randomized method requires MATH iterations, which is MATH times faster than the batch and cyclic methods with MATH . In terms of Big-O notation, if MATH is large, the randomized method has a faster convergence rate. Although the Big-O bounds for randomized and cyclic methods differ by a factor of MATH , note that the cyclic method's Big-O bound is worst-case, while the randomized method's is average-case. In practice, the difference may not be as large as the Big-O notation suggests.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_02_03_convergence_rate_of_stochastic_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_02_04_batch_vs_stochastic_methods",
    "title": "08-02-04 Batch vs Stochastic Methods",
    "chapter": "08",
    "order": 15,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The convergence properties of batch and stochastic methods are as follows: Generally, the stochastic method quickly approaches the optimal point in the early stages, but may not converge as well near the optimal point. In contrast, the batch method converges more slowly but approaches the optimal point more accurately. The figure below compares the convergence of batch and stochastic methods for logistic regression https://en.wikipedia.org/wiki/Logistic regression without regularization : Fig 3 Batch vs Stochastic Gradient Descent 2",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_02_04_batch_vs_stochastic_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter08/08_03_improving_on_the_subgradient_method",
    "title": "08-03 Improving on the Subgradient Method",
    "chapter": "08",
    "order": 16,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "The subgradient method is advantageous because it can be used for nondifferentiable convex functions, making it more general. However, its convergence rate is MATH , which is much slower than the convergence rate of gradient descent, MATH . Is there a way to combine the strengths of gradient descent and the subgradient method? In the next section, we will learn about the proximal gradient descent method, which combines the advantages of both algorithms.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter08/08_03_improving_on_the_subgradient_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_proximal_gradient_descent_and_acceleration",
    "title": "Proximal gradient descent and acceleration",
    "chapter": "09",
    "order": 1,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Proximal gradient descent and acceleration This chapter introduces proximal gradient descent and acceleration techniques for optimization problems involving composite functions. Overview - Proximal gradient descent is used for problems where the objective can be split into a differentiable part and a non-differentiable part. - Acceleration methods, such as Nesterov's acceleration and FISTA, can improve convergence rates. Structure - Section 1: Proximal gradient descent - Section 2: Convergence analysis - Section 3: Matrix completion example - Section 4: Special cases - Section 5: Acceleration methods Refer to each section for details and mathematical formulations.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_proximal_gradient_descent_and_acceleration/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_01_proximal_gradient_descent",
    "title": "09-01 Proximal gradient descent",
    "chapter": "09",
    "order": 2,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Proximal gradient descent Proximal gradient descent is a method for finding the optimum by decomposing the objective function into a differentiable part and a non-differentiable part. In this section, we will look at how to define functions and find the optimum in proximal gradient descent. Decomposable functions Suppose the objective function MATH can be decomposed into two functions MATH and MATH . > MATH Here, the two functions MATH and MATH have the following properties: MATH is convex and differentiable. dom MATH MATH is convex and non-differentiable. If MATH is differentiable, you can find the next position using gradient descent: > MATH Note In gradient descent, the function MATH is approximated near MATH by a second-order Taylor expansion, and the Hessian MATH is replaced by MATH . The minimum of this approximation is chosen as the next position. See Chapter 6 Gradient descent for details > \\begin align x^+ = \\underset z \\text argmin \\underbrace f x + \\nabla f x ^T z - x + \\frac 1 2t \\parallel z - x \\parallel 2 ^2 \\tilde f t z \\end align However, if MATH is not differentiable, you cannot use gradient descent. But if MATH is composed as MATH , can't we approximate the differentiable part MATH with a quadratic? This idea leads to Proximal gradient descent . In this method, you adjust to the best position that is close to the location predicted by gradient descent for MATH and also makes the non-differentiable function MATH small. This process can be expressed as follows: > MATH \\begin align x^+ & = \\underset z \\text argmin \\tilde g t z + h z \\\\ & = \\underset z \\text argmin \\ g x + \\nabla g x ^T z - x + \\frac 1 2t \\parallel z - x \\parallel 2 ^2 + h z \\\\ & = \\underset z \\text argmin \\nabla g x ^T z - x + \\frac 1 2t \\parallel z - x \\parallel 2 ^2 + \\frac t 2 \\parallel \\nabla g x \\parallel 2 ^2 + h z \\\\ & = \\underset z \\text argmin \\frac 1 2t \\left 2t \\nabla g x ^T z - x + \\parallel z - x \\parallel 2 ^2 + t^2 \\parallel \\nabla g x \\parallel 2 ^2 \\right + h z \\\\ & = \\underset z \\text argmin \\frac 1 2t \\left \\parallel z - x \\parallel 2 ^2 + 2t \\nabla g x ^T z - x + t^2 \\parallel \\nabla g x \\parallel 2 ^2 \\right + h z \\\\ & = \\underset z \\text argmin \\frac 1 2t \\parallel z - x - t \\nabla g x \\parallel 2 ^2 + h z \\\\ \\end align MATH When moving from the 2nd to the 3rd line, the term MATH is removed as a constant term with respect to z, and the term MATH is added. In the final equation, the first term MATH is the term that brings it closer to the gradient update position of MATH , and the second term MATH is the term that reduces MATH . Proximal gradient descent Proximal gradient descent starts from an initial point MATH and iteratively applies the following update: > MATH , MATH Here, MATH is defined as the proximal mapping: > \\begin align \\text prox t x = \\underset z \\arg \\min \\frac 1 2t \\parallel x - z \\parallel 2^2 + h z \\end align If we rewrite this in the update form we have seen so far, it becomes: > \\begin align x^ k = x^ k-1 - t k \\cdot G t k x^ k-1 , \\space \\space \\text where \\space G t x = \\frac x-\\text prox t x - t \\nabla g x t \\\\ \\end align What good did this do? What is the benefit of doing this? Could it be just changing the problem to another form of minimization problem? The key point is that for most of the main MATH functions, MATH can be computed analytically. In other words, it can be calculated as follows: The mapping function MATH depends only on MATH , not on MATH . The function MATH can be very complex, but here we only need to compute the gradient MATH . The convergence analysis will be done concerning the number of iterations of the algorithm. Note that in each iteration, computing MATH may vary in computational cost depending on MATH . Example: ISTA Let's look at an example of proximal gradient descent. In the previous chapter, the lasso criterion was defined as follows when MATH and MATH : > \\begin align f \\beta = \\frac 1 2 \\parallel y - X\\beta \\parallel 2^2 + \\lambda \\parallel \\beta \\parallel 1 \\\\ \\end align Here, MATH and MATH . In this case, the proximal mapping is defined as: > MATH \\begin align \\text prox t \\beta & = \\underset z \\text argmin \\frac 1 2t \\parallel \\beta - z \\parallel 2^2 + \\lambda \\parallel z \\parallel 1 \\\\ & = S \\lambda t \\beta \\\\ \\end align MATH MATH is the soft-thresholding operator, defined as follows. For more details, see Chapter 7 Subgradient > MATH \\begin align S \\lambda t \\beta i = \\begin cases \\beta i - \\lambda & \\mbox if \\beta i \\gt \\lambda \\\\ 0 & \\mbox if \\lambda \\le \\beta i \\le \\lambda, i = 1, ..., n \\\\ \\beta i + \\lambda & \\mbox if \\beta i \\lt -\\lambda \\\\ \\end cases \\\\ \\end align MATH Since the gradient of MATH is MATH , the proximal gradient update is defined as: > MATH This algorithm is a very simple algorithm called the iterative soft-thresholding algorithm ISTA . Beck and Teboulle 2008 , \"A fast iterative shrinkage-thresholding algorithm for linear inverse problems\" In the following figure, you can clearly see the performance difference between the subgradient method and proximal gradient descent. In terms of the number of iterations, the performance of proximal gradient descent is comparable to that of gradient descent. Fig 1 Example of proximal gradient ISTA vs. subgradient method convergence rate 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_01_proximal_gradient_descent/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_02_convergence_analysis",
    "title": "09-02 Convergence analysis",
    "chapter": "09",
    "order": 3,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Convergence analysis In this section, we analyze the convergence of proximal gradient descent. Convergence Analysis For the objective function MATH , we assume the following: MATH is convex, differentiable, and dom MATH , and MATH is Lipschitz continuous MATH . MATH is convex, and MATH needs to be computed. Convergence Theorem > Proximal gradient descent satisfies the following equation for a fixed step size MATH : >\\begin align f x^ k - f^ \\ \\le \\frac \\lVert x^ 0 - x^ \\ \\rVert^2 2 2tk \\end align Proximal gradient descent has a convergence rate of MATH or MATH , which is the same performance as gradient descent. However, this performance is based on the number of iterations, not the number of operations. The number of operations can vary depending on the function MATH . Backtracking line search The backtracking line search method of proximal gradient descent is similar to gradient descent but operates only on the smooth part MATH , not the function MATH . First, set the parameter to MATH 0 \\begin align g x - tG t x > g x - t \\nabla g x ^T G t x + \\frac t 2 \\parallel G t x \\parallel 2 ^2 \\end align This backtracking condition means that the value of the function MATH at the next step location MATH is greater than the value of the Taylor second-order approximation of the function MATH . If MATH in this equation, we can see that MATH , which becomes identical to the backtracking condition of gradient descent. Note: For more details on the backtracking of gradient descent, refer to 06-02-02 backtracking line search % multilang post url contents/chapter06/21-03-20-06 02 02 backtracking line search % . Backtracking line search algorithm If we summarize this in an algorithm, it is as follows. However, MATH 1. Initialize the parameters. MATH , MATH 2. In each iteration, initialize MATH . MATH 3. If the condition MATH is satisfied, reduce MATH . Repeat step 3 while this condition is met. 4. Perform the gradient descent update MATH . 5. If the termination condition is not met, go back to step 2. When backtracking in proximal gradient descent, since MATH is computed repeatedly, the proximal mapping included in MATH is also computed repeatedly. Since the proximal mapping is very costly to compute, the overall computational cost of backtracking can be high. Convergence Theorem Under the same assumptions as above, the backtracking line search method also satisfies the same performance. > Proximal gradient descent satisfies the following equation for backtracking line search. The step size is MATH . > MATH f x^ k −f^ \\star ≤ \\frac \\lVert x^ 0 − x^ \\star \\rVert 2 ^ 2 2 t min k , \\space t \\text min = \\text min \\ 1, \\beta / L \\ \\\\ MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_02_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_03_example_matrix_completion",
    "title": "09-03 Example: matrix completion",
    "chapter": "09",
    "order": 4,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Example: matrix completion In various applications, measured data is often represented as a matrix. In these cases, most entries in the matrix may be empty, with only a few entries containing observed data, leading to a sparse matrix. The challenge of filling in the missing entries in such a matrix is referred to as the Matrix completion problem. For instance, this issue can occur in recommendation systems when suggesting products or services to customers who have not yet made a purchase. Matrix Completion Problem The Matrix completion problem can be articulated as follows: Let the matrix MATH represent the observed data, and let MATH denote the entries that contain observed data. The matrix MATH is the estimated matrix used to predict the missing entries. > MATH The first term in the objective function aims to minimize the error between matrix MATH and the observed data, while the second term encourages matrix MATH to be low-rank. It is assumed that matrix B resides on a low-dimensional manifold. Consequently, matrix MATH fills in the missing entries with the lowest-dimensional values that align with the observed data. Reference Trace Norm The trace norm of a matrix is defined as the sum of its singular values. > MATH Here, MATH is positive semi-definite, and the singular values are ordered as MATH . Reference L1 Norm Regularizer vs. Trace Norm Regularizer This problem can be interpreted as matrix soft-thresholding, where the vector in the original soft-thresholding is substituted with a matrix. In the regularizer term, the L1 norm regularizer for vectors MATH is replaced by the trace norm regularizer MATH for matrices, and indeed, the functions of the two regularizers are analogous. The L1 norm regularizer induces sparsity in the vector, while the trace norm regularizer similarly induces sparsity in the singular value vector of the matrix. When the matrix is diagonal, the diagonal elements can be viewed as the singular value vector, and the trace norm regularizer minimizes the sum of the singular values, promoting sparsity in the singular value vector. In this context, the trace norm MATH serves as an approximation for MATH . Proximal gradient descent When this problem is framed using a projection operator, proximal gradient descent can be effectively utilized. Projection Operator Let's define the projection operator MATH for the observed values as follows: > MATH P \\Omega B ij = > \\begin cases B ij , & i,j ∈ \\Omega \\\\\\ 0, & i,j \\notin \\Omega > \\end cases MATH Objective Function Utilizing the projection operator, the objective function is defined as: > MATH Now, the function MATH consists of a differentiable part MATH and a non-differentiable part MATH . Proximal Mapping To apply proximal gradient descent, we need to compute the gradient of the function MATH and define the proximal mapping. Gradient of MATH : MATH Proximal mapping: > MATH \\begin align \\text prox t B = \\underset Z \\text argmin \\frac 1 2t \\Vert B−Z \\Vert F^2 + \\lambda \\Vert Z \\Vert tr \\end align MATH Matrix SVD & Soft-thresholding The proximal mapping corresponds to matrix soft-thresholding at level MATH : MATH . Typically, matrix MATH is large, so Singular Vector Decomposition SVD is employed to minimize the computational load. If we perform SVD such that MATH , then MATH can be defined as: > MATH Here, MATH is a diagonal matrix defined as follows: > MATH Reference MATH inducement How is this expression derived? Assuming MATH , we have: Differentiating the right-hand side of MATH with respect to Z yields the following result. > MATH Rearranging this expression gives: > MATH \\begin align Z & = B - \\lambda t \\cdot \\partial \\lVert Z \\rVert tr \\\\ & = U \\Sigma V^T - \\lambda t \\cdot UV^T + W \\\\ & = U \\Sigma - \\lambda t V^T - \\lambda t W \\\\ & = U \\Sigma - \\lambda V^T \\\\ \\end align MATH The final expression can be obtained when the Lipschitz constant is MATH and MATH is 0. Thus, we have MATH , leading to the derivation of the following expression: > MATH Reference Derivative of Trace Norm If MATH , the derivative of the trace norm is given by: > MATH Here, MATH represents the operator norm, with the largest singular value being less than 1. Additionally, MATH is orthogonal to both MATH and MATH . For proof, refer to Derivative of the nuclear norm with respect to its argument https://math.stackexchange.com/questions/701062/derivative-of-the-nuclear-norm-with-respect-to-its-argument Proximal gradient update Now, let's define the proximal gradient update equation. > MATH When MATH , MATH is Lipschitz continuous, allowing us to choose a fixed step size of MATH . Consequently, the update equation simplifies to: > MATH Here, MATH represents the projection onto the unobserved values, satisfying the equation MATH . In this equation, MATH denotes the observed part, while MATH represents the missing part. The MATH function is straightforward to compute, as it only requires performing SVD and calculating MATH . Soft-Impute Algorithm This algorithm is known as Soft-Impute and provides a simple yet effective solution for matrix completion. When dealing with large matrices, the computational cost of SVD can be high. However, due to the sparsity of MATH and the low rank of MATH in this algorithm, SVD can be performed efficiently. For more detailed information, please refer to the paper: Mazumder et al. 2011 , \"Spectral regularization algorithms for learning large incomplete matrices\"",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_03_example_matrix_completion/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_04_special_cases",
    "title": "09-04 Special cases",
    "chapter": "09",
    "order": 5,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Special cases Proximal gradient descent is also called composite gradient descent or generalized gradient descent. Why is it called \"generalized” ? The reason is that Proximal gradient descent encompasses all of the following special cases: - MATH gradient descent - MATH projected gradient descent - MATH proximal minimization algorithm Therefore, all these algorithms have a convergence rate of MATH . Projected gradient descent When MATH is the indicator function of a closed convex set MATH , the problem of minimizing MATH over the set MATH can be reformulated as follows. Note: MATH must be a closed set so that the projection is well-defined. > MATH > > MATH I C x = >\\begin cases >0, & x \\in C \\\\ >\\infty, & x \\notin C >\\end cases MATH At this time, the proximal mapping is defined as follows. > MATH >\\begin align >\\text prox t x >&= \\underset z \\text argmin \\frac 1 2t \\lVert x−z \\rVert 2^2 + I C z \\\\ >& = \\underset z \\in C \\text argmin \\lVert x−z \\rVert 2^2 >\\end align MATH As a result, the proximal mapping MATH is the projection operator onto MATH . The proximal gradient update step is: MATH . In other words, after performing the gradient descent update, you project onto MATH . In the figure below, the pink rectangle is the feasible set MATH , and the current position is the upper of the two points below the rectangle. After taking a gradient descent step, you move outside MATH , so you project back onto MATH to return inside the feasible set. Fig1 Projected Gradient Descent 3 Proximal minimization algorithm Consider the problem of minimizing a convex function MATH as follows. Here, MATH does not need to be differentiable and MATH . > \\begin align \\min x h x \\end align The proximal mapping is defined as: > MATH \\begin align x^ k &= \\text prox t k \\big x^ k-1 - t k \\nabla g x^ k-1 \\big , \\qquad k = 1, 2, 3, ... \\\\ &= \\text prox t k \\big x^ k-1 \\big , \\qquad \\qquad \\qquad \\qquad \\; k = 1, 2, 3, ... \\\\ \\end align MATH Therefore, the proximal gradient update step is: > MATH This proximal gradient method, defined only by MATH with MATH , is called the proximal minimization algorithm PMA . This method is faster than subgradient methods, but if the proximal mapping does not have a closed form, it cannot be implemented. What happens if we can't evaluate prox? Theoretically, when applying proximal gradient to MATH , it is assumed that the prox function can be computed exactly. That is, it is assumed that the minimum can be found exactly via the proximal mapping. > MATH In general, it is not clear what happens if the minimum is only approximated. However, if the error in approximating the prox operator can be precisely controlled, it has been shown that the original convergence rate can be maintained. Schmidt et al. 2011 , Convergence rates of inexact proximal-gradient methods for convex optimization In practice, if prox can be computed approximately, it will be performed with high accuracy.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_04_special_cases/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_05_acceleration",
    "title": "09-05 Acceleration",
    "chapter": "09",
    "order": 6,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Acceleration Acceleration techniques are used to speed up the convergence of optimization algorithms. In the context of proximal gradient methods, acceleration can significantly improve the rate at which solutions are found. Nesterov's acceleration Nesterov's acceleration is a popular technique that introduces momentum to the update steps, allowing the algorithm to move faster towards the optimum. FISTA FISTA Fast Iterative Shrinkage-Thresholding Algorithm is an accelerated proximal gradient method that achieves a convergence rate of MATH . Practical considerations While acceleration can improve convergence, it may also introduce instability or oscillations in some cases. It is important to monitor the behavior of the algorithm and adjust parameters as needed.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_05_acceleration/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_05_01_accelerated_proximal_gradient_method",
    "title": "09-05-01 Accelerated proximal gradient method",
    "chapter": "09",
    "order": 7,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Accelerated proximal gradient method If proximal gradient descent is accelerated, it can achieve the optimal convergence rate of MATH . Nesterov proposed four methods, three of which are acceleration methods: - 1983: Original acceleration method for smooth functions - 1988: Another acceleration method for smooth functions - 2005: Technique for smoothing nonsmooth functions along with the original acceleration method - 2007: Acceleration method for composite functions Now, let's look at the method of Beck and Teboulle 2008 , which extends Nesterov's 1983 method for composite functions. Accelerated proximal gradient method As before, suppose MATH is convex and differentiable, and MATH is convex. The problem is defined as: > \\begin align \\min x g x + h x \\end align The Accelerated proximal gradient method considers the direction in which MATH was moving so that the direction does not change abruptly when moving to the next position. In other words, it gives momentum to the direction of progress, creating inertia to continue moving in the same direction as before. The initial value of the algorithm is set to MATH . Then, after calculating the position MATH considering momentum, the proximal gradient is performed. > MATH \\begin align v & = x^ k-1 + \\frac k-2 k + 1 x^ k-1 −x^ k-2 \\\\ x^ k & = \\text prox t k v − t k \\nabla g v , k = 1,2,3,... \\\\ \\end align MATH - In the first step MATH , MATH is zero, so it is the same as the proximal gradient update. - In the next steps, MATH has momentum in the previous direction MATH . As MATH increases, the momentum weight increases and converges to 1. - When MATH , this is the same as the accelerated gradient method . The figure below shows how the momentum weight changes as MATH changes. In this figure, the value is negative when k = 0, but since the momentum term is zero at that time, it does not cause any problems. As k increases, the weight approaches 1, so the value is updated further and helps to converge faster. Fig1 Momentum weights 3 Lasso example If we apply the accelerated proximal gradient to the Lasso example seen earlier, we get the results shown in the figure below. It can be seen that the accelerated proximal gradient has much faster performance compared to subgradient or proximal gradient methods. There is a part in the middle of the graph that jumps, which is called “Nesterov ripples.” Therefore, the accelerated proximal gradient is not monotonic decreasing and is not a descent method. Fig2 Accelerated Proximal Gradient 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_05_01_accelerated_proximal_gradient_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_05_02_convergence_analysis",
    "title": "09-05-02 Convergence analysis",
    "chapter": "09",
    "order": 8,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Convergence analysis In this section, we analyze the convergence of the accelerated proximal gradient method. Convergence Theorem Suppose MATH is convex and differentiable, MATH is convex, and MATH is Lipschitz continuous with constant MATH . Then, the accelerated proximal gradient method with fixed step size MATH satisfies the following: > MATH This means the convergence rate is MATH , which is faster than the standard proximal gradient method. Backtracking line search The backtracking line search for the accelerated proximal gradient method is similar to that for the standard proximal gradient method, but the step size is chosen adaptively. For more details, refer to Beck and Teboulle 2009 , \"A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems\" FISTA .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_05_02_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_05_03_example_FISTA",
    "title": "09-05-03 Example: FISTA",
    "chapter": "09",
    "order": 9,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Example: FISTA In this section, we introduce an example of the Fast Iterative Shrinkage-Thresholding Algorithm FISTA , which is an accelerated proximal gradient method. FISTA Algorithm The FISTA algorithm solves problems of the form: > MATH where MATH is convex and differentiable, and MATH is convex. The update steps are: 1. Initialize MATH , MATH . 2. For MATH : - MATH - MATH - MATH Application to Lasso FISTA can be applied to Lasso regression, which involves minimizing a least squares loss with an MATH regularization term. FISTA achieves faster convergence compared to standard proximal gradient methods.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_05_03_example_FISTA/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter09/09_05_04_is_acceleration_always_useful",
    "title": "09-05-04 Is acceleration always useful?",
    "chapter": "09",
    "order": 10,
    "owner": "Kyeongmin Woo",
    "lesson_type": "",
    "content": "Is acceleration always useful? Acceleration methods such as FISTA can achieve faster convergence rates in theory, but in practice, acceleration is not always beneficial. When acceleration may not help - If the problem is ill-conditioned, acceleration can cause oscillations or instability. - For some non-smooth problems, acceleration may not improve convergence. - The \"Nesterov ripples\" phenomenon can cause non-monotonic behavior in the objective value. Practical advice - Use acceleration methods when the problem is well-conditioned and smooth. - Monitor convergence and stability when applying acceleration. - If instability occurs, consider switching to standard proximal gradient methods.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter09/09_05_04_is_acceleration_always_useful/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter10/10_Duality_in_Linear_Programs",
    "title": "10 Duality in Linear Programs",
    "chapter": "10",
    "order": 1,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Starting from this chapter, we will examine duality, which plays a significant role in optimization theory. From an optimization perspective, duality can be simply described as the concept that a single optimization problem can be viewed from two perspectives: the primal problem and the dual problem. In this chapter, we specifically explore duality in linear programs. Rather than directly applying it to general convex problems, we will derive the dual problem from the primal problem by applying it to linear programs, and organize how the relationship between these two problems is established and what properties they have under specific conditions.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter10/10_Duality_in_Linear_Programs/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter10/10_01_Lower_Bounds_in_Linear_Programs",
    "title": "10-01 Lower Bounds in Linear Programs",
    "chapter": "10",
    "order": 2,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; Example 1: Form where the constraint contains the objective function Suppose we want to find the lower bound value B of the optimal value for a given convex problem. > MATH >\\begin align >B \\leq \\min x f x . >\\end align > MATH Let's specifically consider the lower bound of linear programs. We will examine cases ranging from simple cases to generalized forms in order. First, taking the simplest form of LP problem as an example > MATH >\\begin align >&\\min x, y >& & x+y \\\\\\\\ >&\\text subject to >& & x + y \\geq 2 \\\\\\\\ >& & & x, y \\geq 0. \\\\\\\\ >\\end align > MATH The above problem already includes the lower bound of the objective function in the constraint, so we can easily see that MATH . Furthermore, let's examine the case where the constraint does not include a lower bound. Example 2: Form where the objective function can be expressed as a linear combination of constraints 1 > MATH >\\begin align >&\\min x, y >& & x+3y \\\\\\\\ >&\\text subject to >& & x + y \\geq 2 \\\\\\\\ >& & & x, y \\geq 0. \\\\\\\\ >\\end align > MATH If MATH are feasible, then multiplying the three constraints by scalar values and adding or subtracting them still satisfies all three constraints. Therefore, for such an LP problem, the process of multiplying constraints by scalar values and adding or subtracting them, i.e., expressing the objective function as a linear combination of constraints, is possible, and as a result, we can find MATH . > MATH >\\begin align >& & x + y \\geq 2 \\\\\\\\ >& + & 0x \\geq 0 \\\\\\\\ >& + & 2y \\geq 0 \\\\\\\\ >& = & x + 3y \\geq 2 \\\\\\\\ > >& & \\text Lower bound \\ B = 2. >\\end align > MATH Generalizing further by applying arbitrary variables to represent the objective function, we get the following: > MATH >\\begin align >&\\min x, y >& & px+qy \\\\\\\\ >&\\text subject to >& & x + y \\geq 2 \\\\\\\\ >& & & x, y \\geq 0. \\\\\\\\ >\\end align > MATH Similar to the second example, by multiplying the constraints by scalar values a, b, c respectively, the objective function can be expressed as a linear combination of these three. > MATH >\\begin align >& & a x+y \\geq 2a \\\\\\\\ >& + & bx \\geq 0 \\\\\\\\ >& + & cy \\geq 0 \\\\\\\\ >& = & a+b x+ a+c y \\geq 2a \\\\\\\\ >&&\\text Lower bound \\ B=2a, \\\\ >&&\\text for any satisfying a,b,c below \\\\\\\\ >& & a + b = p \\\\\\\\ >& & a + c = q \\\\\\\\ >& & a,b,c \\geq 0. \\\\\\\\ >\\end align > MATH For the lower bound to satisfy being 2a as above, since the inequality sign would be reversed in the process of multiplying by scalar values and this would not hold, the conditions MATH must be positive and the sum of scalar values must equal the objective function, i.e., the conditions MATH and MATH must be satisfied. A new optimization problem can be defined by maximizing the lower bound result obtained as above. In this case, the conditions that satisfy the lower bound become the constraints in this problem. > MATH >\\begin align >&\\max a, b, c >& & 2a \\\\\\\\ >&\\text subject to >& & a + b = p \\\\\\\\ >& & & a + c = q \\\\\\\\ >& & & a, b, c \\geq 0 \\\\\\\\ >\\end align > MATH The original LP problem above is called the primal LP, and the form that redefines the optimization problem by maximizing the lower bound in the primal LP is called the dual LP. Note that the number of optimization variables in the dual problem equals the number of constraints in the primal problem. > MATH >\\begin align >\\text Primal LP \\qquad >&\\qquad \\min x, y & px+qy \\\\\\\\ >&\\qquad \\text subject to & x + y \\geq 2 \\\\\\\\ >&\\qquad & x, y \\geq 0 \\\\\\\\ >\\\\\\\\ >\\\\\\\\ >\\text Dual LP \\qquad >&\\qquad \\max a, b, c & 2a \\\\\\\\ >&\\qquad \\text subject to & a + b = p \\\\\\\\ >&\\qquad & a + c = q \\\\\\\\ >&\\qquad & a, b, c \\geq 0 \\\\\\\\ >\\end align > MATH Example 3: Form where the objective function can be expressed as a linear combination of constraints 2 As a final example, let's examine a form where the inequality signs in the constraints are reversed and equality constraints are included. > MATH >\\begin align >&\\min x, y & px+qy \\\\\\\\ >&\\text subject to & x \\geq 0 \\\\\\\\ >& & y \\leq 1 \\\\\\\\ >& & 3x + y = 2 \\\\\\\\ >\\\\\\\\ >& & ax \\geq 0 \\\\\\\\ >& + & -by \\geq -b \\\\\\\\ >& + & 3cx + cy = 2c \\\\\\\\ >& = & a+3c x+ -b+c y \\geq 2c-b >\\\\\\\\ >\\\\\\\\ >&& \\text Lower bound \\ B=2c-b, \\\\ >&& \\text for any satisfying a,b,c below \\\\\\\\ >& & a + 3c = p \\\\\\\\ >& & -b + c = q \\\\\\\\ >& & a,b \\geq 0 \\\\\\\\ >\\end align > MATH Here, c is a scalar value multiplied to both sides of the equality, so any value can be multiplied without restriction. Consequently, the dual LP can be defined as follows. > MATH >\\begin align >&\\qquad \\max a, b, c & 2c-b \\\\\\\\ >&\\qquad \\text subject to & a + 3c = p \\\\\\\\ >&\\qquad & -b + c = q \\\\\\\\ >&\\qquad & a, b \\geq 0 \\\\\\\\ >\\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter10/10_01_Lower_Bounds_in_Linear_Programs/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter10/10_02_Duality_in_general_LPs",
    "title": "10-02 Duality in general LPs",
    "chapter": "10",
    "order": 3,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In 10-01 % multilang post url contents/chapter10/21-03-22-10 01 Lower Bounds in Linear Programs % , we examined the primal and dual of LP problems with single-dimensional variables. In 10-02, we want to examine the dual for LPs in general form. The general form of LP is as follows: Given MATH , > MATH >\\begin align >&\\min x &&c^ T x\\\\\\\\ >&\\text subject to &&Ax = b\\\\\\\\ >& &&Gx \\leq h.\\\\\\\\ >\\end align > MATH Similar to the example in 10-01, we define dual variables MATH equal in number to the number of constraints, and can define the objective function of the dual problem and constraints as the sum of products of constraints and each dual variable. > MATH >\\begin align >& &u^ T Ax-b = 0\\\\\\\\ >& + &v^ T Gx-h \\leq 0\\\\\\\\ >& = &u^ T Ax-b + v^ T Gx-h \\leq 0.\\\\\\\\ >\\end align > MATH Remember that the dual variable MATH for equality has no constraints, while MATH is a dual variable for inequality and thus has the additional constraint of being positive. By organizing the last equation to represent the objective function of the primal LP, we get the dual LP. > MATH >\\begin align >u^ T Ax-b + v^ T Gx-h \\leq 0 \\\\\\\\ >\\underbrace -A^ T u-G^ T v ^ T =c^ T x\\geq-b^ T u-h^ T v \\\\\\\\ >\\text Lower bound is -b^ T u-h^ T v \\\\\\\\ >\\text for x \\text primal feasible, and any u, v satisfies, \\\\\\\\ >c = -A^ T u-G^ T v \\\\\\\\ >v\\geq 0. \\\\\\\\ >\\end align > MATH That is, when MATH , the optimal value of the primal has a lower bound of MATH . Consequently, the dual LP can be defined as follows. > MATH >\\begin align >&\\max u,v &&-b^ T u-h^ T v \\\\\\\\ >&\\text subject to &&c = -A^ T u-G^ T v \\\\\\\\ >& &&v\\geq 0. >\\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter10/10_02_Duality_in_general_LPs/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter10/10_03_Max_flow_and_min_cut",
    "title": "10-03 Max flow and min cut",
    "chapter": "10",
    "order": 4,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; As an example of duality in linear programs, we want to examine the max flow min cut problem. Directed Graph, Condition of flow Fig 1 Directed Graph 3 There is a directed graph MATH as shown above, and let the edge connecting vertex i and vertex j, MATH , i.e., the flow from i to j, be MATH . Each edge has a capacity, i.e., the maximum flow that can flow through it. Let this be MATH . As a simple example, this can be understood as a graph representation of some flow from a source s flowing out to a sink t . It is a graph form that can be applied to various problems such as urban drainage/power transmission planning, material transportation, etc. Here, the flow satisfies three conditions. 1. MATH is always a positive number greater than or equal to 0: MATH 2. MATH must be less than the maximum flow determined for the edge, i.e., the capacity limit capacity MATH : MATH >\\begin align >\\text Value of max flow &\\leq \\text dual LP of max flow \\\\ >&= \\text Optimal value for LP relaxed min cut \\\\ >&\\leq \\text Capacity of min cut \\\\ >\\end align > MATH In this page, we will show the inequality relationship by the reverse process of dual and relaxation adding constraints to the LP problem to convert it to an integer program . Although not covered here, in reality, all three results are equal. This is called the max flow min cut theorem, which states that the maximum flow in a network is equal to the minimum capacity of a cut. More generally, under certain conditions, the optimal values of the primal and dual problems are equal, which is called strong duality. In LP problems, except for the case where both the primal and dual problems are infeasible, strong duality holds. This will be discussed in Chapter 11. First, let's look at the two problems, derive the dual from the max flow problem, and show that by adding specific conditions to the dual problem reverse of relaxation , it transforms into the min cut problem. Max flow problem The max flow problem is to find the maximum flow from s to t in a graph that satisfies the above conditions. > MATH >\\begin align >&\\max f\\in \\mathbb R ^ |E| && \\sum s,j \\in E f sj \\\\ >&\\text subject to && f ij \\geq 0,\\,f ij \\leq c i,j \\,\\, \\text for all i, j \\in E \\\\ >&&& \\sum i, k \\in E f ik =\\sum k,j \\in E f kj \\,\\, \\text for all k\\in V \\backslash \\ s,t\\ .\\\\ >\\end align > MATH Min cut problem Fig 2 Graph Cut Example 3 The min cut problem divides all vertices of the graph into two sets: the shaded region and the unshaded region, as shown in the figure. One set contains the source, and the other contains the sink, while the remaining vertices are arbitrarily assigned to either set here, the set containing the source is called A, and the set containing the sink is called B . The sum of the capacities of the edges going from set A to set B is defined as the cut. In other words, a cut is a partition of the graph's vertices such that the source and sink are in different partitions. The min cut problem is to find the minimum value of this cut for a given graph. In the general definition of the min cut problem, since it is defined on a directed graph, it always satisfies source MATH , sink MATH . This part is omitted in the problem definition below. > MATH >\\begin align >&\\min b\\in \\mathbb R ^ |E| ,\\, x \\in \\mathbb R ^ |V| && \\sum i,j \\in E b ij c ij \\\\ >&\\text subject to && b ij \\geq x i -x j \\\\ >&&& b ij ,\\,x i ,\\,x j \\,\\in \\ 0,1 \\ \\\\ >&&&\\text for all i, j.\\\\ >\\end align > MATH Intuitively, the max flow problem is to find the maximum flow from the source, and the min cut problem is to find the minimum total capacity that can be sent from the source set to the sink set, so it is clear that these two problems are closely related. Dual of Max flow problem Let's derive the dual for the max flow optimization problem. First, define the dual variables for the constraints in order as MATH . In the dual of the max problem, the upper bound will be minimized, so the organized form should be in the form of primal objective MATH something. Therefore, organize the equation to find the upper bound of MATH for the constraints. This can be organized as follows. > MATH >\\begin align >\\sum i,j \\in E \\Big -a ij f ij +b ij f ij -c ij \\Big + \\sum k \\in V\\backslash \\ s,t\\ x k \\Big \\sum i,k \\in E f ik - \\sum k,j \\in E f kj \\Big \\leq 0\\\\ >\\text for any a ij , b ij \\geq 0, i, j \\in E, \\text and x k , k\\in V \\backslash \\ s,t\\ . >\\end align > MATH Organize the MATH terms related to the primal LP objective function on the left, and the rest on the right. Next, since we want the upper bound of the primal LP, find the equation such that the terms multiplied by MATH on the left match the primal LP objective function. The condition that satisfies this equation becomes the constraint in the dual LP. That is, organize the equation so that the MATH term is 1 only in MATH and 0 elsewhere. This process is detailed as follows. > MATH >\\begin align >\\sum i,j \\in E \\Big b ij -a ij f ij \\Big +\\sum k\\in V\\backslash \\ s,t\\ x k \\Big \\sum i,k \\in E f ik -\\sum k,j \\in E f kj \\Big \\leq \\sum i,j \\in E b ij c ij . >\\end align > MATH Here, our goal is to make the result of the left side when MATH be MATH and 0 for other cases. The k in the x term of the second sigma does not include the source and sink, and can be divided into three cases: when MATH , when MATH , and when MATH . Case 1. MATH For the term multiplied by MATH , it is eliminated by the third condition of flow except for the case of MATH . Therefore, the sigma for the x term of the second term can be organized as follows. > MATH >\\begin align >&=\\sum s,j \\in E \\Big b sj -a sj f sj \\Big +x j \\sum s,j \\in E f sj +\\sum k\\in V\\backslash \\\\ s,t,j\\\\ x k \\Big \\underbrace \\sum s,k \\in E f sk -\\sum k,j \\in E f kj =0 \\Big \\\\\\\\ >&=\\sum s,j \\in E \\Big b sj -a sj +x j \\Big f sj , \\ j \\in V \\backslash \\ s,t\\ ,\\\\\\\\ >\\end align > MATH Case 2. MATH For the term multiplied by MATH , it is eliminated by the third condition of flow except for the case of MATH . Therefore, the sigma for the x term of the second term can be organized as follows. > MATH >\\begin align >&=\\sum i,t \\in E \\Big b it -a it f it \\Big -x i \\sum i,t \\in E f it +\\sum k\\in V\\backslash \\ s,t,i\\ x k \\Big \\underbrace \\sum i,k \\in E f ik -\\sum k,t \\in E f kt =0 \\Big \\\\\\\\ >&=\\sum i,t \\in E \\Big b it -a it -x i \\Big f it , \\ i \\in V\\backslash \\ s,t\\ ,\\\\\\\\ >\\end align > MATH Case 3. MATH For the term multiplied by MATH , it is eliminated by the third condition of flow except for the cases of MATH and MATH . Therefore, the sigma for the x term of the second term can be organized as follows. > MATH >\\begin align >&=\\sum i,j \\in E \\Big b ij -a ij f ij \\Big +x j \\sum i,j \\in E f ij -x i \\sum i,j \\in E f ij +\\sum k\\in V\\backslash \\ s,t,i,j\\ x k \\Big \\underbrace \\sum i,k \\in E f ik -\\sum k,j \\in E f kj =0 \\Big \\\\\\\\ >&=\\sum i,j \\in E \\Big b ij -a ij +x j -x i \\Big f ij , \\ i, j \\in V \\backslash \\ s,t\\ . \\\\\\\\ >\\end align > MATH The objective function of the primal LP matches with the case 1 of the above, where the term is 1 in MATH , and for the other cases, it makes the multiplied term 0, completing the form of left side being the objective function and the right side being the upper bound. > MATH >\\begin align >&b sj -a sj +x j = 1\\\\\\\\ >&b it -a it -x i = 0\\\\\\\\ >&b ij -a ij +x j -x i = 0\\\\\\\\ >&\\text Result in, \\\\\\\\ >&\\sum s,j \\in E f sj \\leq \\sum i,j \\in E b ij c ij . >\\end align > MATH Therefore, the dual problem is to find the minimum value of the upper bound objective function of dual LP for the dual variables MATH , and this minimum value becomes the best upper bound. A dummy variable MATH is eliminated while maintaining the conditions. Additionally, by adding the condition that flow occurs from source to sink in the directed graph, the equation becomes: > MATH >\\begin align >&\\min b\\in \\mathbb R ^ |E| ,\\, x\\in \\mathbb R ^ |V| && \\sum i,j \\in E b ij c ij \\\\\\\\ >&\\text subject to && b ij +x j -x i \\geq 0 \\,\\, \\text for all i,j \\in E \\\\\\\\ >&&& b\\geq 0, x s =1,x t =0 .\\\\\\\\ >\\end align > MATH Dual LP to Integer program Now, we want to show that this dual LP is the same as the LP relaxation of the min cut problem. Therefore, we will go through the process of converting it to an integer program by adding conditions to the above dual LP problem. The variable MATH is not defined for vertices other than s and t. Therefore, to narrow the scope of the problem, let's add a condition that the remaining vertices except s and t belong to either group s or t. In other words, let's assume that all vertices belong to either group 0 or 1. This is equivalent to determining the vertex partition for the min cut. > MATH >\\begin align >x i \\in \\ 0,1 \\ \\ \\ \\text for all i\\in V. >\\end align > MATH Let's define the group that belongs to 1 as set A, and the group that belongs to 0 as set B. Also, let's define that the source s belongs to A, and the sink t belongs to B. With the above definitions, MATH acts as an on/off switch, being 1 for edges going from set A to set B, and 0 otherwise. This can be organized as follows. > MATH >\\begin align >&\\text Let A= \\ i:x i =1 \\ ,\\, B= \\ i:x i =0 \\ \\\\\\\\ >&\\text note that s \\in A, \\,t \\in B, \\text and b ij \\geq x i -x j \\,\\,\\,\\, \\text for \\, i,j \\in E, \\,\\, b\\geq 0,\\\\\\\\ >\\end align > MATH > MATH >\\begin align >\\text Simply say, \\qquad \\begin cases b ij =1 \\qquad \\text if i\\in A, j\\in B\\\\\\\\ >0 \\qquad\\qquad \\text otherwise .\\end cases >\\end align > MATH The above result is the same as the formulation of the min cut problem. Relationship between Max flow and Min cut problem 2 That is, the dual problem of the max flow problem is the result of removing the condition that the vertices except s and t in the min cut problem are included in 0 or 1 relaxation . The optimal value of max flow MATH dual LP upper bound , and this relaxation expands the domain scope of the optimization variable, so the optimal value LP relaxed min cut MATH capacity of min cut. Summarizing these three results, we get the following result. > MATH >\\begin align >\\text Value of max flow &\\leq \\text Dual LP of max flow \\\\\\\\ >&= \\text Optimal value for LP relaxed min cut \\\\\\\\ >&\\leq \\text Capacity of min cut \\\\\\\\ >\\end align > MATH For the equality of these three results, refer to the max-flow min-cut theorem 11 , and for a representative algorithm for solving the max flow min cut problem, refer to the Ford-Fulkerson algorithm 12 .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter10/10_03_Max_flow_and_min_cut/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter10/10_04_Another_Perspective_on_LP_duality",
    "title": "10-04 Another Perspective on LP duality",
    "chapter": "10",
    "order": 5,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In the case of duality discussed earlier, for LP, we multiplied the constraints of the primal problem by dual variables, obtained their linear combination, and then organized it to separate out the primal's objective function to obtain a bound. The separated remaining terms something in the formula below served as the bound for the primal problem. That is, they became the objective function of the dual problem, and the conditions created during the formula development process became the constraints of the dual problem. Writing this partial process the part of the above content that separates out the primal objective function to obtain a bound as a formula, it looks like this: > MATH >\\begin align >&\\min x &&f x \\\\\\\\ >&\\text subject to &&Ax = b\\\\\\\\ >& &&Gx \\leq h\\\\\\\\ >\\end align > MATH > MATH >\\begin align >& &\\text for any u,\\, v\\geq 0,\\\\\\\\ >& &u^ T Ax-b = 0\\\\\\\\ >& + &v^ T Gx-h \\leq 0\\\\\\\\ >& = &u^ T Ax-b + v^ T Gx-h \\leq 0\\\\\\\\ >& \\approx &f x +\\text something . \\\\\\\\ >\\end align > MATH However, for optimization problems that are not linear programs, most cannot express the objective function as a linear combination of constraints. In this chapter, we examine the perspective of duality that is applicable to more universally common problems all convex, most non-convex . We will find the duality of LP using this method called Lagrangian, and examine more detailed discussions in Chapter 11. Looking at the equations up to the linear combination form for the primal LP problem described above, we can understand the following relationship: > MATH >\\begin align >c^ T x\\geq c^ T x+\\overbrace u^ T \\underbrace Ax-b =0 +\\underbrace v^ T \\geq 0 \\underbrace Gx-h \\leq 0 ^ \\leq 0 := L x,u,v . >\\end align > MATH The right side of the inequality has a value less than or equal to the left side according to the conditions. Also, we define this expression as a function MATH for x, u, v. Here, if we call the set satisfying the constraints of the primal LP primal feasible set C, we can understand the following relationship: > MATH >\\begin align >C = \\ x: Ax=b, Gx\\leq h \\ , >\\end align > MATH > MATH >\\begin align >f^ =\\min x\\in C f x \\geq \\min x\\in C L x,u,v \\geq \\min x L x,u,v :=g u,v .\\\\\\\\ >\\end align > MATH In other words, MATH becomes a lower bound of MATH for any u or MATH satisfying MATH . Let's examine the lower bound value determined by MATH . > MATH >\\begin align g u,v = min x c^ T x+u^ T Ax-b + v^ T Gx-h \\\\\\\\ = \\min x c+A^ T u+G^ T v ^ T x - b^ T u-h^ T v \\\\\\\\ \\begin cases = -b^ T u-h^ T v \\qquad &\\text if \\ c = -A^ T u-G^ T v \\\\\\\\ -\\infty \\qquad &\\text otherwise . \\end cases >\\end align > MATH As can be seen from the equation, when MATH is not satisfied, it has a value of MATH due to the MATH term. Since we want to find the lower bound closest to MATH , we want to find the value that maximizes MATH . This is MATH , the value when MATH is satisfied, and this matches the Dual LP we obtained with the first method. > MATH >\\begin align >f^ \\geq g u,v , \\qquad \\text provided v \\geq 0\\\\\\\\ >\\text find the biggest lowerbound g u,v \\\\\\\\ >\\max u,v g u,v \\\\\\\\ >\\text s.t. v \\geq 0. >\\end align > MATH This method is also applicable to other types of optimization problems that are not in LP form.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter10/10_04_Another_Perspective_on_LP_duality/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter10/10_05_Matrix_Games",
    "title": "10-05 Matrix Games",
    "chapter": "10",
    "order": 6,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In this chapter, we examine mixed strategies for matrix games, which is an example of primal LP and dual LP in game theory. The setup assumes two players, J and R, and a payout matrix MATH . Game Setup Fig 1 Game Setup 3 The payout matrix is the amount of reward J must give to R when J chooses strategy MATH row and R chooses strategy MATH column MATH . However, if this value is positive, J gives R a reward equal to the size of the corresponding matrix, and if negative, R gives J a reward equal to the size of the corresponding matrix. This setting is also called a zero-sum setting, where if the reward R will receive or must pay is MATH and J's reward is MATH , then in each game the result of the rewards is MATH , and the total sum of the two rewards is always 0. Also, we assume that both players use mixed strategies. Mixed strategies is the assumption that each player's choice follows a specific probability distribution or is sampled from a specific probability distribution . > MATH >\\begin align >x : P \\text J chooses i = x i , \\qquad i=1,...m\\\\\\\\ >y : P \\text R chooses j = y j , \\qquad j=1,...n.\\\\\\\\ >\\end align > MATH If they know each other's mixed strategy, i.e., probability distribution, each player can calculate the payout they expect to get, i.e., the expected payout. > MATH >\\begin align >\\sum i=1 ^ m \\sum j=1 ^ n x i y j P ij = x^ T Py.\\\\\\\\ >\\end align > MATH Considering that the sign of the payout matrix is defined as the amount J gives to R, J will try to minimize this expected payout because J wants to give as little as possible to R, and R will try to maximize this expected payout because R wants to receive as much as possible from J. 이제 두 player의 입장에서 각자가 상대의 mixed strategy를 고려하여, 이 expected payout을 최대화 R의 입장 혹은 최소화 J의 입장 하려는 관점을 살펴보고, 서로가 서로를 optimal하게 행동하는 전제하에, 두 입장에서 유도되는 optimal strategy를 구하고, 결과적으론 Von Neumman's minimax theorem에 의해 두 결과가 같다는 것을 확인할 것이다. Minimizing Expected Payout : J's Perspective 먼저 R이 J의 strategy MATH 를 알고 있다고 가정하자. R은 expected payout MATH 를 maximize하고자 할 것이다. > MATH >\\begin align >\\max\\ x^ T Py : y\\geq0, 1^ T y = 1\\ = \\max i=1,...n P^ T x i .\\\\\\\\ >\\end align > MATH 이때 R은 식의 내용처럼 MATH 중 가장 큰 값을 갖는 i row index 를 찾게되고, 이 i에 대응되는 MATH 를 1로 가지고 나머지의 row index에 대해선 0을 가지는 strategy가 R에게 있어서 expected payout을 maximize하는 strategy일 것이다. R이 위처럼 최적으로 행동할 것을 알고 있을 때, J의 최적의 strategy는 밑의 식을 만족하는 distribution MATH 일 것이다. > MATH >\\begin align >& \\min x >& &\\max i=1,...n P^ T x i \\\\\\\\ >& \\text subject to >& & x\\geq 0, 1^ T x =1.\\\\\\\\ >\\end align > MATH Convex function의 maximization 또한 convex function이 된다. 이를 첫 번째 관점의 문제 정의라고 칭할 것이다. 또한 이 최적화 문제의 해를 optimal expected payout MATH 이라고 정하자. 또 하나 유념할 점은 게임참가자, 즉 player들이 모두 최적으로 행동한다는 가정이 기본적인 형태의 게임이론 formulation에서 가정이 된다. Maximizing Expected Payout : R's Perspective 두 번째 관점으로 J가 R의 strategy MATH 를 알고 있다고 가정하자. J는 expected payout을 minimize하고자 할 것이다. > MATH >\\begin align >\\min \\ x^ T Py : x\\geq0, 1^ T x = 1\\ = \\min j=1,...n Py j .\\\\\\\\ >\\end align > MATH 같은 논리로, J가 위처럼 최적으로 행동할 것을 알고 있을 때 R의 최적의 strategy는 밑의 식을 만족하는 distribution MATH 이다. > MATH >\\begin align >& \\max y >& & \\min j=1,...m Py j \\\\\\\\ >&\\text subject to >& &y\\geq 0, 1^ T y =1.\\\\\\\\ >\\end align > MATH 위와 마찬가지로 이를 두 번째 관점의 문제 정의라고 칭하고, 이 최적화 문제의 해를 MATH 라고 하자. player R이 이 expected payout을 maximize하고자 하기 때문에, 첫 번째, 즉, R이 J의 strategy를 미리 알고 있다는 가정 하에 결정되는 expected payout MATH 이 두 번째 가정보다 더 크거나 같은 값을 가질 것이라 쉽게 예상할 수 있다. MATH Von Neumann's minimax theorem 하지만, Von Neumann's minimax theorem에 따르면 MATH 가 된다. 실제 minimax theorem은 다음과 같다. > MATH >\\begin align >&\\text Let X\\subset \\mathbb R ^ N \\text and Y\\subset \\mathbb R ^ m \\text be compact convex sets. \\\\\\\\ > &\\text If f:X\\times Y\\rightarrow \\mathbb R \\text is a continuous function that is convex-concave, i.e. \\\\\\\\ > &\\qquad f \\cdot, y : X\\rightarrow\\mathbb R \\text is convex for fixed y, \\text and \\\\\\\\ > &\\qquad f x, \\cdot : Y\\rightarrow\\mathbb R \\text is concave for fixed x.\\\\\\\\ > &\\text Then we have that \\\\\\\\ >&\\min x\\in X \\max y\\in Y f x,y = \\max y\\in Y \\min x\\in X f x,y .\\\\\\\\ >\\end align > MATH 해당 내용의 증명은 생략한다. Proof of each perspective having Primal and Dual relationship 이제 위 두 가지 관점의 경우에 대한 expected payout이 LP 문제로써 서로 primal, dual 관계이고, Von Neumman's minimax theorem에 의하여 두 결과가 같다는 점을 이용하여, strong duality를 만족함을 보이고자 한다. 먼저 첫 번째 관점의 문제를 다음과 같이 reformulate 할 수 있다. > MATH >\\begin align >& \\min x \\max i=1,...m >& & P^ T x i \\\\\\\\ >&\\text subject to >& &x\\geq 0, 1^ T x = 1\\\\\\\\ >\\end align > MATH > > MATH >\\begin align >\\Leftrightarrow \\\\\\\\ >& \\min x,t >& & t \\\\\\\\ >&\\text subject to >& &x\\geq0, 1^ T x = 1, P^ T x \\preceq t. \\\\\\\\ >\\end align > MATH MATH 를 MATH 의 항들 중 가장 큰 값과 같게 만들어주는 문제로 reformulate 하였다. 이제 여기에 앞서 배운 duality의 두 번째 방법인 Lagrangian을 구하고, Lagrange dual function MATH 를 구하면, > MATH >\\begin align >&L x, t, u, v, y &&= t-u^ T x+v 1-1^ T x +y^ T P^ T x-t1 \\\\\\\\ >&g u, v, y &&= \\min x,t \\quad L x, t, u, v, y \\\\\\\\ >&&&= \\begin cases v \\qquad &\\text if 1-1^ T y = 0, Py-u-v1=0\\\\\\\\ -\\infty \\qquad &\\text otherwise. \\end cases >\\end align > MATH MATH 는 slack variable이므로, 이를 제거하고 식을 정리하면 다음과 같다. > MATH >\\begin align >&\\max y,v \\qquad \\quad && v\\\\\\\\ >&\\text subject to \\quad && y\\geq0, 1^ T y = 1\\\\\\\\ >&&& Py\\geq v. >\\end align > MATH 이는 두 번째 관점의 문제의 primal LP이다. 따라서 두 관점은 dual 관계에 있고 두 문제의 optimal value는 같으므로, strong duality가 성립한다. 일반적으로 LP문제에서는, 향 후의 내용에서 다루지만, primal과 dual 중 하나만 feasible하다면 strong duality가 성립한다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter10/10_05_Matrix_Games/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter11/11_00_Duality_in_General_Programs",
    "title": "11 Duality in General Programs",
    "chapter": "11",
    "order": 1,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "Review: duality in linear program Given MATH , MATH , MATH , MATH , MATH , Primal LP: > MATH \\begin alignat 1 \\min x & \\quad c^T x \\\\\\\\ s.t. & \\quad Ax = b \\\\\\\\ & \\quad Gx \\leq h \\end alignat MATH Dual LP: > MATH \\begin alignat 1 \\max u,b & \\quad -b^T u - h^T v \\\\\\\\ s.t. & \\quad - A^T u - G^T v = c \\\\\\\\ & \\quad v \\geq 0 \\end alignat MATH Explanation 1: For all MATH and MATH , and for any primal feasible MATH , the following holds. > MATH \\begin equation u^T Ax-b + v^T Gx-h \\leq 0 \\end equation MATH 즉, > MATH \\begin equation -A^Tu - G^Tv ^T x \\geq -b^Tu - h^T v \\end equation MATH By this relationship, if MATH , we obtain a lower bound on the primal optimum. Explanation 2: For all MATH and MATH , and for any primal feasible MATH , > MATH \\begin equation c^T x \\geq c^T x + u^T Ax-b + v^T Gx -h := L x,u,v \\end equation MATH Thus, if MATH is the primal feasible set and MATH is the primal optimum, then > MATH \\begin equation f^ \\geq \\min x \\in C L x,u,v \\geq \\min x L x,u,v := g u,v \\end equation MATH In other words, MATH is a lower bound on MATH . > MATH g u,v = \\begin cases -b^T u - h^T v & \\text if MATH \\\\\\\\ -\\infty & \\text otherwise \\end cases MATH The second explanation yields the same dual as the first, but it is completely general and applies to arbitrary optimization problems including nonconvex ones .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter11/11_00_Duality_in_General_Programs/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter11/11_01_Lagrangian",
    "title": "11-1 Lagrangian",
    "chapter": "11",
    "order": 2,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "We now look at the Lagrangian form for the following optimization problem. Here, the optimization problem need not be convex. > MATH \\begin alignat 1 \\min x & \\quad f x \\\\\\ s.t. & \\quad h i x \\leq 0, i = 1,\\dots,m \\\\\\ & \\quad l j x = 0, j=1,\\dots,r \\end alignat MATH The Lagrangian is defined as follows. > MATH \\begin equation L x,u,v = f x + \\sum i=1 ^m u i h i x + \\sum j=1 ^r v j l j x \\end equation MATH Here, MATH , MATH , MATH implicitly, MATH for MATH u MATH \\begin equation L x,u,v = f x + \\sum i=1 ^ m u i \\underbrace h i x \\leq 0 + \\sum j=1 ^r v j \\underbrace l j x =0 \\leq f x \\end equation MATH In other words, the Lagrangian has the following important property. > For all MATH , MATH , we have MATH For example, in the figure below, Fig 1 Example of Lagrangian 1 The solid line indicates the function MATH . The dashed line indicates the function MATH . Here, the feasible set is roughly MATH . Each dotted line indicates the function MATH for MATH , MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter11/11_01_Lagrangian/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter11/11_02_Lagrange_dual_function",
    "title": "11-2 Lagrange dual function",
    "chapter": "11",
    "order": 3,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "Let MATH be the primal feasible set and MATH the primal optimal value. Minimizing MATH over all MATH yields the following lower bound. > MATH \\begin equation f^ \\geq \\min x \\in C L x,u,v \\geq \\min x L x,u,v := g u,v \\end equation MATH Here, MATH is called the Lagrange dual function and provides a lower bound on MATH for any dual feasible MATH , MATH . For example, in the figure below Fig 2 Example of Lagrangian dual function 1 The dashed horizontal line indicates the function MATH . The dual variable is MATH . The solid line indicates MATH . Example: Quadratic Program 1 Positive Definite MATH Consider the following quadratic problem here MATH > MATH \\begin alignat 1 \\min x & \\quad \\frac 1 2 x^T Q x + c^T x \\\\\\\\ s.t. & \\quad Ax = b, \\\\\\\\ & \\quad x \\geq 0 \\end alignat MATH Then, Lagrangian: > MATH \\begin equation L x,u,v = \\frac 1 2 x^T Q x + c^T x - u^Tx + v^T Ax-b \\end equation MATH Lagrangian dual function: From the expression above, to minimize the Lagrangian function, differentiate with respect to MATH and find MATH such that the derivative is zero. \\begin equation Qx - c-u+A^T v = 0, \\end equation 즉, \\begin equation Qx = c-u+A^T v \\end equation 이 때, MATH 는 positive definite하므로 역행렬이 존재하므로, MATH 를 찾으면, MATH 임을 알 수 있다. 따라서, MATH 를 Lagrangian 함수에 대입을 하면, 아래를 얻을 수 있다. MATH \\begin alignat 1 & c - u + A^T v ^T Q^ -1 ^T Q Q^ -1 c - u + A^T v - c - u + A^T v ^T Q^ -1 c - u + A^T v - b^T v \\\\\\ = & c - u + A^T v ^T Q^ -1 c - u + A^T v - c - u + A^T v ^T Q^ -1 c - u + A^T v - b^T v \\\\\\ = & -\\frac 1 2 c-u+A^Tv ^T Q^ -1 c-u+A^T v - b^T v \\end alignat MATH 따라서, > MATH \\begin equation g u,v = \\min x L x,u,v = -\\frac 1 2 c-u+A^Tv ^T Q^ -1 c-u+A^T v - b^T v \\end equation MATH For all MATH and MATH , this corresponds to a lower bound on the primal optimum MATH . 2 Positive Semidefinite MATH 위와 같은문제이나, 이번에는 MATH 이면, Lagrangian: > MATH \\begin equation L x,u,v = \\frac 1 2 x^T Q x + c^T x - u^Tx + v^T Ax-b \\end equation MATH Lagrangian dual function: MATH 가 positive definite 할 때 처럼, 아래 식을 만족하는 MATH 를 찾아야 한다. MATH \\begin equation Qx = c-u+A^T v \\end equation MATH 이 때, MATH 는 positive semi-definite이므로 역행렬이 존재하지 않을 수 있다. 따라서, 다음의 두가지 경우를 고려해야 한다. 1 MATH . In this case, there exists MATH satisfying MATH , which can be found using the generalized inverse MATH the Moore-Penrose pseudo-inverse, MATH . 2 MATH . In this case, there is no MATH satisfying MATH , meaning there is no MATH that minimizes MATH , and the minimum of MATH is MATH . From these two cases, the Lagrangian dual function can be summarized as follows. > MATH g u,v = \\begin cases -\\frac 1 2 c-u+A^T v ^T Q^ + c - u + A^T v - b^T v & \\text if MATH \\\\\\\\ -\\infty & \\text otherwise \\end cases MATH For all MATH , MATH with MATH , MATH is a nontrivial lower bound on MATH . Example: Quadratic Program in 2D For example, in the following figure, MATH is a quadratic function over variables greater than 0 MATH , and the dual function MATH is a quadratic function over variables greater than 0 MATH . Fig 3 Example of quadratic program in 2D The blue point is the optimal dual value, and the red point is the optimal primal value. For all MATH , the dual function MATH provides a lower bound on MATH . The maximum of the dual function MATH matches exactly the value MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter11/11_02_Lagrange_dual_function/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter11/11_03_Lagrange_dual_problem",
    "title": "11-3 Lagrange dual problem",
    "chapter": "11",
    "order": 4,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "Consider the following optimization problem: > MATH \\begin alignat 1 \\min x & \\quad f x \\\\ s.t. & \\quad h i x \\leq 0, i=1,\\dots,m \\\\ & \\quad l j x = 0, j=1,\\dots,r \\end alignat MATH The dual function MATH satisfies MATH for all MATH and MATH . Therefore, we can obtain the best lower bound by maximizing MATH over all feasible MATH and MATH . This is called the Lagrange dual problem. > MATH \\begin alignat 1 \\max u,v & \\quad g u,v \\\\ s.t. & \\quad u \\geq 0 \\end alignat MATH Here, if we denote the dual optimal value as MATH , then MATH . This is called weak duality. This property always holds even when the primal problem is not convex. Moreover, the dual problem is always a convex optimization problem, even when the primal problem is not convex. By definition, MATH is concave with respect to MATH , and MATH is a convex constraint. Therefore, the dual problem corresponds to a concave maximization problem. > \\begin alignat 1 g u,v & = \\min x \\ f x + \\sum i=1 ^m u i h i x + \\sum j=1 ^r v j l j x \\ \\\\ & = - \\underbrace \\max x \\ -f x - \\sum i=1 ^m u i h i x - \\sum j=1 ^r v j l j x \\ \\text pointwise maximum of convex functions in MATH \\end alignat Example: nonconvex quartic minimization Let us minimize the function MATH subject to MATH . Fig 4 Example of nonconvex quadratic minimization In this case, the dual function MATH is as follows: > MATH \\begin equation g u = \\min i=1,2,3 \\ F i^4 u - 50 F i^2 u + 100 F i u \\ \\end equation MATH where, for MATH , > MATH \\begin alignat 1 F i u = & \\frac - a i 12\\cdot 2^ 1/3 \\left 432 100-u - 432^2 100-u ^2 - 4\\cdot 1200^3 ^ 1/2 \\right ^ 1/3 \\\\ & - 100 \\cdot 2^ 1/3 \\frac 1 \\left 432 100-u - 432^2 100-u ^2 - 4\\cdot 1200^3 ^ 1/2 \\right ^ 1/3 \\end alignat MATH and MATH . While it is difficult to determine whether MATH is concave just by looking at the function, we can know that MATH is concave under the convexity property of duality.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter11/11_03_Lagrange_dual_problem/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter11/11_04_Strong_duality",
    "title": "11-4 Strong duality",
    "chapter": "11",
    "order": 5,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "When a problem satisfies MATH , this is called strong duality . The Slater condition is a sufficient condition for strong duality. Slater condition: If the primal problem is convex and there exists at least one strictly feasible MATH , then strong duality holds. > MATH \\begin equation h 1 x MATH \\begin alignat 1 \\min \\beta, \\beta 0, \\xi & \\quad \\frac 1 2 \\parallel\\beta\\parallel 2^2 + C \\sum i=1 ^n \\xi i \\\\\\\\ s.t. & \\quad \\xi i \\geq 0, i=1,\\dots,n \\\\\\\\ & \\quad y i x i^T \\beta + \\beta o \\geq 1 - \\xi i, i=1,\\dots,n \\end alignat MATH Dual 변수, MATH 를 사용하여 Lagrangian을 정의해보자. > \\begin equation L \\beta,\\beta 0,\\xi,v,w = \\frac 1 2 \\parallel\\beta\\parallel\\ 2^2 + C\\sum i=1 ^n \\xi i - \\sum i=1 ^n v i \\xi i + \\sum i=1 ^n w i 1-\\xi i - y i x i^T\\beta + \\beta o \\end equation MATH 에 대해 최소화해서 다음과 같은 Lagrangian dual function을 구할 수 있다. > MATH g v,w = \\begin cases -\\frac 1 2 w^T\\tilde X \\tilde X ^T w + 1^Tw, &\\text if MATH \\\\\\\\ -\\infty, &\\text otherwise \\end cases MATH Here MATH . Therefore, the SVM dual problem after eliminating the slack variable MATH becomes the following. > MATH \\begin alignat 1 \\max w & \\quad -\\frac 1 2 w^T\\tilde X \\tilde X ^T w + 1^Tw \\\\\\\\ s.t. & \\quad 0 \\leq w \\leq C1, w^Ty = 0 \\end alignat MATH Since the primal problem satisfies the Slater condition, strong duality holds. That is, the objective is convex and the inequality constraints are affine in MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter11/11_04_Strong_duality/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter11/11_05_Duality_gap",
    "title": "11-5 Duality gap",
    "chapter": "11",
    "order": 6,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "If the primal problem has value MATH at MATH , and the dual problem has value MATH at MATH , then the difference MATH is called the duality gap. Meanwhile, these feasible values always satisfy the following relation > MATH \\begin equation f x - f^ \\leq f x - g u,v , \\end equation MATH If the duality gap is MATH , then MATH is an optimal solution of the primal problem and MATH are optimal for the dual problem. Also, if the duality gap satisfies MATH , it implies MATH , so algorithms that solve problems iteratively can use the duality gap as a stopping criterion.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter11/11_05_Duality_gap/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter12/12_00_KKT_conditions",
    "title": "12 KKT Conditions",
    "chapter": "12",
    "order": 1,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "When the primal problem is convex, the Karush–Kuhn–Tucker KKT conditions become sufficient conditions for primal & dual optimal points with zero duality gap. Additionally, when the objective function and constraint functions of the primal problem are differentiable and satisfy strong duality, the primal & dual optimal points always satisfy the KKT conditions. KKT conditions hold a very important position in optimization. These conditions allow some special problems to be solved analytically, and many convex optimization algorithms can be interpreted as methods for solving KKT conditions 1 . In this chapter, we will learn about the definition and properties of KKT conditions and look at some examples based on them. As a side note , KKT conditions were originally introduced to the world by Harold W. Kuhn and Albert W. Tucker in 1951, and at that time they were called KT Kuhn-Tucker conditions. Later, scholars discovered that the necessary conditions for this problem had been addressed by William Karush's master's thesis in 1939, and from then on, Karush's name was included and they became known as KKT Karush–Kuhn–Tucker conditions 3 .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter12/12_00_KKT_conditions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter12/12_01_Karush_Kuhn_Tucker_conditions",
    "title": "12-01 Karush-Kuhn-Tucker conditions",
    "chapter": "12",
    "order": 2,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; Let us consider the following general optimization problem. > MATH >\\begin align > &\\min x && f x \\\\\\\\ > &\\text subject to && h i x \\le 0, \\text i=1,\\dots,m \\\\\\\\ > & && l j x = 0, \\text j=1,\\dots,r .\\\\\\\\ >\\end align > MATH The Karush–Kuhn–Tucker KKT conditions or KKT conditions consist of the following conditions 3 . MATH Stationarity : When MATH are fixed, the subdifferential with respect to MATH contains 0. MATH Complementary Slackness : At least one of MATH and MATH has value 0. MATH Primal Feasibility : Indicates whether the constraints of the primal problem are satisfied. MATH Dual Feasibility : Indicates whether the constraints of the dual problem are satisfied. Sufficiency For a convex primal problem, when there exist MATH that satisfy the KKT conditions, the following process shows that MATH are primal & dual solutions with zero duality gap. > MATH >\\begin align > g \\lambda^\\star, \\nu^\\star &= \\min x L x, \\lambda^\\star, \\nu^\\star \\\\\\\\ > &= L x^\\star, \\lambda^\\star, \\nu^\\star \\\\\\\\ > &= f x^\\star + \\sum i=1 ^m \\lambda i^\\star h i x^\\star + \\sum j=1 ^r \\nu j^\\star l j x^\\star \\\\\\\\ > &= f x^\\star >\\end align > MATH 1. MATH is a convex function. sum of convex functions 2. MATH therefore MATH . 3. By complementary slackness and primal feasibility, MATH . Necessity When MATH are primal & dual solutions with zero duality gap for example, satisfying Slater's condition , all the inequalities below become equalities, so in this problem MATH satisfy the KKT conditions. > MATH >\\begin align > f x^\\star &= g \\lambda^\\star, \\nu^\\star \\\\\\\\ > &= \\min x \\big f x + \\sum i=1 ^ m \\lambda i^\\star h i x + \\sum j=1 ^ r \\nu j^\\star l j x \\big \\\\\\\\ > &\\le f x^\\star + \\sum i=1 ^m \\lambda i^\\star h i x^\\star + \\sum j=1 ^r \\nu j^\\star l j x^\\star \\\\\\\\ > &\\le f x^\\star >\\end align > MATH 1. MATH means zero duality gap. 2. To satisfy MATH , complementary slackness and primal feasibility must be satisfied. 3. If MATH is satisfied, all inequalities in the above derivation become equalities. Putting it together In summary, KKT conditions are: Sufficient conditions for primal & dual solutions with zero duality gap. If strong duality holds, they become necessary conditions for primal & dual solutions. That is, for problems that satisfy strong duality, the following relationship holds. > MATH >\\begin align > x^\\star, \\lambda^\\star, \\nu^\\star \\text are primal and dual solutions \\\\\\\\ > \\Leftrightarrow x^\\star, \\lambda^\\star, \\nu^\\star \\text satisfy the KKT conditions \\\\\\\\ >\\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter12/12_01_Karush_Kuhn_Tucker_conditions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter12/12_02_Example_quadratic_with_equality_constraints",
    "title": "12-02 Example quadratic with equality constraints",
    "chapter": "12",
    "order": 3,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; A quadratic program with only equality constraints is as follows. > MATH >\\begin align > &\\min x && 1/2 x^T P x + q^T x + r \\\\\\\\ > &\\text subject to && Ax = b ,\\\\\\\\ >&\\text where &&P \\in \\mathbb S + ^n \\text and A \\in \\mathbb R ^ \\text p x n . >\\end align > MATH This problem is convex and has no inequality constraints, so it satisfies Slater's condition Strong duality . If the primal & dual solutions are MATH , then by KKT conditions they satisfy the following conditions 1 . Stationarity: MATH Complementary Slackness: Since there are no inequality constraints, this does not need to be considered. Primal & dual feasibility: MATH These conditions can be concisely expressed using a block matrix, which is called the KKT matrix 3 . > MATH > \\begin bmatrix > P & A^T \\\\\\\\ > A & 0 \\\\\\\\ > \\end bmatrix > \\begin bmatrix > x^\\star \\\\\\\\ > \\nu^\\star \\\\\\\\ > \\end bmatrix > = > \\begin bmatrix > -q \\\\\\\\ > b \\\\\\\\ > \\end bmatrix > MATH Solving this matrix equation gives the primal & dual solutions for the given problem. An interesting fact is that this problem can also be seen as computing the Newton step for an equality constrained problem 3 . For the problem MATH , if we set P, q, r as follows, then the objective function of the quadratic program becomes identical to the second-order Taylor expansion of MATH . > MATH , MATH , MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter12/12_02_Example_quadratic_with_equality_constraints/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter12/12_03_Example_water_filling",
    "title": "12-03 Example water-filling",
    "chapter": "12",
    "order": 4,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; Suppose we have the following convex optimization problem. > MATH >\\begin align > &\\min x && - \\sum i=1 ^n \\log \\alpha i + x i \\\\\\\\ > &\\text subject to && x \\succeq 0, 1^Tx = 1 ,\\\\\\\\ >&\\text where \\alpha i > 0. >\\end align > MATH This problem is about allocating power to n communication channels and arose in information theory. The variable MATH represents the output of the transmitter allocated to the i-th channel, and MATH represents the capacity or communication rate of that channel. That is, this problem is to determine how much power should be allocated to each channel to maximize the total communication rate 1 . Let the Lagrange multipliers for the inequality constraint MATH and equality constraint MATH be MATH and MATH respectively. The KKT conditions for the given problem are as follows. > MATH >\\begin align >x^\\star \\succeq 0, \\\\\\\\ >1^Tx^\\star = 1, \\\\\\\\ >\\lambda^\\star \\succeq 0, \\\\\\\\ >\\lambda i^\\star x i^\\star = 0, \\text i = 1, \\dots, n, \\\\\\\\ > -1 / \\alpha i + x i^\\star - \\lambda i^\\star + \\nu^\\star = 0, \\text i= 1, \\dots, n. >\\end align > MATH Using the equations obtained from KKT conditions, we can find MATH analytically. First, we eliminate MATH from the equations using it as a slack variable. > MATH >\\begin align >x^\\star \\succeq 0, \\\\\\\\ >1^Tx^\\star = 1, \\\\\\\\ >x i^\\star \\nu^\\star - 1 / \\alpha i + x i^\\star = 0, \\text i = 1, \\dots, n, \\\\\\\\ > \\nu^\\star \\ge 1/ \\alpha i + x i^\\star , \\text i= 1, \\dots, n. >\\end align > MATH This is organized as follows by stationarity and complementary slackness. > MATH > x i^\\star = > \\begin cases > 1 / \\nu^\\star - \\alpha i &\\nu^\\star 0 &\\nu^\\star \\ge 1/\\alpha i\\\\\\\\ > \\end cases > = \\max\\ 0, 1/\\nu^\\star - \\alpha i \\ , \\quad i = 1, \\dots, n. > MATH Also, by the condition MATH , MATH sum to 1. > MATH > \\sum i=1 ^n \\max\\ 0, 1/\\nu^\\star - \\alpha i \\ = 1. > MATH The left side of the equation is a piecewise-linear increasing function of MATH , so this equation has a unique solution for fixed MATH . This solution method is called water-filling. When MATH is the ground level for patch MATH , this problem can be thought of as pouring water into each region so that the water level becomes MATH as shown in the figure below. We pour water until the total amount of water becomes 1. Fig1 Illustration of water-filling algorithm 1",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter12/12_03_Example_water_filling/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter12/12_04_Example_support_vector_machines",
    "title": "12-04 Example support vector machines",
    "chapter": "12",
    "order": 5,
    "owner": "Wontak Ryu",
    "lesson_type": "required",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; The support vector machine problem for non-separable sets is as follows. > MATH >\\begin align > &\\min \\beta, \\beta-0, \\xi && \\frac 1 2 \\rVert\\beta\\rVert 2^2 + C\\sum i=1 ^n \\xi i \\\\\\\\ > &\\text subject to && \\xi i \\ge 0, \\quad i = 1, \\dots, n \\\\\\\\ > & && y i x i^T \\beta + \\beta-0 \\ge 1 - \\xi i, \\quad i = 1, \\dots, n,\\\\\\\\ >&&&\\text given y \\in \\ -1, 1\\ ^n \\text and X \\in \\mathbb R ^ n \\times p . >\\end align > MATH When the Lagrange multipliers for the two inequality constraints of the given problem are MATH respectively, the Lagrangian function is as follows. > MATH Using the above Lagrangian function, we can find the following conditions that make this problem satisfy the KKT stationarity condition. Derive the conditions where the Lagrangian function becomes 0 when differentiated with respect to MATH respectively > MATH >0 = \\sum i=1 ^n w i^\\star y i, \\quad \\beta = \\sum i=1 ^n w i^\\star y i x i, \\quad w^\\star = C \\cdot 1 - v^\\star > MATH Also, complementary slackness for the two inequality constraints is as follows. > MATH > v i^\\star \\xi i = 0, \\quad w i^\\star 1 - \\xi i - y i x i^T \\beta + \\beta-0 =0, \\quad 1 = 1, \\dots, n. > MATH That is, at optimality, MATH is satisfied, and when MATH , MATH becomes nonzero, and such point i is called support points . For a support point i where MATH , MATH is located on the hyperplane and MATH . For a support point i where MATH , MATH is located on the opposite side of the hyperplane and MATH . MATH SVM문제를 최적화 하기 전, non-support points를 걸러내는데 위 방법을 이용할 수 있다 non-support points를 걸러냄으로써 계산 효율을 높일 수 있다 . 사실 KKT conditions는 이 문제의 solution을 도출하기 위한 직접적인 역할을 하지는 않지만, 우리는 이를 통해 SVM 문제를 더 잘 이해하기 위한 직관을 얻을 수 있다 3 .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter12/12_04_Example_support_vector_machines/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter12/12_05_Constrained_and_Lagrange_forms",
    "title": "12-05 Constrained and Lagrange forms",
    "chapter": "12",
    "order": 6,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In statistics and machine learning, we often move between constrained form and Lagrange form . Let us define the constrained form and Lagrangian form as follows. Constrained Form C hereafter > MATH >\\min x \\: f x \\quad \\text subject to h x \\le t,\\\\\\\\ >\\text where t \\in \\mathbb R \\text is a tuning parameter. > MATH Lagrange Form L hereafter > MATH >\\min x \\: f x + \\lambda \\cdot h x ,\\\\\\\\ >\\text where \\lambda \\ge 0 \\text is a tuning parameter. > MATH When MATH are convex, let us examine the cases where the two problems yield the same solution. 1. C to L : When C is strictly feasible satisfies Slater's condition and satisfies strong duality, if there exists a dual solution MATH that minimizes the following objective function for the solution MATH of C , then MATH is also a solution of L . MATH 2. L to C : If MATH is a solution of L and C with MATH satisfies the KKT conditions, then MATH is also a solution of C . This is because MATH that satisfy the KKT conditions of L also satisfy the KKT conditions of C with MATH . > MATH L 의 KKT conditions: > > MATH > \\begin align > \\nabla x f x^\\star + \\lambda^\\star \\nabla x h x^\\star &= 0\\\\\\\\ > \\lambda^\\star &\\ge 0\\\\\\\\ > \\end align > MATH > > > MATH MATH 인 C 의 KKT conditions: > > MATH > \\begin align > \\nabla x f x^\\star + \\lambda^\\star \\nabla x h x^\\star &= 0\\\\\\\\ > \\lambda^\\star &\\ge 0\\\\\\\\ > \\lambda^\\star \\underbrace h x^\\star - h x^\\star =0 &= 0 > \\end align > MATH In conclusion, 1 and 2 show the following relationships respectively. Fig1 Conclusion 3 So, under what circumstances do C and L show perfect equivalence? For example, when MATH such as norm , MATH , and MATH , perfect equivalence is shown. Due to the given conditions, the constraint in C becomes MATH , and by setting MATH to MATH , L also imposes the same constraint MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter12/12_05_Constrained_and_Lagrange_forms/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter12/12_06_Uniqueness_in_L1_penalized_problems",
    "title": "12-06 Uniqueness in L1 penalized problems",
    "chapter": "12",
    "order": 7,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; The following MATH penalized linear regression problem is also well known as the lasso problem. > MATH >\\begin align >&&&\\hat \\beta \\in \\text argmin \\beta \\in \\mathbb R ^p \\frac 1 2 \\| y - X\\beta \\|^2 2 + \\lambda \\|\\beta\\| 1, \\qquad \\\\\\\\ >&& \\text --- 1 &\\text given y \\in \\mathbb R ^n, \\\\\\\\ >&&& \\text a matrix X \\in \\mathbb R ^ n \\text x p \\ \\text of predictor variables, \\\\\\\\ >&&& \\text and a tuning parameter \\lambda \\ge 0. >\\end align > MATH The above Lasso problem has a unique solution when it is strictly convex, i.e., when MATH . On the other hand, when MATH rank X Interestingly, in some special cases, the Lasso problem is guaranteed to have a unique solution regardless of the dimension of MATH 13 . > Theorem: When the function MATH is differentiable and strictly convex, MATH , and MATH follows some continuous probability distribution on MATH , the following optimization problem always has a unique solution. Moreover, the solution consists of at most MATH nonzero components. There are no restrictions on the dimension of MATH . That is, it is valid even when p >> n Basic facts and the KKT conditions > Lemma 1. For arbitrary MATH , the lasso problem 1 has the following properties. > > 1. It has either a unique solution or infinitely many solutions. > 2. All lasso solutions MATH have the same MATH value. >3. When MATH , all lasso solutions MATH have the same MATH norm MATH . MATH > Proof. > 1. If 1 has two solutions MATH , MATH , then for any MATH 0 > 2. & 3. Suppose there are two solutions MATH , MATH . Let the optimal value be MATH . Then for any solution MATH MATH 0 > MATH >\\begin align > &\\frac 1 2 \\| y - X \\alpha \\hat \\beta ^ 1 + 1 - \\alpha \\hat \\beta ^ 2 \\| 2^2 + \\lambda \\| \\alpha \\hat \\beta ^ 1 + 1 - \\alpha \\hat \\beta ^ 2 \\| 1 \\\\ > & = \\alpha c^\\star + 1-\\alpha c^\\star = c^\\star >\\end align > MATH > > To satisfy this equality, MATH must always have the same value for any solution MATH , and when MATH , MATH must also always be the same. Returning to the beginning, the KKT conditions for the lasso problem 1 are as follows. > MATH >\\begin align >&&X^T y - X\\hat \\beta = \\lambda \\gamma, \\qquad \\text --- 2 \\\\\\\\ >&&\\gamma i \\in > \\begin cases > \\ sign \\hat \\beta i \\ & if \\hat \\beta i \\neq 0 \\\\\\\\ > -1, 1 & if \\hat \\beta i = 0, > \\end cases \\\\\\\\ > &&\\text for i = 1, \\dots, p. \\text --- 3 \\\\\\\\ > &&\\text Here \\gamma \\in \\mathbb R ^p \\text is called a subgradient of the function \\\\ > &&f x = \\| x \\| 1 \\text evaluated at x = \\hat \\beta . >\\end align > MATH 즉, 1 의 solution인 MATH 는 어떤 MATH 에 대해 2 와 3 을 만족한다. 위에서 얻은 KKT conditions를 이용하여 lasso solution에 대한 조건을 좀 더 명시적인 형태로 변환해보도록 하자. 이후의 진행에서는 유도의 간결함을 위해 MATH 를 가정하도록 한다. 우선 equicorrelation set MATH 을 다음과 같이 정의한다. MATH 는 MATH 인 모든 인덱스 MATH 와 MATH 이면서 MATH 인 모든 인덱스 MATH 를 원소로 가진 집합이다. MATH \\mathcal E = \\ i \\in \\ 1, \\dots, p \\ : \\vert X i^T y - X\\hat \\beta \\vert = \\lambda \\ . \\qquad \\text --- 4 MATH 또한 equicorrelation sign MATH 를 다음과 같이 정의한다. 여기서 MATH 는 행렬 X에서 MATH 인 column MATH 외의 모든 column을 0 벡터로 교체한 행렬을 의미한다. MATH s = sign X^T \\mathcal E y -X\\hat \\beta . \\qquad \\text --- 5 MATH 여기서 MATH 는 MATH 에 대해 다음과 같이 표현할 수 있다: MATH and MATH . 또한 Lemma1-2에 의해 MATH 는 유일한 값을 가지므로 이는 MATH , MATH 이 유일함을 암시한다. 3 의 subgradient MATH 에 대한 정의에 의해 모든 lasso solution MATH 에 대해 MATH 임을 알 수 있다. 그러므로 2 를 MATH 블록에 대해 표현하면 다음과 같다. MATH X^T \\mathcal E y - X \\mathcal E \\hat \\beta \\mathcal E = \\lambda \\gamma \\mathcal E = \\lambda s. \\qquad \\text --- 6 MATH 6 의 양변에 MATH 를 곱하면 다음과 같이 정리된다 MATH 는 MATH 의 pseudoinverse matrix . MATH \\begin align & X^T \\mathcal E X \\mathcal E \\hat \\beta \\mathcal E = X^T \\mathcal E y - X^T \\mathcal E ^+ \\lambda s \\\\\\\\ \\Leftrightarrow & X \\mathcal E \\hat \\beta \\mathcal E = X^T \\mathcal E X^T \\mathcal E ^+ y - X^T \\mathcal E ^+ \\lambda s . \\end align MATH MATH 이므로 위 등식은 곧 아래와 같다. MATH X \\hat \\beta = X^T \\mathcal E X^T \\mathcal E ^+ y - X^T \\mathcal E ^+ \\lambda s , \\qquad \\text --- 7 MATH 그리고 임의의 lasso solution MATH 는 다음과 같다. MATH \\begin align & \\hat \\beta -\\mathcal E = 0 \\text and \\hat \\beta \\mathcal E = X^T \\mathcal E ^+ y - X^T \\mathcal E + b, \\qquad \\text --- 8 \\\\\\\\ & \\text where b \\in null X \\mathcal E . \\end align MATH Sufficient conditions for uniqueness 8 의 MATH 의 유일함이 보장되기 위해서는 MATH 이 되어야 한다 MATH 은 유일하기 때문에 . MATH 이어야 함을 주지하고 8 의 등식을 변형하면 다음의 결론을 얻게 된다. > Lemma 2. 임의의 MATH 에 대해, 만약 MATH , 또는 MATH 참고 https://www.quora.com/When-the-null-space-of-a-matrix-is-the-zero-vector-the-matrix-is-in\\vertible-Why/answer/Alexander-Farrugia ,이면 lasso solution은 유일 unique 해지며, 이는 곧 다음과도 같다. > MATH >\\begin align >&& \\hat \\beta -\\mathcal E = 0 \\text and \\hat \\beta \\mathcal E = X^T \\mathcal E X^T \\mathcal E ^ -1 X^T \\mathcal E y - \\lambda s , \\qquad \\text --- 9 \\\\\\\\ >&& \\text where \\mathcal E \\text and s \\text are the equicorrelation set and signs as defined in 4 and 5 . >\\end align > MATH 참고로 이 solution은 많아 봐야 MATH 의 nonzero components로 구성된다. 그렇다면 MATH 을 암시하는 MATH 에 대한 좀 더 자연스러운 조건에 대해 알아보도록 하자. 이를 알아보기에 앞서 우선 MATH 이라 가정해보겠다. 이 경우, 어떤 MATH 에 대해 다음과 같은 등식을 만족한다. MATH X i = \\sum j \\in \\mathcal E \\backslash \\ i\\ c j X j,\\\\\\\\ \\text where c j \\in \\mathbb R , j \\in \\mathcal E . MATH 위 등식의 양변에 MATH 를 곱해주고, 우항에 MATH 을 곱해준다. MATH s i X i = \\sum j \\in \\mathcal E \\backslash \\ i\\ s i s j c j \\cdot s j X j . \\qquad \\text --- 10 MATH MATH 로 r lasso residual 을 정의하면 임의의 MATH 에 대해 MATH 를 만족한다. r을 위 10 의 양변에 곱해주면 MATH 에 대한 부등식을 얻을 수 있다. MATH 이라 가정 MATH \\lambda = \\sum j \\in \\mathcal E \\backslash \\ i\\ s i s j c j \\lambda \\quad \\text and \\quad \\sum j \\in \\mathcal E \\backslash \\ i\\ s i s j c j = 1. MATH 즉, MATH 이면, 어떤 MATH 에 대해 다음 등식이 성립한다. MATH s iX i = \\sum j \\in \\mathcal E \\backslash \\ i\\ a j \\cdot s j X j, \\text with \\sum j \\in \\mathcal E \\backslash \\ i\\ a j = 1. MATH 위 등식은 MATH 이 MATH 의 affine span 위에 존재한다는 의미와도 같다. 또한 이는 어떤 k+2개의 원소를 포함한 subset으로는 최대 k dimensional affine space만을 표현할 수 있다는 것과도 같다. Fig 1 4 elements on 2-dimensional affine space 3 우리가 원하는 것은 행렬 MATH 가 MATH 을 만족하는 것이며, 이는 곧 행렬 MATH 의 column들이 general position https://en.wikipedia.org/wiki/General position 에 있는 것과도 같다. 바꿔말하면, 그 어떤 k-dimensional affine subspace도 set 안의 k+1개보다 더 많은 element를 포함하지 않는다는 것이다. > Lemma 3. 만약 행렬 MATH 의 column들이 general position에 있으면, 임의의 MATH 와 MATH 에 대한 lasso solution은 유일 unique 하며 또한 이 solution은 9 를 만족한다. 그렇다면 어떤 행렬 MATH 가 항상 위 조건을 만족할 수 있을까? 결론부터 말하자면 다음과 같다. > Lemma 4. 행렬 MATH 의 모든 원소가 MATH 상의 continuous probability distribution을 따른다면, 임의의 MATH 와 MATH 에 대해 lasso solution은 unique하고 항상 9 를 만족한다. 왜냐하면 continuous probability distribution을 따를때, 모든 column vector들은 linearly independent하기 때문이다. 참고 https://math.stackexchange.com/questions/432447/probability-that-n-vectors-drawn-randomly-from-mathbbrn-are-linearly-ind?rq=1 General convex loss functions 좀 더 일반적인 lasso problem에 대해서도 같은 내용을 적용할 수 있다 13 . MATH \\hat \\beta \\in \\text argmin \\beta \\in \\mathbb R ^p f X\\beta + \\lambda \\|\\beta\\| 1, \\qquad \\text --- 11 MATH > Lemma 5. 만약 행렬 MATH 의 모든 원소가 MATH 상의 continuous probability distribution을 따를때, 미분 가능하고 strictly convex인 임의의 함수 MATH 는 임의의 MATH 에 대해 11 의 문제에서 항상 유일 unique 한 solution을 보장한다. 이 solution은 많아봐야 MATH 개의 nonzero components로 구성된다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter12/12_06_Uniqueness_in_L1_penalized_problems/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_Duality_uses_and_correspondences",
    "title": "13 Duality uses and correspondences",
    "chapter": "13",
    "order": 1,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "In this chapter, we will learn about the uses of Duality and related correspondences through examples. Notice In this chapter, the optimal solution MATH and the conjugate MATH of MATH are distinguished and denoted as such.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_Duality_uses_and_correspondences/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_01_Uses_of_duality",
    "title": "13-01 Uses of duality",
    "chapter": "13",
    "order": 2,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Two key uses of duality Let's revisit the two key characteristics of duality covered in Chapter 11. • When MATH is primal feasible and MATH are dual feasible, the difference between the primal problem MATH and the dual problem MATH is called the duality gap between MATH and MATH . > MATH When the duality gap is 0, this is called zero duality gap, which means that the solution to the dual problem is optimal. Also, the upper bound MATH is always less than or equal to the optimal value MATH . For detailed reasons, please refer to the content in Chapter 11 % multilang post url contents/chapter11/21-03-24-11 00 Duality in General Programs % . Therefore, the following derivation is possible. Proof > MATH \\begin align > f^ \\star &\\ge g u, v \\\\ > -f^ \\star &\\le -g u, v \\\\ > f x -f^ \\star &\\le \\underbrace f x -g u, v \\text duality gap \\\\ > g^ \\star -g x &\\le \\underbrace f x -g u, v \\text duality gap \\\\ > \\end align MATH Moreover, the duality gap can be used as a stopping criterion for algorithms. • When dual optimal MATH are given, under the condition of strong duality, the primal solution minimizes the Lagrangian MATH for all MATH i.e., satisfies the stationarity condition . This can be used for computing the primal solution.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_01_Uses_of_duality/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_02_Solving_the_primal_via_the_dual",
    "title": "13-02 Solving the primal via the dual",
    "chapter": "13",
    "order": 3,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "An important consequence of stationarity Under the condition of strong duality, when dual solution MATH is given, the primal solution MATH can be found by solving the following Lagrangian: > MATH Often, the solution to such unconstrained problems can be represented by explicitly bringing the characteristics of the primal solution through the dual solution. Moreover, if the solution to this problem is unique, the dual solution becomes the primal solution MATH . This is very useful when solving the dual problem is easier than solving the primal problem directly. Example from B & V page 249: > MATH > MATH \\begin align > g v &= \\min x \\sum i=1 ^n f i x i + v b−a^Tx \\\\\\ > &= bv + \\min x \\sum i=1 ^n f i x i −va^Tx \\\\\\ > &= bv + \\min x \\sum i=1 ^n f i x i −v \\sum i=1 ^n a ix i \\\\\\ > &= bv + \\sum i=1 ^n \\underbrace \\min x i \\ f i x i − a ivx i \\ -f^ i a iv \\\\\\ > &= bv − \\sum i=1 ^n f^ i a iv > \\end align MATH Here MATH denotes the conjugate of MATH . Therefore, the dual problem can be represented as follows: > MATH Also, by multiplying by minus - , the maximum problem can be represented as the following minimum problem: > MATH This is a convex minimization problem in scalar variables that can be solved much more easily than the primal problem. When MATH is given, the primal solution MATH can be solved as follows: > MATH The strict convexity of each MATH means that this has a unique solution. That is, MATH is obtained through the calculation of MATH for each MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_02_Solving_the_primal_via_the_dual/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_03_Dual_norms",
    "title": "13-03 Dual norms",
    "chapter": "13",
    "order": 4,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Let's examine an arbitrary norm MATH : > MATH > > MATH The dual norm of norm MATH is denoted as MATH and is defined as follows: > MATH Suppose there exists some arbitrary MATH . Then when we let MATH , we have MATH . Also, MATH holds, therefore a form similar to the Cauchy-Schwartz inequality MATH is established. Let's examine examples of dual norms under specific conditions: > MATH > > MATH Proof > Let's prove that the following holds for the MATH norm dual: > > MATH > > Proof > > MATH > > Meanwhile, by Holder's inequality, the following holds: > > MATH > > From 1 above, the following relationship holds: > > MATH > > Therefore, if we find MATH such that MATH and MATH satisfies MATH , we can see that MATH holds. > > Meanwhile, if we set MATH and MATH , then MATH > satisfies MATH , and we can confirm that MATH . > > Therefore, the following holds: > > MATH The dual norm of a dual norm becomes the original norm again. > MATH Let's examine the following problem: > MATH When the optimal value is MATH , the Lagrangian is expressed as follows: > MATH When expressed in terms of the dual norm MATH , it becomes: > MATH > > MATH Note > MATH > > MATH > > MATH > > MATH > > MATH MATH with MATH > > MATH MATH > > MATH > > MATH > > MATH > > dual problem > > MATH Therefore, the Lagrange dual problem is as follows: > MATH Since there are no inequality constraints, Slater's condition is satisfied, and according to strong duality, MATH . That is, MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_03_Dual_norms/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_04_Conjugate_function",
    "title": "13-04 Conjugate function",
    "chapter": "13",
    "order": 5,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "For a given function MATH , the conjugate MATH is defined as follows: > MATH Fig1 Illustration of conjugate function 1 Note MATH is always convex since it is the pointwise maximum of convex affine functions MATH . Here, MATH does not necessarily need to be convex. MATH is the maximum gap between the linear function MATH and MATH . From B & V page 91 Conjugation for differentiable MATH is called the Legendre transform. Properties: • Fenchel’s inequality: for any MATH > MATH > MATH • The conjugate of a conjugate is MATH , so MATH holds. • If MATH is closed and convex, then MATH . • If MATH is closed and convex, then for all MATH , the following holds: > MATH \\begin align > x ∈ ∂f^ ∗ y &\\iff y ∈ ∂f x \\\\\\ > &\\iff f x + f^ ∗ y = x^Ty \\\\\\ > \\end align MATH • If MATH , then MATH holds. Examples: • Let's examine the case where MATH is a simple quadratic as follows: > MATH , where MATH Then MATH is strictly concave in MATH and reaches its maximum at MATH . That is, MATH . Proof > MATH \\begin align > f^ y & = \\max x \\left y^Tx -\\frac 1 2 x^TQx \\right \\\\\\ > & = -\\min x \\left \\frac 1 2 x^TQx- y^Tx \\right , x^ \\star = Q^ -1 y \\\\\\ > & = -\\frac 1 2 y^TQ^ -1 QQ^ -1 y + y^TQ^ -1 y \\\\\\ > & = \\frac 1 2 y^TQ^ -1 y \\\\\\ > \\end align MATH > Fenchel's inequality: for any MATH > MATH • Indicator function: If MATH , then its conjugate is as follows: > MATH called the support function of MATH • Norm: If MATH , then its conjugate is as follows: > MATH where MATH is the dual norm of MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_04_Conjugate_function/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_04_01_Example_lasso_dual",
    "title": "13-04-01 Example lasso dual",
    "chapter": "13",
    "order": 6,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Let’s look at the lasso problem for MATH : > MATH > > MATH > > MATH > > MATH The dual function of the above is constant = MATH . Therefore, we can transform the primal problem as follows: > MATH The dual function of the transformed problem is: > MATH \\begin align > g u &= \\min \\beta,z \\frac 1 2 \\| y-z \\|^2 2 + \\lambda \\| \\beta \\| 1 + u^T z-X\\beta \\\\ > &= \\frac 1 2 \\| y \\|^2 2 - \\frac 1 2 \\| y-u \\|^2 2 - I \\ v : \\| v \\| \\infty \\leq 1 \\ X^Tu/\\lambda \\\\ > \\end align MATH > Proof > MATH \\begin align > g u &= \\min \\beta,z \\frac 1 2 \\| y-z \\|^2 2 + \\lambda \\| \\beta \\| 1 + u^T z-X\\beta \\\\ > &= \\underbrace \\left \\min z \\frac 1 2 \\| y - z \\|^2 2 + u^Tz \\right 1 + \\underbrace \\left \\min \\beta \\lambda \\| \\beta \\| 1 + u^TX\\beta \\right 2 \\\\ > \\end align MATH > > MATH > > MATH \\begin align > 1 \\cdots \\left \\min z \\frac 1 2 \\| y - z \\|^2 2 + u^Tz \\right > &= \\frac 1 2 \\| u \\|^2 2 + u^T y - u \\\\ > &= -\\frac 1 2 \\| y - u \\|^2 2 + \\frac 1 2 \\| y \\|^2 2 \\\\ > \\end align MATH > MATH \\begin align > 2 \\cdots \\left \\min \\beta \\lambda \\| \\beta \\| 1 + u^TX\\beta \\right > &= - \\lambda \\max \\beta \\frac u^Tx \\lambda \\beta - \\| \\beta \\| 2 \\\\ > &= - \\lambda \\left \\| \\frac u^Tx \\lambda \\| \\infty \\leq 1 \\right \\\\ > &= - \\lambda \\left \\| u^Tx \\| \\infty \\leq \\lambda \\right \\\\ > \\end align MATH > \\therefore g u = -\\frac 1 2 \\| y - u \\|^2 2 + \\frac 1 2 \\| y \\|^2 2 + - \\lambda \\left \\| u^Tx \\| \\infty \\leq \\lambda \\right > = \\frac 1 2 \\| y \\|^2 2 - \\frac 1 2 \\| y-u \\|^2 2 - I \\ v : \\| v \\| \\infty \\leq 1 \\ X^Tu/\\lambda Therefore, the lasso dual problem is: > MATH This is equivalent to: > MATH Check Slater’s condition is satisfied, so strong duality holds. > MATH > > MATH note The optimal value in the previous problem is not the optimal lasso objective value. However, the dual solution MATH and the lasso solution MATH satisfy MATH . This is satisfied by the KKT stationarity condition MATH i.e., MATH . Therefore, lasso satisfies the dual residual. Fig2 Lasso Dual 1",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_04_01_Example_lasso_dual/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_04_02_Conjugates_and_dual_problems",
    "title": "13-04-02 Conjugates and dual problems",
    "chapter": "13",
    "order": 7,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Conjugates can often be represented through the derivation of dual problems for Lagrangian minimization problems as follows: > MATH For example, consider the following expression: > MATH The following expression has constraints added to the above expression and is equivalent to it: > MATH Converting this to a Lagrange dual function gives: > MATH Therefore, the dual problem of the original expression can be defined as follows: > MATH Examples • Indicator function: The dual of MATH is as follows: > MATH > > where MATH is the support function of MATH • Norms: The dual of MATH is as follows: MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_04_02_Conjugates_and_dual_problems/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_04_03_Shifting_linear_transformations",
    "title": "13-04-03 Shifting linear transformations",
    "chapter": "13",
    "order": 8,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "The dual formulation helps with shifting linear transformations between parts of the objective function and other domains. Let’s look at the following: > MATH The following expression is equivalent: > MATH This leads to the following dual process: > MATH > MATH > MATH And the dual is: > MATH Example Norms and their dual norms are related as follows: MATH , the problems > MATH > > MATH The first expression is the primal, and the second is the dual, which can be solved directly.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_04_03_Shifting_linear_transformations/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_05_Dual_cones",
    "title": "13-05 Dual cones",
    "chapter": "13",
    "order": 9,
    "owner": "",
    "lesson_type": "",
    "content": "Dual cones There exists a cone MATH . Recalling the content covered earlier in 02-06-01 % multilang post url contents/chapter02/21-02-11-02 06 01 Dual cones % , this means MATH . > MATH This is called a dual cone , and it is always a convex cone even when MATH is not convex . Fig3 Dual Cones 1 Note MATH From B & V page 52 An important property here is that if MATH is closed and a convex cone, then MATH . Examples: • The dual cone of a linear subspace MATH is MATH , i.e., the orthogonal complement. E.g., MATH • The dual cone of the norm cone MATH is the norm cone of its dual norm MATH . • The positive semidefinite cone MATH is a self-dual convex cone, meaning MATH . Why is this the case? Let's verify: > MATH MATH 's eigenvalue decomposition Dual cones and dual problems Consider the cone constrained problem: > MATH When MATH is the support function of MATH , the dual problem of the above expression is as follows: > MATH When MATH is a cone, this can be easily defined as follows: > MATH Here MATH is the dual cone of MATH . because MATH This is quite useful because many different types of constraints can appear as cone constraints.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_05_Dual_cones/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter13/13_06_Dual_subtleties_Double_dual",
    "title": "13-06 Dual subtleties & Double dual",
    "chapter": "13",
    "order": 10,
    "owner": "Wontak Ryu",
    "lesson_type": "",
    "content": "Dual subtleties • Sometimes we can transform dual problems into equivalent problems and still call them dual problems. Also, in strong duality, we can use solutions of transformed dual problems for analyzing or computing characteristics of primal solutions. Note The optimal value of a transformed dual problem is not necessarily the optimal value of the primal. • A common way to derive dual problems for unconstrained problems is to first transform the primal by adding dummy variables and equality constraints. Generally, how to do this is ambiguous. Different choices can lead to different dual problems. Double dual Let's consider a general minimization problem with linear constraints: > MATH The Lagrangian is as follows: > MATH Therefore, the dual problem is as follows: > MATH Recall property If MATH is closed and convex, we explained earlier that the dual of the dual is the primal in this case MATH . In fact, the connection between dual and dual conjugate goes much deeper beyond linear constraints. Consider the following: > MATH >\\begin align > & \\min x && f x \\\\ > &\\text subject to && h i x ≤ 0, i = 1,...m \\\\ > &&&l j x = 0, j = 1,...r >\\end align MATH If MATH and MATH are closed and convex, and MATH are affine, then the dual of the dual is the primal. This is provided as a minimization problem from the perspective of bifunctions. for more, read Chapters 29 and 30 of Rockafellar",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter13/13_06_Dual_subtleties_Double_dual/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_newton_method",
    "title": "14 Newton's Method",
    "chapter": "14",
    "order": 1,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this chapter, we look at Newton's Method. Newton's method is an approach for finding the minimum of a function with a twice differentiable function using the second-order Taylor expansion. It finds the minimum by approximating the function and iteratively updating the solution. Near the optimum, it achieves quadratic convergence and is much faster than gradient descent. References and further readings S. Boyd and L. Vandenberghe 2004 , \"Convex optimization\", Chapter 9 and 10 Y. Nesterov 1998 , \"Introductory lectures on convex optimization: a basic course\", Chapter 2 Y. Nesterov and A. Nemirovskii 1994 , \"Interior-point polynomial methods in convex programming\", Chapter 2 J. Nocedal and S. Wright 2006 , \"Numerical optimization\", Chapters 6 and 7 L. Vandenberghe, Lecture notes for EE 236C, UCLA, Spring 2011-2012",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_newton_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_01_newton_method",
    "title": "14-01 Newton's method",
    "chapter": "14",
    "order": 2,
    "owner": "Minjoo Lee",
    "lesson_type": "required",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; The gradient descent methods we have discussed are called first-order methods because the solutions are based on the first derivative of the function. Newton's method is a second-order method , meaning the solution requires computing the second derivative. Let us consider an optimization problem for a function MATH that is unconstrained, twice differentiable, convex, and has dom MATH = MATH . > MATH >\\begin align >\\min x f x >\\end align > MATH In Gradient descent % multilang post url contents/chapter06/21-03-20-06 00 gradient descent % , we performed the following process for this function MATH : 1. Perform second-order Taylor approximation 2. Assume the Hessian matrix corresponding to the second derivative term as MATH , i.e., the identity matrix divided by t step size 3. Perform quadratic approximation to proceed with the update step The detailed process is explained in the gradient descent update step on the next page. The update step formula at each iteration is as follows: > MATH >\\begin align >&\\text choose initial x^ 0 \\in \\mathbb R ^ n ,\\\\\\\\ >&x^ k = x^ k-1 - t k \\cdot \\nabla f x^ k-1 , \\qquad k = 1,2,3,... >\\end align > MATH Newton's method pure Newton's method actually computes the second derivative term that was assumed as MATH in gradient descent, performs quadratic approximation, and proceeds with the update step. This process is also explained in the Newton's method update step on the next page. The update step formula at each iteration is as follows: > MATH >\\begin align >&\\text choose initial x^ 0 \\in \\mathbb R ^ n ,\\\\\\\\ >&x^ k = x^ k-1 - \\Big \\nabla^ 2 f x^ k-1 \\Big ^ -1 \\nabla f x^ k-1 , \\qquad k = 1,2,3,... >\\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_01_newton_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_02_interpretation_and_properties",
    "title": "14-02 Interpretation & Properties",
    "chapter": "14",
    "order": 4,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In this chapter, before examining the properties of Newton's method, we learn about applying Newton's method to root finding problems of objective functions. Subsequently, we will examine two important properties of Newton's method: Affine invariance and Local convergence.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_02_interpretation_and_properties/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_02_01_root_finding",
    "title": "14-02-01 Root finding",
    "chapter": "14",
    "order": 5,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In this chapter, we apply Newton's method to the root finding problem. The Newton's method used in optimization problems has some differences, which are explained here. Newton's method in optimization https://en.wikipedia.org/wiki/Newton%27s method in optimization Newton's method in root finding https://en.wikipedia.org/wiki/Newton%27s method Newton's method for root finding Suppose we have a vector function MATH . The root finding problem is to find MATH such that MATH . > MATH >\\begin align >F x = 0. >\\end align > MATH This problem is solved by choosing an initial value MATH and applying Newton's method iteratively: > MATH >\\begin align >&\\text choose initial x^ 0 \\in \\mathbb R ^ n ,\\\\\\\\ >&x^ k =x^ k-1 -\\nabla F x^ k-1 ^ -1 F x^ k-1 , \\qquad k=1,2,3,...\\\\\\\\ >\\end align > MATH Here, MATH is the Jacobian matrix of MATH at MATH . The Newton step MATH can be derived using the linear approximation of MATH : > MATH >\\begin align >F y \\approx F x + F^ ' x y-x = 0\\\\\\\\ >y = x^ + =x-F^ ' x ^ -1 F x . >\\end align > MATH Newton's method for optimization problem Suppose we apply Newton's method to an optimization problem formulated as: > MATH >\\begin align >\\min x F x >\\end align > MATH This is equivalent to applying Newton's method to the root finding problem for the gradient MATH of the objective function MATH . In summary, unlike the problem of finding the root of a given function's derivative MATH using Newton's method in optimization problems, the root finding problem requires finding the root of the function value itself MATH using Newton's method. This results in a difference of one order in the derivative term in the update formula for MATH in Newton's method for each problem. Root finding example Consider a function MATH defined as follows: > MATH >\\begin align >F x =x^ 2 -2 >\\end align > MATH Starting with an initial guess of MATH , we apply pure Newton's method, yielding the following results: Fig 1 Newton's method applied on example 3 As the iteration count MATH increases, we can see that the value of MATH approaches the root MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_02_01_root_finding/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_02_02_affine_invariance_of_newton_method",
    "title": "14-02-02 Affine invariance of Newton's method",
    "chapter": "14",
    "order": 6,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; One of the important properties of Newton's method is that it is affine invariant. This means that the direction of the update is independent of affine transformations of the coordinate system. For example, gradient descent is variant to affine transformations, so the convergence speed differs depending on the coordinate space. This page derives the affine invariance property. Affine invariance : proof Let MATH be twice differentiable, and let MATH be nonsingular. Also, define MATH as MATH . MATH . This means that some function MATH that takes MATH as input has the same output as function MATH that takes MATH affine transformed by MATH with respect to MATH as input. To reduce confusion about notation and gradient arguments, we define MATH . Using the chain rule to differentiate both sides once and twice, we get the following results: > MATH >\\begin align >\\nabla g y &= A^ T \\nabla f x \\\\\\\\ >\\nabla^ 2 g y &= A^ T \\nabla^ 2 f x A, >\\end align > MATH The Newton step of MATH with respect to MATH is as follows: > MATH >\\begin align >y^ + = y- \\nabla^ 2 g y ^ -1 \\nabla g y . >\\end align > MATH Here, instead of function MATH , if we transform and organize it in terms of function MATH with respect to MATH , we can derive the Newton step for MATH and MATH . > MATH >\\begin align >y^ + &= y- A^ T \\nabla^ 2 f x A ^ -1 A^ T \\nabla f x \\\\\\\\ >\\Leftrightarrow y^ + &= y-A^ -1 \\nabla^ 2 f x ^ -1 A^ T ^ -1 A^ T \\nabla f x \\\\\\\\ >\\Leftrightarrow Ay^ + &= Ay- \\nabla^ 2 f x ^ -1 \\nabla f x \\\\\\\\ >\\Leftrightarrow x^ + &= x - \\nabla^ 2 f x ^ -1 \\nabla f x . >\\end align > MATH This means that the Newton step is affine invariant, i.e., the updates in coordinate systems transformed by affine transformations represented by non-singular matrices are identical to each other. Using the same method to check the affine invariance of gradient descent by deriving the step update, we can obtain the following result: > MATH >\\begin align >y^ + &= y-t k \\cdot \\nabla g y \\\\\\\\ >\\Leftrightarrow y^ + &= y-t k \\cdot \\nabla f x A^ T \\\\\\\\ >\\Leftrightarrow x^ + &= x - t k \\cdot A\\nabla f x A^ T . >\\end align > MATH In the case of gradient descent, since the Hessian matrix is approximated as MATH for updates, we can see that the direction of the update changes for affine transformed coordinates.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_02_02_affine_invariance_of_newton_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_03_newton_decrement",
    "title": "14-03 Newton decrement",
    "chapter": "14",
    "order": 7,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In this chapter, we define the Newton decrement and examine its meaning. For the optimization problem below, the Newton decrement at MATH is defined as MATH : >\\begin align >\\min x \\quad f x ,\\\\\\\\ >\\end align >\\begin align >\\lambda x = \\nabla f x ^ T \\nabla^ 2 f x ^ -1 \\nabla f x ^ 1/2 . >\\end align Characteristics of Newton decrement First, the Newton decrement is related to the difference between the function MATH and the minimum of its quadratic approximation. Calculating this difference gives: > MATH \\begin align >f x -&\\min y \\big f x +\\nabla f x ^ T y-x +\\frac 1 2 y-x ^ T \\nabla^ 2 f x y-x \\big ,\\\\\\\\ >f x -&\\bigg f x + \\nabla^ T f x \\big - \\nabla^ 2 f x ^ -1 \\nabla f x \\big + \\frac 1 2 \\big - \\nabla^ 2 f x ^ -1 \\nabla f x \\big ^ T \\nabla ^ 2 f x \\big - \\nabla^ 2 f x ^ -1 \\nabla f x \\big \\bigg \\\\\\\\ >&= \\frac 1 2 \\nabla f x ^ T \\nabla^ 2 f x ^ -1 \\nabla f x = \\frac 1 2 \\lambda x ^ 2 . >\\end align MATH Thus, MATH can be considered an approximate bound for the suboptimality gap MATH . Second, the Newton direction in Newton's method for each iteration is MATH , and the Newton decrement is the length of the Newton step in the norm defined by the Hessian MATH . Alternatively, this can be viewed as a type of Mahalanobis distance Wikipedia https://en.wikipedia.org/wiki/Mahalanobis distance , where the new step MATH is the observation, the current position MATH is the mean, and the Hessian of MATH is the covariance. The Mahalanobis distance measures the distance from a point to the mean in the direction of the covariance of the distribution. If we consider the definition of Mahalanobis distance as the distance between a point and the mean of a distribution divided by the standard deviation in that direction, the Newton decrement represents the distance of the new step from the current position, with the Hessian serving as the covariance of the distribution. Third, the Newton decrement can be expressed in terms of the increment and the Hessian. Starting from the step update in Newton's method, we have: >\\begin align >x^ + &= x-\\big \\nabla^ 2 f x \\big ^ -1 \\nabla f x &\\\\ >\\end align >\\begin align >\\Delta x nt &= -\\big \\nabla^ 2 f x \\big ^ -1 \\nabla f x &\\\\ >\\end align >\\begin align >\\nabla f x ^ T \\Delta x nt &= -\\lambda x ^ 2 >\\end align Utilizing these relations, the Newton decrement can also be expressed as: >\\begin align >\\lambda x = \\Delta x nt ^ T \\nabla^ 2 f x \\Delta x nt ^ 1/2 . >\\end align Finally, like the Newton step, the Newton decrement is also affine invariant. In other words, for any nonsingular matrix, if the function MATH is defined, then at MATH , it holds that MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_03_newton_decrement/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_04_backtracking_line_search",
    "title": "14-04 Backtracking line search",
    "chapter": "14",
    "order": 8,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; So far, we have examined pure Newton's method. However, this method does not guarantee convergence, so we use backtracking line search to ensure convergence in the damped Newton's method. Damped Newton's method The pure Newton's method iteratively applies the following update here MATH : >\\begin align >x^ + = x -t \\nabla^ 2 f x ^ -1 \\nabla f x . >\\end align Damped Newton's method uses backtracking line search. If the function value at the updated position is greater than the quadratic approximation, we shrink the step size MATH : > MATH \\begin align >&\\text with parameters 0 &\\text while f x+tv >f x +\\alpha t \\nabla f x ^ T v\\\\ >&\\text shrink t=\\beta t >\\end align MATH Here, MATH and MATH . Example: logistic regression As an example, for logistic regression with n = 500, p = 100, we compare the convergence rate of gradient descent and Newton's method with backtracking line search. Fig 1 Logistic regression 3 Newton's method shows a much faster convergence rate than gradient descent. From the next chapter, we will examine this convergence rate.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_04_backtracking_line_search/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_05_convergence_analysis",
    "title": "14-05 Convergence analysis",
    "chapter": "14",
    "order": 9,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; So far, we have examined pure Newton's method, which only has local convergence properties, and damped Newton's method Newton's method with backtracking line search , which applies backtracking line search to ensure global convergence when convex. In this chapter, we analyze the convergence rate of damped Newton's method. For damped Newton's method, we examine convergence bounds divided into two phases: the phase where backtracking is applied damped phase: slow progress and the locally convergent phase where backtracking is no longer needed pure phase: quadratic convergence . Conditions of MATH for convergence analysis Assume that MATH is convex, twice differentiable, has MATH , and satisfies the following three conditions: 1. MATH is Lipschitz continuous with parameter L. > MATH \\begin align >\\|\\nabla f x - \\nabla f y \\| 2 \\leq L\\|x-y\\| 2 \\quad \\forall x,y. >\\end align MATH 2. MATH is strongly convex with parameter m. Relationship between upper bound MATH and Lipschitz continuous: source https://xingyuzhou.org/blog/notes/strong-convexity , this book: 06-03-05 % multilang post url contents/chapter06/21-03-20-06 03 05 look at the conditions and practicalities % > MATH \\begin align >mI\\preceq\\nabla^ 2 f x \\preceq LI. >\\end align MATH 3. MATH is Lipschitz continuous with parameter M. > MATH \\begin align >\\|\\nabla^ 2 f x -\\nabla^ 2 f y \\| 2 \\leq M\\|x-y\\| 2 \\quad \\forall x,y. >\\end align MATH Convergence analysis If the above three conditions are satisfied, for MATH satisfying MATH , the convergence for each phase can be obtained as follows: >Phase I : \"Damped\" phase, MATH , > > MATH \\begin align >f x^ k+1 -f x^ k \\leq -\\gamma >\\end align MATH > >Phase 2 : \"Pure\" phase, MATH \\|\\nabla f x^ k \\| 2 > MATH \\begin align >\\frac M 2m^ 2 \\|\\nabla f x^ k+1 \\| 2 \\leq \\bigg \\frac M 2m^ 2 \\|\\nabla f x^ k \\| 2 \\bigg ^ 2 . >\\end align MATH Note that once the Pure phase is reached when MATH \\begin align >\\require cancel >& &\\cancel f x^ 1 -f x^ 0 \\leq -\\gamma \\\\\\\\ >& &\\cancel f x^ 2 -\\cancel f x^ 1 \\leq -\\gamma \\\\\\\\ >& &\\vdots \\\\\\\\ >&+ &f x^ k -\\cancel f x^ k-1 \\leq -\\gamma \\\\\\\\ >&= &f x^ k -f x^ 0 \\leq -k\\gamma. >\\end align MATH Subtracting MATH from both sides, we can obtain the following result. Let MATH be the first MATH that satisfies MATH \\begin align >f x^ k -f^ \\star \\geq f x^ 0 -f^ \\star -\\gamma k \\qquad \\text if k \\geq k 0 >\\end align MATH For Phase 2, assume that iteration starts from MATH and proceeds for MATH steps. Also, using MATH \\begin align >& &\\frac M 2m^ 2 \\|\\nabla f^ k 0 +1 \\| 2 \\leq \\big \\frac M 2m^ 2 \\|\\nabla f^ k 0 \\| 2 \\big ^ 2 .\\\\\\\\ >&\\Leftrightarrow &\\frac M 2m^ 2 \\|\\nabla f^ k 0 + k-k 0 \\| 2 \\leq \\bigg \\big \\frac M 2m^ 2 \\|\\nabla f^ k 0 +1 \\| 2 \\big ^ 2 \\bigg ^ k-k 0 \\leq \\frac 1 2 ^ 2^ k-k 0 .\\\\\\\\ >&\\Leftrightarrow &f y \\geq f x +\\nabla f x ^ T y-x +\\frac m 2 \\|y-x\\|^ 2 2 \\geq f x -\\frac 1 2m \\|\\nabla f x \\|^ 2 2 , \\text for all y,\\\\\\\\ >&\\Leftrightarrow &f x^ k -f^ \\star \\leq \\frac 1 2m \\|\\nabla f x^ k \\| 2 ^ 2 \\leq \\frac 2m^ 3 M^ 2 \\frac 1 2 ^ 2^ k-k 0 +1 . >\\end align MATH Therefore, we can organize the convergence according to steps with the MATH -th iteration as the branch point as follows: >Theorem: Newton's method using backtracking line search has two-stage convergence bounds. > MATH \\begin align >&f x^ k -f^ \\star \\leq \\begin cases f x^ 0 -f^ \\star -\\gamma k \\qquad &\\text if k\\leq k 0 \\\\ \\frac 2m^ 3 M^ 2 \\frac 1 2 ^ 2^ k-k 0 +1 \\qquad &\\text if k>k 0 . \\end cases >\\end align MATH >Here, MATH , MATH , and MATH is the step where MATH y=x+t\\Delta x nt MATH f MATH and apply the upper bound of the Lipschitz condition. > > MATH \\begin align >f x+t\\Delta x nt \\leq f x +t\\nabla f x ^ T \\Delta x nt + \\frac L \\|\\Delta x nt \\|^ 2 2 2 t^ 2 , >\\end align MATH > Newton decrement, 증분과 hessian matrix와의 관계와 Strong convexity의 관계를 이용하여 다음과 같이 전개할 수 있다. > > MATH \\begin align >&\\text Since, \\lambda x ^ 2 =\\Delta x nt ^ T \\nabla^ 2 f x \\geq m\\|\\Delta x nt \\|^ 2 2 ,\\\\\\\\ >&f x +t\\nabla f x ^ T \\Delta x nt + \\frac L \\|\\Delta x nt \\|^ 2 2 2 t^ 2 \\leq f x -t\\lambda x ^ 2 + \\frac L 2m t^ 2 \\lambda x ^ 2 , >\\end align MATH > >이 때, backtracking line search의 조건을 만족하기 위해서는 아래를 만족해야 한다. > > MATH \\begin align >f x+t\\Delta x nt \\leq f x - 1-\\frac L 2m t t \\lambda x ^ 2 , \\qquad \\text where, 0 \\end align MATH > >위를 만족하는 t의 최소값을 MATH 라 할 때, MATH 이 되고, 이를 원 식에 대입하면 다음과 같다. > > MATH \\begin align >f x+\\hat t \\Delta x nt \\leq f x -\\frac m 2L \\lambda x ^ 2 \\leq f x -\\alpha \\hat t \\lambda x ^ 2 , >\\end align MATH > >backtracking line search에서 MATH 0 > MATH \\begin align >f x^ + -f x &\\leq -\\alpha t \\lambda x ^ 2 \\\\ > &\\leq -\\alpha\\beta \\frac m L \\lambda x ^ 2 \\\\ > &\\leq -\\alpha\\beta \\frac m L^ 2 \\|\\nabla f x \\|^ 2 2 \\\\ > &\\leq -\\alpha\\beta \\eta^ 2 \\frac m L^ 2 ,\\\\ > &\\gamma = \\alpha\\beta \\eta^ 2 \\frac m L^ 2 . >\\end align MATH Proof 2. Pure phase 이제 MATH \\|\\nabla f x \\| 2 Backtracking line search로 부터 다음과 같은 식이 유도된다. > > MATH \\begin align >\\eta \\leq 3 1-2\\alpha \\frac m^ 2 M . >\\end align MATH > >또한, Lipschitz condition에 따라 MATH 에 대하여, 다음 조건을 만족한다. > > MATH \\begin align >\\|\\nabla^ 2 f x+t\\Delta x nt -\\nabla^ 2 f x \\| 2 \\leq tM \\|\\Delta x nt \\| 2 ,\\\\ >| \\Delta x nt ^ T \\big \\nabla^ 2 f x+t\\Delta x nt -\\nabla^ 2 f x \\big \\Delta x nt | \\leq tM \\|\\Delta x nt \\| 2 ^ 3 . >\\end align MATH > > MATH 라 두면, MATH 이고, 이를 대입한다. > > MATH \\begin align >\\tilde f '' t \\leq \\tilde f '' 0 +tM\\|\\Delta x nt \\|^ 3 2 \\leq tM\\|\\Delta x nt \\|^ 3 2 >\\end align MATH > > MATH 이고, MATH 임을 이용하고, 부등식을 합친다. MATH 이므로 다음과 같이 정리할 수 있다. > > MATH \\begin align >\\tilde f '' t &\\leq \\tilde f '' 0 + tM \\| \\Delta x nt \\| ^ 3 2 \\leq \\lambda x ^ 2 + t\\frac M m^ 3/2 \\lambda x ^ 3 , \\\\ >\\tilde f ' t &\\leq \\tilde f ' 0 +t\\lambda x ^ 2 +t^ 2 \\frac M 2m^ 3/2 \\lambda x ^ 3 ,\\\\ >&= -\\lambda x ^ 2 +t\\lambda x ^ 2 + t^ 2 \\frac L 2m^ 3/2 \\lambda x ^ 3 . >\\end align MATH > >이제 양변을 적분한다. > > MATH \\begin align >\\tilde f t \\leq \\tilde f 0 - t\\lambda x ^ 2 + t^ 2 \\frac 1 2 \\lambda x ^ 2 + t^ 3 \\frac M 6m^ 3/2 \\lambda x ^ 3 . >\\end align MATH > >t = 1로 두면, 아래와 같은 결과를 얻을 수 있다. > > MATH \\begin align >f x+\\Delta x nt \\leq f x -\\frac 1 2 \\lambda x ^ 2 + \\frac M 6m^ 3/2 \\lambda x ^ 3 . >\\end align MATH > >이제 MATH 이라 가정하면, strong convexity 조건에 의해 MATH 이다. 이를 위에 부등식에 대입하면 아래와 같은 결과를 유도할 수 있다. > > MATH \\begin align >f x+\\Delta x nt &\\leq f x - \\lambda x ^ 2 \\frac 1 2 - \\frac M\\lambda x 6m^ 3/2 \\\\ > &\\leq f x -\\alpha \\lambda x ^ 2 \\\\ > &= f x + \\alpha \\nabla f x ^ T \\Delta x nt , >\\end align MATH > >이 결과는 MATH 일때 backtracking line search를 수행하더라도 항상 조건을 만족하기 때문에, MATH 를 감소시키지 않음을 의미한다. 이제 우리는 수렴속도가 quadratic하게 줄어듬을 증명해본다. > MATH 임을 이용한 뒤, 적분의 성질 중 하나인 MATH 를 이용하여 정리하고, Hessian의 Lipschitz 조건을 적분식에 적용하고 정리한다. 마지막으로 strong convexity 조건을 적용하면 증명이 완료된다. 과정을 수식으로 나타내면 아래와 같다. > > MATH \\begin align >\\| \\nabla f x^ + \\| 2 &= \\| \\nabla f x+\\Delta x nt - \\nabla f x - \\nabla^ 2 f x \\Delta x nt \\| 2 \\\\\\\\ >&=\\| \\int^ 1 0 \\big \\nabla^ 2 f x+t\\Delta x nt -\\nabla^ 2 f x \\big \\Delta x nt dt \\| 2 \\\\\\\\ > & \\leq \\frac M 2 \\|\\Delta x nt \\|^ 2 2 \\\\\\\\ > & = \\frac M 2 \\|\\nabla^ 2 f x ^ -1 \\nabla f x \\|^ 2 2 \\\\\\\\ > & \\leq \\frac M 2m^ 2 \\|\\nabla f x \\|^ 2 2 . >\\end align MATH 결론을 다시 정리하면, MATH 일 때, MATH \\begin align >\\frac f x^ 0 -p^ \\star \\gamma . >\\end align MATH pure Newton phase에서의 iteration 횟수의 bound 또한 계산할 수 있다. 위의 식을 MATH , MATH 로 두고, iteration 횟수로 식을 정리하면 다음과 같은 값을 계산할 수 있다. > MATH \\begin align >& &\\epsilon = \\epsilon 0 \\frac 1 2 ^ 2^ k-k 0 +1 \\\\\\\\ >&\\Leftrightarrow &\\frac \\epsilon 0 \\epsilon = 2^ 2^ k-k 0 +1 \\\\\\\\ >&\\Leftrightarrow &k-k 0 +1 = log 2 log 2 \\frac \\epsilon 0 \\epsilon >\\end align MATH 따라서 pure Newton phase에서 iteration 횟수는 MATH 로 bound 된다. 이 두 결과를 더하면, Newton method를 통하여 원하는 정밀도의 해를 얻는데 필요한 iteration 횟수의 upper bound를 정의할 수 있다. > MATH \\begin align >\\frac f x^ 0 -p^ \\star \\gamma + \\log \\log \\frac \\epsilon 0 \\epsilon . >\\end align MATH 문제를 해결할때 요구되는 정밀도 MATH 의 변화에 비해 우변의 두번째 항은 매우 작은 변화를 보이므로, 실제 응용에서는 이를 상수로 두고 추정을 하게 된다. 일반적으로 6번의 iteration은 MATH 의 정밀도를 보인다고 알려져 있다. 일반적으로 말해서, 목적함수 MATH 를 최소화하는데 있어서 필요한 iteration 횟수는 다음과 같다. > MATH \\begin align >\\frac f x^ 0 -p^ \\star \\gamma + 6. >\\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_05_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_06_self_concordance",
    "title": "14-06 Self concordance",
    "chapter": "14",
    "order": 10,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In the previous convergence analysis of Newton's method, there are two major issues. 1 First, in practical problems, it is difficult to find the Lipschitz constant L, the lower and upper bounds of strong convexity m, M, etc., which are included in the formulas. Because of this, while we can observe convergence and convergence rate, it is almost impossible to analyze how many Newton steps are needed in practice. Second, although Newton's method itself is affine invariant, the convergence analysis of Newton's method is not affine invariant. For general functions, the values of the Lipschitz constant or strong convexity bounds change depending on the coordinate transformation. Therefore, in this chapter, we introduce self-concordant functions, which address the above two issues. Self-concordant functions are important and meaningful for three main reasons: 1. The log barrier functions used in interior-point methods are self-concordant functions. 2. In the analysis of Newton's method for self-concordant functions, terms involving constants do not appear. 3. Self-concordance is affine-invariant. That is, the number of Newton iterations required is independent of affine transformations of the coordinate system.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_06_self_concordance/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_06_01_definition_of_self_concordant_functions",
    "title": "14-06-01 Definition of self-concordant functions",
    "chapter": "14",
    "order": 11,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; Self-concordant on MATH A convex function MATH is defined as self-concordant when it satisfies the following equation: > MATH \\begin align >\\|f^ ''' x \\| \\leq 2f^ '' x ^ 3/2 \\qquad \\text for all x\\in \\text dom f. >\\end align MATH As simple examples, linear functions MATH and convex quadratic functions are self-concordant because their third derivative values are 0. Self-concordant on MATH A function MATH is defined as self-concordant when it is self-concordant for any line segment within the domain, that is, for all line segments included in the domain. For example, for all MATH and all MATH , when MATH is defined, if MATH is self-concordant, then f is defined as a self-concordant function in the domain of MATH . Example of self-concordance function 1 MATH , MATH . It can be easily verified that MATH . Furthermore, the sum of self-concordant functions is also self-concordant. When there are self-concordant functions MATH , the sum of self-concordant functions is also self-concordant as shown below. 3 > MATH \\begin align >|f 1 ^ ''' x +f 2 ^ ''' x | \\leq & |f^ ''' 1 x |+|f^ ''' 2 x |\\\\\\\\ > \\leq &2\\big f^ '' 1 x ^ 3/2 +f^ '' 2 x ^ 3/2 \\big \\\\\\\\ >\\leq &2\\big f^ '' 1 x +f^ '' 2 x \\big ^ 3/2 . >\\end align MATH The last step uses the following property: > MATH \\begin align > u^ 3/2 +v^ 3/2 ^ 2/3 \\leq u+v, \\qquad u, v \\geq 0. >\\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_06_01_definition_of_self_concordant_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_06_02_convergence_analysis_for_self_concordant_functions",
    "title": "14-06-02 Convergence analysis for self-concordant functions",
    "chapter": "14",
    "order": 12,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "For self-concordant functions, the result of convergence analysis is as follows: >Theorem Nesterov and Nemirovskii : Newton's method with backtracking line search requires the following number of iterations to achieve MATH : >\\begin align >C \\alpha, \\beta \\big f x^ 0 -f^ \\star \\big + \\log\\log 1/\\epsilon , >\\end align >where MATH is a constant depending on MATH . The proof above is similar to the convergence analysis for Newton's method, but uses the properties of self-concordant functions to organize the steps. See 1 , p.503",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_06_02_convergence_analysis_for_self_concordant_functions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_07_comparison_to_first_order_method",
    "title": "14-07 Comparison to first-order method",
    "chapter": "14",
    "order": 13,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In this chapter, we compare Newton's method and gradient descent from a general perspective. Let the dimension of the domain be MATH . | Item | Newton's method | Gradient descent | | -------- | -------- | -------- | | Memory | MATH storage for MATH Hessian matrix | MATH storage for MATH -dimensional gradient | | Computation | MATH flops computation for MATH linear system | MATH flops computation for MATH -dimensional vector addition | | Backtracking | MATH | MATH | | Conditioning | Affine invariant, less affected by conditioning | Can be strongly affected by conditioning | | Fragility | Sensitive to bugs or numerical errors | More robust than Newton's method | Example Fig 1 Logistic regression 3 Figure 1 above is a logistic regression example discussed in 14-04 % multilang post url contents/chapter14/2021-03-26-14 04 backtracking line search % . If you plot the x-axis as actual computation time, you see the following. In convergence analysis, Newton's method has two phases. In practice, after a certain time, you can observe fast convergence quadratic convergence . In the initial damped phase of Newton's method, the convergence rate is similar to gradient descent. However, since MATH computation is required, the actual computation time may be slower. After backtracking line search is no longer needed, you observe quadratic convergence and very fast progress.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_07_comparison_to_first_order_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_08_special_cases",
    "title": "14-08 Special cases",
    "chapter": "14",
    "order": 14,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; Sparse, structured problems If the linear system matrix in the problem is sparse and structured, and the Hessian can be efficiently computed, we can solve the problem more efficiently. For example, if MATH is sparse and structured for all MATH , such as a band matrix https://en.wikipedia.org/wiki/Band matrix , Newton's method can achieve MATH performance in memory and computation. A band matrix is a matrix where nonzero entries are only near the diagonal. Let's look at two typical examples of functions with structured Hessians: If MATH , then MATH . If MATH is a structured predictor matrix and MATH the Hessian of MATH is diagonal, then MATH is also structured. If MATH is diagonal and MATH is non-smooth, consider minimizing MATH , where MATH is a structured penalty matrix. The Lagrange dual is MATH . In general, MATH is also structured. Equality-constrained Newton's method Now let's look at optimization problems with equality constraints. Generally, we can approach this problem in three ways: > MATH \\begin align >&\\min x f x & \\text subject to Ax=b. >\\end align MATH 1 Reduced-space approach: Restrict the domain to the space satisfying the equality constraint. For the above problem, express MATH as MATH , where MATH spans the null space of MATH and MATH . Then solve for MATH . 2 Equality-constrained Newton's method: Similar to unconstrained Newton's method, but with two differences. First, the initial value must be feasible MATH and MATH . Second, the Newton step MATH must satisfy MATH . See below for details. 3 Dual approach: The Fenchel dual is MATH , and strong duality holds. 16-03 % multilang post url contents/chapter16/21-03-31-16 03 fenchel duality % covers this in detail. Use the conjugate function to solve the dual problem. Here, MATH is the conjugate of MATH . > MATH \\begin align >g v &= -b^ T v + \\min x f x +v^ T Ax \\\\\\\\ > &= -b^ T v - \\max x \\big -A^ T v ^ T x - f x \\big \\\\\\\\ > &= -b^ T v - f^ -A^ T v , >\\end align MATH This leads to the following dual problem: > MATH \\begin align >\\max -b^ T v-f^ -A^ T v . >\\end align MATH Assuming the optimal value exists, this problem is strictly feasible and satisfies Slater's condition. Therefore, as mentioned earlier, strong duality holds, and there exists a MATH such that MATH . 1, p.525 Now, let's examine the second method. To derive a feasible Newton step MATH , we replace the objective function in the original problem with a quadratic approximation around MATH . This can be expressed as: > MATH \\begin align >\\text minimize \\quad &\\hat f x+v = f x + \\nabla f x ^ T v + \\frac 1 2 v^ T \\nabla^ 2 f x v\\\\\\\\ >\\text subject to \\quad &A x+v = b, >\\end align MATH This can also be expressed as: > MATH \\begin align >x^ + = x + tv,\\,\\, \\text where \\\\\\\\ >v = \\underset A x+z =b \\operatorname argmin \\big f x +\\nabla f x ^ T z+\\frac 1 2 z^ T \\nabla^ 2 f x z \\big \\\\\\\\ >\\end align MATH Since MATH , the solution MATH remains within the constraint in the subsequent steps of the iteration. The KKT conditions for this problem can be expressed as follows, and by solving the linear system below, we can obtain the solution. Recall that MATH is the Newton step MATH . > MATH \\begin align >\\begin bmatrix > \\nabla^ 2 f x & A^ T \\\\\\\\ > A & 0 >\\end bmatrix >\\begin bmatrix >v\\\\\\\\ >w >\\end bmatrix >=- >\\begin bmatrix >\\nabla f x \\\\\\\\ >Ax-b >\\end bmatrix >\\end align MATH Here, MATH is the optimal dual variable for the above quadratic problem.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_08_special_cases/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter14/14_09_quasi_newton_methods",
    "title": "14-09 Quasi-Newton methods",
    "chapter": "14",
    "order": 15,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; If the computation of the Hessian is too expensive or the Hessian is singular, we use quasi-Newton methods to approximate the Hessian matrix, i.e., MATH is replaced by MATH , and we use MATH for the update: >\\begin align >x^ + = x - tH^ -1 \\nabla f x >\\end align Some features of quasi-Newton methods are as follows. For more details, see Chapter 18 % multilang post url contents/chapter18/21-03-23-18 00 Quasi Newton methods % . The approximate Hessian MATH is updated at each step. The goal is to use a relatively cheap computation for MATH . The convergence rate is superlinear, but not as fast as Newton's method. Typically, MATH steps of quasi-Newton are equivalent to one step of Newton in terms of progress. Many quasi-Newton methods update MATH at each iteration using propagation techniques.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_09_quasi_newton_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_barrier_method",
    "title": "15 Barrier Method",
    "chapter": "15",
    "order": 1,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this chapter, we will look at one of the 2nd-order methods, the Barrier Method . The Barrier Method is a technique for solving inequality constraint and equality constrained smooth problems , which are considered the most difficult among 2nd-order method problems. For reference, using the gradient to solve an optimization problem is called a 1st-order method, and using the Hessian is called a 2nd-order method. References and further readings S. Boyd and L. Vandenberghe 2004 , \"Convex optimization\", Chapter 11 A. Nemirovski 2004 , \"Interior-point polynomial time methods in convex programming\", Chapter 4 J. Nocedal and S. Wright 2006 , \"Numerical optimization\", Chapters 14 and 19",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_barrier_method",
    "title": "15 Barrier Method",
    "chapter": "15",
    "order": 1,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this chapter, we will look at one of the 2nd-order methods, the Barrier Method . The Barrier Method is a technique for solving inequality constraint and equality constrained smooth problems , which are considered the most difficult among 2nd-order method problems. For reference, using the gradient to solve an optimization problem is called a 1st-order method, and using the Hessian is called a 2nd-order method. References and further readings S. Boyd and L. Vandenberghe 2004 , \"Convex optimization\", Chapter 11 A. Nemirovski 2004 , \"Interior-point polynomial time methods in convex programming\", Chapter 4 J. Nocedal and S. Wright 2006 , \"Numerical optimization\", Chapters 14 and 19",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_barrier_method",
    "title": "15 Barrier Method",
    "chapter": "15",
    "order": 1,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this chapter, we will look at one of the 2nd-order methods, the Barrier Method . The Barrier Method is a technique for solving inequality constraint and equality constrained smooth problems , which are considered the most difficult among 2nd-order method problems. For reference, using the gradient to solve an optimization problem is called a 1st-order method, and using the Hessian is called a 2nd-order method. References and further readings S. Boyd and L. Vandenberghe 2004 , \"Convex optimization\", Chapter 11 A. Nemirovski 2004 , \"Interior-point polynomial time methods in convex programming\", Chapter 4 J. Nocedal and S. Wright 2006 , \"Numerical optimization\", Chapters 14 and 19",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_barrier_method",
    "title": "15 Barrier Method",
    "chapter": "15",
    "order": 1,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this chapter, we will look at one of the 2nd-order methods, the Barrier Method . The Barrier Method is a technique for solving inequality constraint and equality constrained smooth problems , which are considered the most difficult among 2nd-order method problems. For reference, using the gradient to solve an optimization problem is called a 1st-order method, and using the Hessian is called a 2nd-order method. References and further readings S. Boyd and L. Vandenberghe 2004 , \"Convex optimization\", Chapter 11 A. Nemirovski 2004 , \"Interior-point polynomial time methods in convex programming\", Chapter 4 J. Nocedal and S. Wright 2006 , \"Numerical optimization\", Chapters 14 and 19",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/15_01_barrier_method_and_log_barrier_function",
    "title": "15-01 Barrier method and log barrier function",
    "chapter": "15",
    "order": 2,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In the previous chapter, we explored how to solve an equality constrained smooth problem using Newton's method. In this chapter, we will look at methods for solving inequality and equality constrained smooth problems . The basic idea is to transform the problem into an equality constrained smooth problem and solve it using Newton's method. This approach is called the interior method , and in this chapter, we will focus on one type of interior method: the barrier method .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/15_01_barrier_method_and_log_barrier_function/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/15_01_01_inequality_constrained_minimization_problems",
    "title": "15-01-01 Inequality constrained minimization problems",
    "chapter": "15",
    "order": 3,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; Let's consider the following convex optimization problem. > MATH \\begin align &\\min x && f x \\\\ &\\text subject to && Ax = b \\\\ &&& h i x \\leq 0, i = 1, \\dotsc, m \\end align MATH In problems that include inequalities like this, it is difficult to distinguish between binding and non-binding constraints, especially at the boundary of the feasible region. Binding constraints refer to the constraints that affect the solution. Therefore, the interior method is an approach that tries to solve the problem from the interior of the feasible region, not at the boundary. Background of interior method The interior method for general problems was proposed in the 1960s by Anthony V. Fiacco and Garth P. McCormick. At the time, it was overshadowed by popular methods like sequential quadratic programming and the active set method, and only gained attention in the 1980s. The active set method is a theory for determining which constraints affect the optimization result. In the active set method, a constraint is considered active if it is zero, and such constraints are called the active set. However, to find the active set, you need to compute the boundary of the feasible region, and as the number of constraints increases, the computational cost increases. Recognizing these issues, the interior point method was developed to solve problems from the interior rather than the boundary. For example, in LP, if there are MATH constraints, calculating the boundary requires MATH computations, but with the interior method, even for large MATH , the solution can be found within 20–30 Newton steps. More details on performance will be discussed later. Reference: Interior point method https://en.wikipedia.org/wiki/Interior-point method Reference: Active set method https://en.wikipedia.org/wiki/Active set method Reducing equality constrained minimization problem The above problem can be rewritten as MATH . Inequality constraints can be included in the objective function as an indicator function. > MATH \\begin align &\\min x \\ && f x + I C x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH In this way, the problem can be transformed into an equality constrained minimization problem. However, since the indicator function still includes the boundary, it still has the difficulty of boundary computation from the original problem, and since it is not differentiable, it is difficult to apply Newton's method. What if we approximate the indicator function MATH with a barrier function ? In that case, the boundary would not be included and since it is differentiable, Newton's method can be applied. The method of solving problems redefined with barrier functions in this way is called the barrier method, which is introduced in detail in the next section.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/15_01_01_inequality_constrained_minimization_problems/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/15_01_02_log_barrier_function_and_barrier_method",
    "title": "15-01-02 Log barrier function & barrier method",
    "chapter": "15",
    "order": 4,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; Before introducing the barrier method, let's first see how the indicator function can be approximated by a barrier function. Approximation of indicator function In the following figure, you can see the indicator function and the barrier function. The dotted line is the indicator function MATH , and the solid lines are the barrier function MATH for MATH . The barrier function smoothly approximates the indicator function, and when MATH , it provides the best approximation. MATH Logarithmic barrier function Suppose MATH are convex and twice differentiable. For the set MATH , the following function is called the logarithmic barrier function. > \\begin align \\phi x = - \\sum i=1 ^ m \\log -h i x \\end align Here, the set is assumed to be the interior of the feasible set MATH and is non-empty. Barrier method Using the barrier function, the original problem can be approximated as follows. Here, MATH . > MATH \\begin align &\\min x && f x + \\frac 1 t \\phi x & \\qquad & \\min x && tf x + \\phi x \\\\ &\\text subject to && Ax = b & \\iff \\qquad & \\text subject to && Ax = b \\\\ \\end align MATH The method of solving the problem defined in this way using Newton's method is called the barrier method .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/15_01_02_log_barrier_function_and_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/15_01_03_log_barrier_calculus",
    "title": "15-01-03 Log barrier calculus",
    "chapter": "15",
    "order": 5,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "The gradient and Hessian of the log barrier function are as follows. > \\begin align \\phi x = - \\sum i=1 ^ m \\log -h i x \\end align Gradient: > \\begin align \\nabla \\phi x = - \\sum i=1 ^ m \\frac 1 h i x \\nabla h i x \\end align Hessian: > \\begin align \\nabla^2 \\phi x = \\sum i=1 ^ m \\frac 1 h i x ^2 \\nabla h i x \\nabla h i x ^T - \\sum i=1 ^ m \\frac 1 h i x \\nabla^2 h i x \\end align Example: MATH If we compute the gradient and Hessian for the barrier function MATH , we get the following results. > \\begin align \\phi x = -\\sum i=1 ^ n \\log x i \\end align Therefore, MATH and MATH . Gradient: > MATH \\nabla \\phi x = - \\begin bmatrix 1/x 1 \\\\\\ \\vdots \\\\\\ 1/x n \\\\\\ \\end bmatrix = -X^ -1 \\mathbb 1 , \\qquad X = \\text diag x MATH Hessian : > MATH \\nabla^2 \\phi x = \\begin bmatrix 1/x 1^2 & \\cdots & \\\\\\ \\vdots & \\ddots & \\vdots \\\\\\ & \\cdots & 1/x n^2 \\\\\\ \\end bmatrix = X^ -2 MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/15_01_03_log_barrier_calculus/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/15_01_03_log_barrier_calculus",
    "title": "15-01-03 Log barrier calculus",
    "chapter": "15",
    "order": 5,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "The gradient and Hessian of the log barrier function are as follows. > \\begin align \\phi x = - \\sum i=1 ^ m \\log -h i x \\end align Gradient: > \\begin align \\nabla \\phi x = - \\sum i=1 ^ m \\frac 1 h i x \\nabla h i x \\end align Hessian: > \\begin align \\nabla^2 \\phi x = \\sum i=1 ^ m \\frac 1 h i x ^2 \\nabla h i x \\nabla h i x ^T - \\sum i=1 ^ m \\frac 1 h i x \\nabla^2 h i x \\end align Example: MATH If we compute the gradient and Hessian for the barrier function MATH , we get the following results. > \\begin align \\phi x = -\\sum i=1 ^ n \\log x i \\end align Therefore, MATH and MATH . Gradient: > MATH \\nabla \\phi x = - \\begin bmatrix 1/x 1 \\\\\\ \\vdots \\\\\\ 1/x n \\\\\\ \\end bmatrix = -X^ -1 \\mathbb 1 , \\qquad X = \\text diag x MATH Hessian : > MATH \\nabla^2 \\phi x = \\begin bmatrix 1/x 1^2 & \\cdots & \\\\\\ \\vdots & \\ddots & \\vdots \\\\\\ & \\cdots & 1/x n^2 \\\\\\ \\end bmatrix = X^ -2 MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/15_01_03_log_barrier_calculus/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/15_02_central_path",
    "title": "15-02 Central path",
    "chapter": "15",
    "order": 6,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "If we denote the solution to the following barrier problem MATH as MATH , then the central path is the set MATH . > MATH \\begin align > &\\min x \\ && tf x + \\phi x \\\\ > &\\text subject to \\ && Ax = b \\\\ > \\end align MATH Given suitable conditions, the central path set forms a smooth path in MATH , and as MATH , MATH , where MATH is the solution to the original problem. The central path is a set of solutions obtained by gradually redefining the problem for new values of MATH , moving from the interior toward the boundary when it is difficult to find the optimal solution at the boundary directly. Example: central path for an LP Let's find the central path for the following LP problem. > MATH \\begin align >&\\min x \\ && c^Tx \\\\ >&\\text subject to \\ && a i^Tx \\le b i^T, i = 1, \\cdots , 6 \\\\ >\\end align MATH In the following figure, the dotted line represents the logarithmic barrier function MATH . Fig 1 Central path 1 You can see that as MATH , the central path converges to the optimal MATH . At this point, the hyperplane MATH is the tangent to the level curve of MATH passing through MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/15_02_central_path/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_03_properties_and_interpretation",
    "title": "15-03 Properties and interpretation",
    "chapter": "15",
    "order": 7,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this section, we will derive the KKT conditions for the barrier problem and the original problem to see what differences exist. We will also calculate the suboptimality gap for the solutions to both problems.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_03_properties_and_interpretation/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_03_properties_and_interpretation",
    "title": "15-03 Properties and interpretation",
    "chapter": "15",
    "order": 7,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this section, we will derive the KKT conditions for the barrier problem and the original problem to see what differences exist. We will also calculate the suboptimality gap for the solutions to both problems.",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_03_properties_and_interpretation/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_03_properties_and_interpretation",
    "title": "15-03 Properties and interpretation",
    "chapter": "15",
    "order": 7,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this section, we will derive the KKT conditions for the barrier problem and the original problem to see what differences exist. We will also calculate the suboptimality gap for the solutions to both problems.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_03_properties_and_interpretation/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_03_properties_and_interpretation",
    "title": "15-03 Properties and interpretation",
    "chapter": "15",
    "order": 7,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this section, we will derive the KKT conditions for the barrier problem and the original problem to see what differences exist. We will also calculate the suboptimality gap for the solutions to both problems.",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_03_properties_and_interpretation/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_03_01_perturbed_kkt_conditions",
    "title": "15-03-01 Perturbed KKT conditions",
    "chapter": "15",
    "order": 8,
    "owner": "",
    "lesson_type": "",
    "content": "If we derive the KKT conditions from the barrier problem and the original formulation, we get the following. KKT conditions for barrier problem The second term in the KKT conditions for the barrier problem is derived using the gradient of the log barrier function. > MATH \\begin align t \\nabla f x^ t - \\sum i=1 ^ m \\frac 1 h i x^ t \\nabla h i x^ t + A^Tw = 0 \\\\\\ Ax^ t = b, \\quad h i x^ t \\lt 0, \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH KKT conditions for the original problem In the KKT conditions for the original problem, complementary slackness gives MATH , but in practice, it is very difficult to know this boundary condition. > MATH \\begin align \\nabla f x^ + \\sum i=1 ^ m u i^ \\nabla h i x^ + A^Tv^ = 0 \\\\\\ Ax^ = b, \\quad h i x^ \\le 0, \\quad u i^ \\ge 0, \\\\\\ h i x^ \\cdot u i^ = 0, \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH Redefinition of KKT conditions for barrier problem So, what is the relationship between these two KKT conditions? First, let MATH and MATH be defined as follows: > MATH \\begin align u i t = - \\frac 1 t h i x^ t , \\quad v = \\frac 1 t w \\end align MATH Let's redefine the KKT conditions for the barrier problem. Looking at the redefined problem, we see that the KKT conditions are almost identical to those for the original problem. In this equation, MATH becomes 0 as MATH , which matches MATH in the original formulation. > MATH \\begin align & \\nabla f x^ t + \\sum i=1 ^ m u i t \\nabla h i x^ t + tA^Tv = 0 \\\\\\ & Ax^ t = b, \\quad u i t \\cdot h i x^ t = - \\frac 1 t , \\quad h i x^ t \\lt 0, \\quad u i t \\gt 0 , \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_03_01_perturbed_kkt_conditions/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_03_01_perturbed_kkt_conditions",
    "title": "15-03-01 Perturbed KKT conditions",
    "chapter": "15",
    "order": 8,
    "owner": "",
    "lesson_type": "",
    "content": "If we derive the KKT conditions from the barrier problem and the original formulation, we get the following. KKT conditions for barrier problem The second term in the KKT conditions for the barrier problem is derived using the gradient of the log barrier function. > MATH \\begin align t \\nabla f x^ t - \\sum i=1 ^ m \\frac 1 h i x^ t \\nabla h i x^ t + A^Tw = 0 \\\\\\ Ax^ t = b, \\quad h i x^ t \\lt 0, \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH KKT conditions for the original problem In the KKT conditions for the original problem, complementary slackness gives MATH , but in practice, it is very difficult to know this boundary condition. > MATH \\begin align \\nabla f x^ + \\sum i=1 ^ m u i^ \\nabla h i x^ + A^Tv^ = 0 \\\\\\ Ax^ = b, \\quad h i x^ \\le 0, \\quad u i^ \\ge 0, \\\\\\ h i x^ \\cdot u i^ = 0, \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH Redefinition of KKT conditions for barrier problem So, what is the relationship between these two KKT conditions? First, let MATH and MATH be defined as follows: > MATH \\begin align u i t = - \\frac 1 t h i x^ t , \\quad v = \\frac 1 t w \\end align MATH Let's redefine the KKT conditions for the barrier problem. Looking at the redefined problem, we see that the KKT conditions are almost identical to those for the original problem. In this equation, MATH becomes 0 as MATH , which matches MATH in the original formulation. > MATH \\begin align & \\nabla f x^ t + \\sum i=1 ^ m u i t \\nabla h i x^ t + tA^Tv = 0 \\\\\\ & Ax^ t = b, \\quad u i t \\cdot h i x^ t = - \\frac 1 t , \\quad h i x^ t \\lt 0, \\quad u i t \\gt 0 , \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_03_01_perturbed_kkt_conditions/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_03_01_perturbed_kkt_conditions",
    "title": "15-03-01 Perturbed KKT conditions",
    "chapter": "15",
    "order": 8,
    "owner": "",
    "lesson_type": "",
    "content": "If we derive the KKT conditions from the barrier problem and the original formulation, we get the following. KKT conditions for barrier problem The second term in the KKT conditions for the barrier problem is derived using the gradient of the log barrier function. > MATH \\begin align t \\nabla f x^ t - \\sum i=1 ^ m \\frac 1 h i x^ t \\nabla h i x^ t + A^Tw = 0 \\\\\\ Ax^ t = b, \\quad h i x^ t \\lt 0, \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH KKT conditions for the original problem In the KKT conditions for the original problem, complementary slackness gives MATH , but in practice, it is very difficult to know this boundary condition. > MATH \\begin align \\nabla f x^ + \\sum i=1 ^ m u i^ \\nabla h i x^ + A^Tv^ = 0 \\\\\\ Ax^ = b, \\quad h i x^ \\le 0, \\quad u i^ \\ge 0, \\\\\\ h i x^ \\cdot u i^ = 0, \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH Redefinition of KKT conditions for barrier problem So, what is the relationship between these two KKT conditions? First, let MATH and MATH be defined as follows: > MATH \\begin align u i t = - \\frac 1 t h i x^ t , \\quad v = \\frac 1 t w \\end align MATH Let's redefine the KKT conditions for the barrier problem. Looking at the redefined problem, we see that the KKT conditions are almost identical to those for the original problem. In this equation, MATH becomes 0 as MATH , which matches MATH in the original formulation. > MATH \\begin align & \\nabla f x^ t + \\sum i=1 ^ m u i t \\nabla h i x^ t + tA^Tv = 0 \\\\\\ & Ax^ t = b, \\quad u i t \\cdot h i x^ t = - \\frac 1 t , \\quad h i x^ t \\lt 0, \\quad u i t \\gt 0 , \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_03_01_perturbed_kkt_conditions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_03_01_perturbed_kkt_conditions",
    "title": "15-03-01 Perturbed KKT conditions",
    "chapter": "15",
    "order": 8,
    "owner": "",
    "lesson_type": "",
    "content": "If we derive the KKT conditions from the barrier problem and the original formulation, we get the following. KKT conditions for barrier problem The second term in the KKT conditions for the barrier problem is derived using the gradient of the log barrier function. > MATH \\begin align t \\nabla f x^ t - \\sum i=1 ^ m \\frac 1 h i x^ t \\nabla h i x^ t + A^Tw = 0 \\\\\\ Ax^ t = b, \\quad h i x^ t \\lt 0, \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH KKT conditions for the original problem In the KKT conditions for the original problem, complementary slackness gives MATH , but in practice, it is very difficult to know this boundary condition. > MATH \\begin align \\nabla f x^ + \\sum i=1 ^ m u i^ \\nabla h i x^ + A^Tv^ = 0 \\\\\\ Ax^ = b, \\quad h i x^ \\le 0, \\quad u i^ \\ge 0, \\\\\\ h i x^ \\cdot u i^ = 0, \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH Redefinition of KKT conditions for barrier problem So, what is the relationship between these two KKT conditions? First, let MATH and MATH be defined as follows: > MATH \\begin align u i t = - \\frac 1 t h i x^ t , \\quad v = \\frac 1 t w \\end align MATH Let's redefine the KKT conditions for the barrier problem. Looking at the redefined problem, we see that the KKT conditions are almost identical to those for the original problem. In this equation, MATH becomes 0 as MATH , which matches MATH in the original formulation. > MATH \\begin align & \\nabla f x^ t + \\sum i=1 ^ m u i t \\nabla h i x^ t + tA^Tv = 0 \\\\\\ & Ax^ t = b, \\quad u i t \\cdot h i x^ t = - \\frac 1 t , \\quad h i x^ t \\lt 0, \\quad u i t \\gt 0 , \\quad i = 1, \\cdots , m \\\\\\ \\end align MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_03_01_perturbed_kkt_conditions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_04_barrier_method_v0_and_v1",
    "title": "15-04 Barrier method v.0 and v.1",
    "chapter": "15",
    "order": 10,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Barrier method v.0 Barrier method v.0 chooses MATH for MATH and solves the following barrier problem to obtain MATH . > MATH \\begin align &\\min x \\ && tf x + \\phi x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH Here, MATH is the number of constraints and MATH is a multiple of MATH , so as MATH gets smaller, MATH becomes very large, and eventually the end of the central path is reached, making the problem equivalent to the original problem. Therefore, this can be very slow and difficult to solve. Thus, a better approach is to follow the central path to find the solution, which leads to the definition of barrier method v.1 . Barrier method v.1 Barrier method v.1 is a method that gradually increases the value of MATH and solves the following barrier problem multiple times. > MATH \\begin align &\\min x \\ && tf x + \\phi x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH Algorithm The algorithm can be described as follows. 1. Choose MATH and set MATH . 2. At MATH , solve the barrier problem to obtain MATH . 3. While MATH 3-1. Choose MATH . 3-2. Initialize Newton's method with MATH . warm start At MATH , solve the barrier problem to obtain MATH . end while Comments Common update method : MATH , MATH Warm start : In step 3-2, the solution from the previous step is used as the initial value for the next step, which is called warm start. Centering step : Steps 2 and 3-2 in the algorithm, which solve the barrier problem, are called centering steps or outer iterations . Considerations In choosing MATH and MATH , the following trade-offs must be considered. Choice of MATH If MATH is too small, the number of outer iterations increases. In this case, warm start helps. If MATH is too large, many iterations are required for Newton's method to converge in every centering step. Choice of initial algorithm value If MATH is too small, the number of outer iterations increases. If MATH is too large, it becomes the same problem as v.0. Therefore, Newton's method requires many iterations to find MATH in the first centering step. Fortunately, the performance of the actual barrier method is very robust to the choice of MATH and MATH . Moreover, the appropriate range of these parameters varies depending on the problem size. Example of small LP The following figure shows the performance when executing an LP problem with n=50 dimensions and m=100 inequality constraints using the barrier method. It can be confirmed that when MATH , the outer iterations increase, and when MATH , the centering steps increase relatively compared to when MATH . Fig 1 Example of small LP 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_04_barrier_method_v0_and_v1/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_04_barrier_method_v0_and_v1",
    "title": "15-04 Barrier method v.0 and v.1",
    "chapter": "15",
    "order": 10,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Barrier method v.0 Barrier method v.0 chooses MATH for MATH and solves the following barrier problem to obtain MATH . > MATH \\begin align &\\min x \\ && tf x + \\phi x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH Here, MATH is the number of constraints and MATH is a multiple of MATH , so as MATH gets smaller, MATH becomes very large, and eventually the end of the central path is reached, making the problem equivalent to the original problem. Therefore, this can be very slow and difficult to solve. Thus, a better approach is to follow the central path to find the solution, which leads to the definition of barrier method v.1 . Barrier method v.1 Barrier method v.1 is a method that gradually increases the value of MATH and solves the following barrier problem multiple times. > MATH \\begin align &\\min x \\ && tf x + \\phi x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH Algorithm The algorithm can be described as follows. 1. Choose MATH and set MATH . 2. At MATH , solve the barrier problem to obtain MATH . 3. While MATH 3-1. Choose MATH . 3-2. Initialize Newton's method with MATH . warm start At MATH , solve the barrier problem to obtain MATH . end while Comments Common update method : MATH , MATH Warm start : In step 3-2, the solution from the previous step is used as the initial value for the next step, which is called warm start. Centering step : Steps 2 and 3-2 in the algorithm, which solve the barrier problem, are called centering steps or outer iterations . Considerations In choosing MATH and MATH , the following trade-offs must be considered. Choice of MATH If MATH is too small, the number of outer iterations increases. In this case, warm start helps. If MATH is too large, many iterations are required for Newton's method to converge in every centering step. Choice of initial algorithm value If MATH is too small, the number of outer iterations increases. If MATH is too large, it becomes the same problem as v.0. Therefore, Newton's method requires many iterations to find MATH in the first centering step. Fortunately, the performance of the actual barrier method is very robust to the choice of MATH and MATH . Moreover, the appropriate range of these parameters varies depending on the problem size. Example of small LP The following figure shows the performance when executing an LP problem with n=50 dimensions and m=100 inequality constraints using the barrier method. It can be confirmed that when MATH , the outer iterations increase, and when MATH , the centering steps increase relatively compared to when MATH . Fig 1 Example of small LP 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_04_barrier_method_v0_and_v1/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_04_barrier_method_v0_and_v1",
    "title": "15-04 Barrier method v.0 and v.1",
    "chapter": "15",
    "order": 10,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Barrier method v.0 Barrier method v.0 chooses MATH for MATH and solves the following barrier problem to obtain MATH . > MATH \\begin align &\\min x \\ && tf x + \\phi x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH Here, MATH is the number of constraints and MATH is a multiple of MATH , so as MATH gets smaller, MATH becomes very large, and eventually the end of the central path is reached, making the problem equivalent to the original problem. Therefore, this can be very slow and difficult to solve. Thus, a better approach is to follow the central path to find the solution, which leads to the definition of barrier method v.1 . Barrier method v.1 Barrier method v.1 is a method that gradually increases the value of MATH and solves the following barrier problem multiple times. > MATH \\begin align &\\min x \\ && tf x + \\phi x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH Algorithm The algorithm can be described as follows. 1. Choose MATH and set MATH . 2. At MATH , solve the barrier problem to obtain MATH . 3. While MATH 3-1. Choose MATH . 3-2. Initialize Newton's method with MATH . warm start At MATH , solve the barrier problem to obtain MATH . end while Comments Common update method : MATH , MATH Warm start : In step 3-2, the solution from the previous step is used as the initial value for the next step, which is called warm start. Centering step : Steps 2 and 3-2 in the algorithm, which solve the barrier problem, are called centering steps or outer iterations . Considerations In choosing MATH and MATH , the following trade-offs must be considered. Choice of MATH If MATH is too small, the number of outer iterations increases. In this case, warm start helps. If MATH is too large, many iterations are required for Newton's method to converge in every centering step. Choice of initial algorithm value If MATH is too small, the number of outer iterations increases. If MATH is too large, it becomes the same problem as v.0. Therefore, Newton's method requires many iterations to find MATH in the first centering step. Fortunately, the performance of the actual barrier method is very robust to the choice of MATH and MATH . Moreover, the appropriate range of these parameters varies depending on the problem size. Example of small LP The following figure shows the performance when executing an LP problem with n=50 dimensions and m=100 inequality constraints using the barrier method. It can be confirmed that when MATH , the outer iterations increase, and when MATH , the centering steps increase relatively compared to when MATH . Fig 1 Example of small LP 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_04_barrier_method_v0_and_v1/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_04_barrier_method_v0_and_v1",
    "title": "15-04 Barrier method v.0 and v.1",
    "chapter": "15",
    "order": 10,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Barrier method v.0 Barrier method v.0 chooses MATH for MATH and solves the following barrier problem to obtain MATH . > MATH \\begin align &\\min x \\ && tf x + \\phi x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH Here, MATH is the number of constraints and MATH is a multiple of MATH , so as MATH gets smaller, MATH becomes very large, and eventually the end of the central path is reached, making the problem equivalent to the original problem. Therefore, this can be very slow and difficult to solve. Thus, a better approach is to follow the central path to find the solution, which leads to the definition of barrier method v.1 . Barrier method v.1 Barrier method v.1 is a method that gradually increases the value of MATH and solves the following barrier problem multiple times. > MATH \\begin align &\\min x \\ && tf x + \\phi x \\\\ &\\text subject to \\ && Ax = b \\\\ \\end align MATH Algorithm The algorithm can be described as follows. 1. Choose MATH and set MATH . 2. At MATH , solve the barrier problem to obtain MATH . 3. While MATH 3-1. Choose MATH . 3-2. Initialize Newton's method with MATH . warm start At MATH , solve the barrier problem to obtain MATH . end while Comments Common update method : MATH , MATH Warm start : In step 3-2, the solution from the previous step is used as the initial value for the next step, which is called warm start. Centering step : Steps 2 and 3-2 in the algorithm, which solve the barrier problem, are called centering steps or outer iterations . Considerations In choosing MATH and MATH , the following trade-offs must be considered. Choice of MATH If MATH is too small, the number of outer iterations increases. In this case, warm start helps. If MATH is too large, many iterations are required for Newton's method to converge in every centering step. Choice of initial algorithm value If MATH is too small, the number of outer iterations increases. If MATH is too large, it becomes the same problem as v.0. Therefore, Newton's method requires many iterations to find MATH in the first centering step. Fortunately, the performance of the actual barrier method is very robust to the choice of MATH and MATH . Moreover, the appropriate range of these parameters varies depending on the problem size. Example of small LP The following figure shows the performance when executing an LP problem with n=50 dimensions and m=100 inequality constraints using the barrier method. It can be confirmed that when MATH , the outer iterations increase, and when MATH , the centering steps increase relatively compared to when MATH . Fig 1 Example of small LP 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_04_barrier_method_v0_and_v1/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_05_convergence_analysis",
    "title": "15-05 Convergence analysis",
    "chapter": "15",
    "order": 11,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Assuming the centering step in the barrier method is solved exactly, we can obtain the following convergence results. Convergence Theorem After MATH centering steps, the barrier method satisfies the following equation. Here, MATH is the number of outer iterations. > MATH \\begin align f x^ k - f^ \\le \\frac m \\mu^k t^ 0 \\end align MATH That is, to reach the desired accuracy level MATH with the barrier method, you need the following number of centering steps plus one for the first centering step. > MATH \\begin align \\frac log m/ t^ 0 \\epsilon \\log \\mu + 1 \\end align MATH Therefore, we see that the convergence is linear, with MATH . Newton's method has quadratic convergence with MATH , but in this case, the problem is very difficult, so linear convergence is not such a bad result. For the definitions of linear and quadratic convergence, refer to the Wiki. Reference: Rate of convergence https://en.wikipedia.org/wiki/Rate of convergence",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_05_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_05_convergence_analysis",
    "title": "15-05 Convergence analysis",
    "chapter": "15",
    "order": 11,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Assuming the centering step in the barrier method is solved exactly, we can obtain the following convergence results. Convergence Theorem After MATH centering steps, the barrier method satisfies the following equation. Here, MATH is the number of outer iterations. > MATH \\begin align f x^ k - f^ \\le \\frac m \\mu^k t^ 0 \\end align MATH That is, to reach the desired accuracy level MATH with the barrier method, you need the following number of centering steps plus one for the first centering step. > MATH \\begin align \\frac log m/ t^ 0 \\epsilon \\log \\mu + 1 \\end align MATH Therefore, we see that the convergence is linear, with MATH . Newton's method has quadratic convergence with MATH , but in this case, the problem is very difficult, so linear convergence is not such a bad result. For the definitions of linear and quadratic convergence, refer to the Wiki. Reference: Rate of convergence https://en.wikipedia.org/wiki/Rate of convergence",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_05_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_05_convergence_analysis",
    "title": "15-05 Convergence analysis",
    "chapter": "15",
    "order": 11,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Assuming the centering step in the barrier method is solved exactly, we can obtain the following convergence results. Convergence Theorem After MATH centering steps, the barrier method satisfies the following equation. Here, MATH is the number of outer iterations. > MATH \\begin align f x^ k - f^ \\le \\frac m \\mu^k t^ 0 \\end align MATH That is, to reach the desired accuracy level MATH with the barrier method, you need the following number of centering steps plus one for the first centering step. > MATH \\begin align \\frac log m/ t^ 0 \\epsilon \\log \\mu + 1 \\end align MATH Therefore, we see that the convergence is linear, with MATH . Newton's method has quadratic convergence with MATH , but in this case, the problem is very difficult, so linear convergence is not such a bad result. For the definitions of linear and quadratic convergence, refer to the Wiki. Reference: Rate of convergence https://en.wikipedia.org/wiki/Rate of convergence",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_05_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_05_convergence_analysis",
    "title": "15-05 Convergence analysis",
    "chapter": "15",
    "order": 11,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Assuming the centering step in the barrier method is solved exactly, we can obtain the following convergence results. Convergence Theorem After MATH centering steps, the barrier method satisfies the following equation. Here, MATH is the number of outer iterations. > MATH \\begin align f x^ k - f^ \\le \\frac m \\mu^k t^ 0 \\end align MATH That is, to reach the desired accuracy level MATH with the barrier method, you need the following number of centering steps plus one for the first centering step. > MATH \\begin align \\frac log m/ t^ 0 \\epsilon \\log \\mu + 1 \\end align MATH Therefore, we see that the convergence is linear, with MATH . Newton's method has quadratic convergence with MATH , but in this case, the problem is very difficult, so linear convergence is not such a bad result. For the definitions of linear and quadratic convergence, refer to the Wiki. Reference: Rate of convergence https://en.wikipedia.org/wiki/Rate of convergence",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_05_convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_07_feasibility_methods",
    "title": "15-07 Feasibility methods",
    "chapter": "15",
    "order": 13,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "So far, we have implicitly assumed that the first centering step MATH starts from a strictly feasible point to compute MATH . This means that MATH is a strictly feasible point satisfying the following conditions: > MATH Maximum infeasibility How do we find MATH ? We can solve the following problem to find it. > MATH \\begin align &\\min x, s \\ && s \\\\ &\\text subject to \\ && h i x \\le s,& i = 1, \\cdots, m \\\\ &&& Ax = b \\\\ \\end align MATH The goal is to make the solution MATH negative. This problem is called the feasibility method . Finding a strictly feasible starting point is easy, so it can also be solved using the barrier method. That is, you can add slack variables to the inequality constraint MATH and convert it to an equality constraint to solve it. When solving this problem, high accuracy is not required; you just need to find a feasible MATH with MATH s MATH \\begin align &\\min x, s \\ && 1^Ts \\\\ &\\text subject to \\ && h i x \\le s i,& i = 1, \\cdots, m \\\\ &&& Ax = b \\\\ \\end align MATH The advantage of this method is that by looking at the solution MATH , you can tell if the problem is infeasible. That is, if any element of MATH is greater than or equal to 0, the corresponding constraint is not satisfied.",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_07_feasibility_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_07_feasibility_methods",
    "title": "15-07 Feasibility methods",
    "chapter": "15",
    "order": 13,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "So far, we have implicitly assumed that the first centering step MATH starts from a strictly feasible point to compute MATH . This means that MATH is a strictly feasible point satisfying the following conditions: > MATH Maximum infeasibility How do we find MATH ? We can solve the following problem to find it. > MATH \\begin align &\\min x, s \\ && s \\\\ &\\text subject to \\ && h i x \\le s,& i = 1, \\cdots, m \\\\ &&& Ax = b \\\\ \\end align MATH The goal is to make the solution MATH negative. This problem is called the feasibility method . Finding a strictly feasible starting point is easy, so it can also be solved using the barrier method. That is, you can add slack variables to the inequality constraint MATH and convert it to an equality constraint to solve it. When solving this problem, high accuracy is not required; you just need to find a feasible MATH with MATH s MATH \\begin align &\\min x, s \\ && 1^Ts \\\\ &\\text subject to \\ && h i x \\le s i,& i = 1, \\cdots, m \\\\ &&& Ax = b \\\\ \\end align MATH The advantage of this method is that by looking at the solution MATH , you can tell if the problem is infeasible. That is, if any element of MATH is greater than or equal to 0, the corresponding constraint is not satisfied.",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_07_feasibility_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_07_feasibility_methods",
    "title": "15-07 Feasibility methods",
    "chapter": "15",
    "order": 13,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "So far, we have implicitly assumed that the first centering step MATH starts from a strictly feasible point to compute MATH . This means that MATH is a strictly feasible point satisfying the following conditions: > MATH Maximum infeasibility How do we find MATH ? We can solve the following problem to find it. > MATH \\begin align &\\min x, s \\ && s \\\\ &\\text subject to \\ && h i x \\le s,& i = 1, \\cdots, m \\\\ &&& Ax = b \\\\ \\end align MATH The goal is to make the solution MATH negative. This problem is called the feasibility method . Finding a strictly feasible starting point is easy, so it can also be solved using the barrier method. That is, you can add slack variables to the inequality constraint MATH and convert it to an equality constraint to solve it. When solving this problem, high accuracy is not required; you just need to find a feasible MATH with MATH s MATH \\begin align &\\min x, s \\ && 1^Ts \\\\ &\\text subject to \\ && h i x \\le s i,& i = 1, \\cdots, m \\\\ &&& Ax = b \\\\ \\end align MATH The advantage of this method is that by looking at the solution MATH , you can tell if the problem is infeasible. That is, if any element of MATH is greater than or equal to 0, the corresponding constraint is not satisfied.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_07_feasibility_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_07_feasibility_methods",
    "title": "15-07 Feasibility methods",
    "chapter": "15",
    "order": 13,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "So far, we have implicitly assumed that the first centering step MATH starts from a strictly feasible point to compute MATH . This means that MATH is a strictly feasible point satisfying the following conditions: > MATH Maximum infeasibility How do we find MATH ? We can solve the following problem to find it. > MATH \\begin align &\\min x, s \\ && s \\\\ &\\text subject to \\ && h i x \\le s,& i = 1, \\cdots, m \\\\ &&& Ax = b \\\\ \\end align MATH The goal is to make the solution MATH negative. This problem is called the feasibility method . Finding a strictly feasible starting point is easy, so it can also be solved using the barrier method. That is, you can add slack variables to the inequality constraint MATH and convert it to an equality constraint to solve it. When solving this problem, high accuracy is not required; you just need to find a feasible MATH with MATH s MATH \\begin align &\\min x, s \\ && 1^Ts \\\\ &\\text subject to \\ && h i x \\le s i,& i = 1, \\cdots, m \\\\ &&& Ax = b \\\\ \\end align MATH The advantage of this method is that by looking at the solution MATH , you can tell if the problem is infeasible. That is, if any element of MATH is greater than or equal to 0, the corresponding constraint is not satisfied.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_07_feasibility_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_08_formal_barrier_method",
    "title": "15-08 Formal barrier method",
    "chapter": "15",
    "order": 14,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "If a convex function MATH defined on an open convex set MATH satisfies the following conditions, then the function is a self-concordant barrier with parameter MATH . MATH is self-concordant For all MATH , the Newton decrement is bounded by the constant MATH as follows. > MATH Let's consider the following LP problem. Here, MATH is the closure of the domain MATH . > MATH \\begin align &\\min x \\ && c^Tx \\\\ &\\text subject to \\ && x \\in \\bar D \\\\ \\end align MATH This problem can be approximated as follows. > MATH \\begin align &\\min x && tc^Tx + \\phi x \\\\ \\end align MATH Here, let MATH and let the corresponding Newton decrement be MATH . Key observation: When MATH > MATH \\begin align \\lambda t^+ x \\le & \\frac t^+ t \\lambda t^+ x + \\left \\frac t^+ t -1 \\right \\sqrt \\nu \\\\\\ \\end align MATH Theorem > MATH \\begin align & \\text if \\quad \\lambda t x \\le \\frac 1 9 \\quad \\text and \\quad \\frac t^+ t \\le 1 + \\frac 1 8 \\sqrt \\nu \\quad \\text then \\quad \\lambda t^+ x^+ \\le \\frac 1 9 \\\\ & \\qquad \\qquad \\text for \\quad x^+ = x - \\nabla^2 \\phi t^+ x ^ -1 \\nabla \\phi t^+ x \\end align MATH In conclusion, if we start with MATH such that MATH and choose MATH , then one Newton step per centering step is sufficient.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_08_formal_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_08_formal_barrier_method",
    "title": "15-08 Formal barrier method",
    "chapter": "15",
    "order": 14,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "If a convex function MATH defined on an open convex set MATH satisfies the following conditions, then the function is a self-concordant barrier with parameter MATH . MATH is self-concordant For all MATH , the Newton decrement is bounded by the constant MATH as follows. > MATH Let's consider the following LP problem. Here, MATH is the closure of the domain MATH . > MATH \\begin align &\\min x \\ && c^Tx \\\\ &\\text subject to \\ && x \\in \\bar D \\\\ \\end align MATH This problem can be approximated as follows. > MATH \\begin align &\\min x && tc^Tx + \\phi x \\\\ \\end align MATH Here, let MATH and let the corresponding Newton decrement be MATH . Key observation: When MATH > MATH \\begin align \\lambda t^+ x \\le & \\frac t^+ t \\lambda t^+ x + \\left \\frac t^+ t -1 \\right \\sqrt \\nu \\\\\\ \\end align MATH Theorem > MATH \\begin align & \\text if \\quad \\lambda t x \\le \\frac 1 9 \\quad \\text and \\quad \\frac t^+ t \\le 1 + \\frac 1 8 \\sqrt \\nu \\quad \\text then \\quad \\lambda t^+ x^+ \\le \\frac 1 9 \\\\ & \\qquad \\qquad \\text for \\quad x^+ = x - \\nabla^2 \\phi t^+ x ^ -1 \\nabla \\phi t^+ x \\end align MATH In conclusion, if we start with MATH such that MATH and choose MATH , then one Newton step per centering step is sufficient.",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_08_formal_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/vi/chapter15/chapter16/15_08_formal_barrier_method",
    "title": "15-08 Formal barrier method",
    "chapter": "15",
    "order": 14,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "If a convex function MATH defined on an open convex set MATH satisfies the following conditions, then the function is a self-concordant barrier with parameter MATH . MATH is self-concordant For all MATH , the Newton decrement is bounded by the constant MATH as follows. > MATH Let's consider the following LP problem. Here, MATH is the closure of the domain MATH . > MATH \\begin align &\\min x \\ && c^Tx \\\\ &\\text subject to \\ && x \\in \\bar D \\\\ \\end align MATH This problem can be approximated as follows. > MATH \\begin align &\\min x && tc^Tx + \\phi x \\\\ \\end align MATH Here, let MATH and let the corresponding Newton decrement be MATH . Key observation: When MATH > MATH \\begin align \\lambda t^+ x \\le & \\frac t^+ t \\lambda t^+ x + \\left \\frac t^+ t -1 \\right \\sqrt \\nu \\\\\\ \\end align MATH Theorem > MATH \\begin align & \\text if \\quad \\lambda t x \\le \\frac 1 9 \\quad \\text and \\quad \\frac t^+ t \\le 1 + \\frac 1 8 \\sqrt \\nu \\quad \\text then \\quad \\lambda t^+ x^+ \\le \\frac 1 9 \\\\ & \\qquad \\qquad \\text for \\quad x^+ = x - \\nabla^2 \\phi t^+ x ^ -1 \\nabla \\phi t^+ x \\end align MATH In conclusion, if we start with MATH such that MATH and choose MATH , then one Newton step per centering step is sufficient.",
    "url": "/optimization-for-data-science-iuh-2025/contents/vi/chapter15/chapter16/15_08_formal_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter15/chapter16/15_08_formal_barrier_method",
    "title": "15-08 Formal barrier method",
    "chapter": "15",
    "order": 14,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "If a convex function MATH defined on an open convex set MATH satisfies the following conditions, then the function is a self-concordant barrier with parameter MATH . MATH is self-concordant For all MATH , the Newton decrement is bounded by the constant MATH as follows. > MATH Let's consider the following LP problem. Here, MATH is the closure of the domain MATH . > MATH \\begin align &\\min x \\ && c^Tx \\\\ &\\text subject to \\ && x \\in \\bar D \\\\ \\end align MATH This problem can be approximated as follows. > MATH \\begin align &\\min x && tc^Tx + \\phi x \\\\ \\end align MATH Here, let MATH and let the corresponding Newton decrement be MATH . Key observation: When MATH > MATH \\begin align \\lambda t^+ x \\le & \\frac t^+ t \\lambda t^+ x + \\left \\frac t^+ t -1 \\right \\sqrt \\nu \\\\\\ \\end align MATH Theorem > MATH \\begin align & \\text if \\quad \\lambda t x \\le \\frac 1 9 \\quad \\text and \\quad \\frac t^+ t \\le 1 + \\frac 1 8 \\sqrt \\nu \\quad \\text then \\quad \\lambda t^+ x^+ \\le \\frac 1 9 \\\\ & \\qquad \\qquad \\text for \\quad x^+ = x - \\nabla^2 \\phi t^+ x ^ -1 \\nabla \\phi t^+ x \\end align MATH In conclusion, if we start with MATH such that MATH and choose MATH , then one Newton step per centering step is sufficient.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter15/chapter16/15_08_formal_barrier_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter16/16_duality_revisited",
    "title": "16 Duality Revisited",
    "chapter": "16",
    "order": 1,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this chapter, we organize essential background knowledge about duality before covering the Primal-Dual Interior-Point method. The Primal-Dual Interior-Point method can be viewed as an extension of the Barrier method, and the concept of duality emerges as a key topic in the process of developing the content. References and further readings O. Guler 2010 , “Foundations of Optimization”, Chapter 11. J. Renegar 2001 , “A mathematical view of interior-point methods in convex optimization,” Chapters 2 and 3. S. Wright 1997 , “Primal-dual interior-point methods”, Chapters 5 and 6.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter16/16_duality_revisited/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter16/16_01_lagrangian_duality_revisited",
    "title": "16-01 Lagrangian duality revisited",
    "chapter": "16",
    "order": 2,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In this section, we will show that primal and dual problems can be defined using the Lagrangian, and use this definition to derive dual problems for standard form linear programming and quadratic programming. Furthermore, we will derive the dual problem for linear programming with barrier problems applied, showing that its form is the same as the barrier problem for the dual problem of linear programming. First, let's define the primal problem and Lagrangian as follows. Primal problem > MATH >\\begin align > \\mathop \\text minimize x &\\quad f x \\\\\\\\ > \\text subject to &\\quad h i x \\leq 0, i = 1, \\ldots, m \\\\\\\\ > &\\quad l j x = 0, j = 1, \\ldots, r >\\end align > MATH Lagrangian > MATH >L x,u,v = f x + \\sum i=1 ^m u i h i x + \\sum j=1 ^r v j l j x > MATH At this point, the primal problem and dual problem can be redefined as problems involving the Lagrangian. Rewritten primal problem > MATH >\\min x \\mathop \\max u,v u \\geq 0 L x,u,v > MATH The redefined primal problem does not explicitly state constraints, but it has the effect of acting like an indicator function for any infeasible MATH that violates the constraints. 1. If MATH for some MATH , then MATH is an infeasible point. In this case, MATH diverges to MATH due to MATH , so it acts as an indicator function for any MATH that violates the inequality constraint. 2. If MATH for some MATH , then MATH is an infeasible point. In this case, MATH diverges to MATH due to MATH , so it acts as an indicator function for any MATH that violates the equality constraint. Rewritten dual problem > MATH >\\mathop \\max u,v u \\geq 0 \\min x L x,u,v > MATH In the dual problem, relaxation of the domain is necessary, so it should not act as an indicator function for the constraints of the primal problem. Since taking MATH for fixed MATH cannot enforce the constraints of the primal problem, the redefined dual problem also has the effect of relaxing the domain. Reference: 11-02 Lagrange dual function % multilang post url contents/chapter11/21-03-24-11 02 Lagrange dual function % Weak and strong duality Let's revisit weak duality and strong duality. Theorem: weak duality When MATH and MATH are the optimal values for the primal problem and dual problem respectively, the following is always satisfied: MATH p \\ge d MATH Theorem: strong duality refined Slater's condition For the domain set MATH , assume that MATH are convex and MATH are affine. If there exists MATH that satisfies the following: > MATH \\begin align >h i \\hat x \\ & \\lt 0, \\ && i=1, \\dots, p \\\\ >h i \\hat x \\ & \\le 0, \\ && i=p+1, \\dots, m \\\\ >l j \\hat x \\ & = 0, \\ && j = 1, \\dots, r \\end align MATH then MATH is guaranteed for the optimal values MATH of the primal problem and dual problem. Example: linear programming Let's derive the dual problem of linear programming using the dual problem defined earlier. Primal problem of LP in standard form > MATH >\\begin align > \\mathop \\text minimize x &\\quad c^Tx \\\\\\\\ > \\text subject to &\\quad Ax = b \\\\\\\\ > &\\quad x \\ge 0 >\\end align > MATH According to the previous definition, the dual problem of the above problem is as follows: MATH We substitute the relationship MATH obtained by solving MATH into the dual problem. MATH This can be organized as follows: Dual problem of LP > MATH >\\begin align > \\mathop \\text maximize s,y &\\quad b^Ty \\\\\\\\ > \\text subject to &\\quad A^Ty + s = 0 \\\\\\\\ > &\\quad s \\ge 0 >\\end align > MATH Example: convex quadratic programming Now let's derive the dual problem of quadratic programming using the dual problem defined earlier. Primal problem of QP in standard form > MATH >\\begin align > \\mathop \\text minimize x &\\quad \\frac 1 2 x^T Q x + c^Tx \\\\ > \\text subject to &\\quad Ax = b \\\\ > &\\quad x \\ge 0, \\\\ > >\\end align MATH > > MATH According to the previous definition, the dual problem of the above problem is as follows: MATH We substitute the relationship MATH obtained by solving MATH into the dual problem. MATH \\begin align &\\mathop \\max s,y,x s\\ge0 \\: \\frac 1 2 x^T A^Ty +s - c + c^Tx - s^Tx + b-Ax ^T y \\quad \\text s.t. Qx = A^Ty +s - c\\\\\\\\ &= \\mathop \\max s,y,x s\\ge0 \\: x^T A^Ty +s - c + c^Tx - s^Tx + b-Ax ^T y - \\frac 1 2 x^T A^Ty +s - c \\quad \\text s.t. Qx = A^Ty +s - c\\\\\\\\ &= \\mathop \\max s,y,x s\\ge0 \\: b^Ty - \\frac 1 2 x^T A^Ty +s - c \\quad \\text s.t. Qx = A^Ty +s - c\\\\\\\\ &= \\mathop \\max s,y,x s\\ge0 \\: b^Ty - \\frac 1 2 x^T Q x \\quad \\text s.t. Qx = A^Ty +s - c \\end align MATH This can be organized as follows: Dual problem of QP > MATH >\\begin align > \\mathop \\text maximize s,y,x &\\quad b^Ty - \\frac 1 2 x^T Q x\\\\\\\\ > \\text subject to &\\quad A^Ty + s - c = Qx \\\\\\\\ > &\\quad s \\ge 0 >\\end align > MATH The dual problem of a quadratic problem is also a quadratic problem. Example: barrier problem for linear programming The barrier problem for linear programming is defined as follows: Barrier problem for LP > MATH >\\begin align > \\mathop \\text minimize x &\\quad c^Tx - \\tau \\sum i=1 ^n \\log x i \\\\ > \\text subject to &\\quad Ax = b, \\\\ >\\end align MATH > > MATH According to the previous definition, the dual problem of the above problem is as follows: MATH \\begin align \\max y \\min x \\: L x,y &= \\max y \\min x \\: c^Tx - \\tau \\sum i=1 ^n \\log x i + b-Ax ^T y\\\\\\\\ &= \\max y \\min x \\: \\underbrace c-A^Ty s \\doteq c-A^Ty x - \\tau \\sum i=1 ^n \\log x i + b^Ty\\\\\\\\ &= \\max y \\min x \\: \\sum i=1 ^n \\big s i^Tx i - \\tau \\log x i \\big + b^Ty \\quad \\text s.t. A^Ty +s = c \\end align MATH Here, MATH will be minimized when MATH . Therefore, let's substitute MATH for MATH in the dual problem. MATH \\begin align &\\max s,y \\: b^Ty + n\\tau - \\tau \\sum i=1 ^n log \\frac \\tau s i \\quad \\text s.t. A^Ty +s = c\\\\\\\\ &= \\max s,y \\: b^Ty + \\tau \\sum i=1 ^n log s i + n\\tau - n\\tau\\log \\tau \\quad \\text s.t. A^Ty +s = c\\\\\\\\ \\end align MATH Since MATH can be omitted from the problem, the dual problem can be organized as follows: Dual problem of Barrier problem for LP > MATH >\\begin align > \\mathop \\text maximize s,y &\\quad b^Ty + \\tau \\sum i=1 ^n log s i \\\\\\\\ > \\text subject to &\\quad A^Ty + s = c \\\\\\\\ >\\end align > MATH We can see that this problem is identical to the barrier problem for the dual problem of linear programming.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter16/16_01_lagrangian_duality_revisited/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter16/16_02_optimality_conditions",
    "title": "16-02 Optimality conditions",
    "chapter": "16",
    "order": 3,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In this section, we will examine the KKT optimality conditions for primal problems and barrier problems respectively, and then compare their differences. KKT optimality conditions Let's review the KKT conditions that we covered in Chapter 12. KKT conditions are used as conditions for determining optimality. Primal problem > MATH >\\begin align > \\mathop \\text minimize x &\\quad f x \\\\\\\\ > \\text subject to &\\quad h i x \\leq 0, i = 1, \\ldots, m \\\\\\\\ > &\\quad l j x = 0, j = 1, \\ldots, r >\\end align > MATH When the given primal problem is convex, KKT conditions become sufficient conditions for primal & dual optimality. That is, when MATH are convex and MATH are affine, if MATH satisfy the following KKT conditions, then MATH and MATH are primal & dual optimal with zero duality gap. We assume that MATH are differentiable. 참고: 12-01 KKT conditions % multilang post url contents/chapter12/21-04-02-12 00 KKT conditions % KKT conditions for the given primal problem > MATH >\\begin align >l i &= 0, \\quad i=1, \\dots, r\\\\\\\\ >u i^\\star, -h i x^\\star &\\ge 0, \\quad i=1, \\dots, m\\\\\\\\ >u i^\\star h i x^\\star &= 0, \\quad i=1, \\dots, m\\\\\\\\ >\\nabla f x^\\star + \\sum i=1 ^m \\nabla h i x^\\star u^\\star i + \\sum i=1 ^r \\nabla l i x^\\star v i^\\star &= 0.\\\\\\\\ >\\end align > MATH Central path equations Let's also examine the conditions for determining the optimality of barrier problems. Barrier problem > MATH \\begin align \\mathop \\text minimize x &\\quad f x + \\tau \\phi x \\\\\\\\ &\\quad l j x = 0, j = 1, \\ldots, r \\\\\\\\ \\end align MATH > > MATH By organizing the KKT conditions for barrier problems, we can derive the following optimality conditions. Note the differences in the inequality constraint and complementary slackness conditions compared to the KKT optimality conditions for primal problems examined earlier. Reference: 15-03-01 Perturbed KKT conditions % multilang post url contents/chapter15/21-03-28-15 03 01 perturbed kkt conditions % Optimality conditions for barrier problem and its dual > MATH \\begin align l i &= 0, \\quad i=1, \\dots, r\\\\\\\\ u i t , -h i x^\\star t &\\gt 0, \\quad i=1, \\dots, m\\\\\\\\ u i t h i x^\\star t &= -\\tau, \\quad i=1, \\dots, m\\\\\\\\ \\nabla f x^\\star t + \\sum i=1 ^m \\nabla h i x^\\star t u i t + \\sum i=1 ^r \\nabla l i x^\\star t \\hat v i^\\star &= 0,\\\\\\\\ \\end align \\\\\\\\ MATH > > MATH Special case: linear programming Recall: Primal problem of LP in standard form > MATH >\\begin align > \\mathop \\text minimize x &\\quad c^Tx \\\\\\\\ > \\text subject to &\\quad Ax = b \\\\\\\\ > &\\quad x \\ge 0 >\\end align > MATH Recall: Dual problem of LP > MATH >\\begin align > \\mathop \\text maximize s,y &\\quad b^Ty \\\\\\\\ > \\text subject to &\\quad A^Ty + s = c \\\\\\\\ > &\\quad s \\ge 0 >\\end align > MATH Linear programming has the nice property of always satisfying strong duality due to the refined Slater's condition, since the inequality constraints are affine. The optimality conditions for LP are as follows: > MATH >\\begin align >A^T y^\\star + s^\\star &= c\\\\\\\\ >Ax^\\star &= b\\\\\\\\ >X S \\mathbb 1 &= 0\\\\\\\\ >x^\\star, s^\\star &\\ge 0,\\\\\\\\ >\\end align MATH > > MATH Note that MATH is equivalent to MATH . We use MATH notation for convenience in algorithms that will be introduced later. Algorithms for linear programming We introduce two representative methods for solving LP using optimality conditions. 1. Simplex: A method that maintains conditions 1, 2, and 3 while gradually satisfying condition 4. 2. Interior-point methods: A method that maintains condition 4 while gradually satisfying conditions 1, 2, and 3. This will be covered in the next chapter. Central path for linear programming Recall: Barrier problem for LP > MATH \\begin align \\mathop \\text minimize x &\\quad c^Tx - \\tau \\sum i=1 ^n \\log x i \\\\\\\\ \\text subject to &\\quad Ax = b, \\\\\\\\ \\text where &\\quad \\tau > 0 \\end align MATH Recall: Dual problem of Barrier problem for LP > MATH >\\begin align > \\mathop \\text maximize s,y &\\quad b^Ty + \\tau \\sum i=1 ^n log s i \\\\\\\\ > \\text subject to &\\quad A^Ty + s = c \\\\\\\\ >\\end align > MATH The optimality conditions for the barrier problem of LP are as follows: > MATH \\begin align A^T y^\\star + s^\\star &= c\\\\\\\\ Ax^\\star &= b\\\\\\\\ X S \\mathbb 1 &= \\tau \\mathbb 1 \\\\\\\\ x^\\star, s^\\star &\\gt 0,\\\\\\\\ \\text where &\\quad X = Diag x^\\star , S = Diag s^\\star \\end align MATH Conditions 3 and 4 show differences from the KKT conditions of the primal LP.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter16/16_02_optimality_conditions/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter16/16_03_fenchel_duality",
    "title": "16-03 Fenchel duality",
    "chapter": "16",
    "order": 4,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "MathJax.Hub.Config displayAlign: \"center\" ; In 13-04 Conjugate function % multilang post url contents/chapter13/21-04-05-13 04 Conjugate function % , we learned how to derive dual problems using conjugate functions. Fenchel duality refers to dual problems derived from conjugate functions that have the following form: MATH \\max v -f^ A^Tv - g^ -v MATH Let's explore where this form of problem is derived from. Primal problem > MATH > \\min x \\quad f x + g Ax > MATH The given problem can be redefined with an added equality constraint. Primal problem rewritten > MATH \\begin align &\\min x,z \\ && f x + g z \\\\ &\\text subject to \\ && Ax = z. \\end align MATH Let's derive the dual problem of the redefined primal problem using conjugate functions. Recall: MATH MATH \\begin align &\\max v \\min x, z \\quad L x,z,v \\\\\\\\ = &\\max v \\min x, z \\quad f x + g z + v^T z - Ax \\\\\\\\ = &\\max v \\min x, z \\quad v^Tz + g z - A^Tv ^Tx + f x \\\\\\\\ = &\\max v \\quad -f^ A^Tv - g^ -v \\\\\\\\ \\end align MATH Fenchel duality > MATH > \\max v -f^ A^Tv - g^ -v > MATH Nice Property: If MATH are convex and closed, the dual of the dual becomes the primal again. Symmetric Example: conic programming Primal problem of CP in standard form > MATH \\begin align \\mathop \\text minimize x &\\quad c^Tx \\\\\\\\ \\text subject to &\\quad Ax = b \\\\\\\\ &\\quad x \\in K \\end align MATH The above problem can be redefined using two functions MATH and MATH . Note: MATH \\begin equation f x + g Ax = \\begin cases 0, & \\text if \\ Ax=b, x \\in K \\\\\\\\ \\infty, & \\text otherwise \\end cases \\end equation MATH Primal problem of CP rewritten > MATH > \\begin align > &\\min x, z \\ && f x + g z \\\\\\ > &\\text subject to \\ && z =Ax \\\\\\ > \\end align > MATH Deriving dual problem of CP Let's derive the dual problem from the redefined CP primal problem. First, expanding functions MATH and MATH gives us the following: > MATH > \\begin align > & \\min x, z && \\; c^Tx + I K x + I \\ b\\ z \\\\\\ > &\\text subject to && \\; z =Ax \\\\ > \\end align > MATH Let's expand the problem using conjugate functions from the definition of the dual problem. > MATH > \\begin align > & \\max y \\; \\min x, z \\; L x, z, y \\\\\\ > = \\; & \\max y \\; \\min x, z \\; c^Tx + I K x + I \\ b\\ z + y^T z-Ax \\\\\\ > = \\; & \\max y \\;\\min x, z \\; c - A^Ty ^Tx + I K x \\;+ \\; y^Tz + I \\ b\\ z \\\\\\ > = \\; & \\max y \\; \\min x, z \\; - A^Ty - c ^Tx - I K x \\; - \\; - y^Tz - I \\ b\\ z \\\\\\ > = \\; & \\max y \\; - I K^ A^Ty - c - I \\ b\\ ^ -y \\\\\\ > = \\; & \\max y \\; - I -K^ A^Ty - c - I \\ b\\ ^ -y \\\\ > \\end align > MATH MATH can be expressed as a constraint. > MATH > \\begin align > A^Ty - c & = -s, \\; -s \\in -K^ \\\\\\ > \\Leftrightarrow A^Ty + s & = c, \\; s \\in K^ \\\\\\ > \\end align > MATH Since MATH , the problem can be organized as follows: > MATH > \\begin align > &\\max y, s \\ && - -b^Ty - I \\ b\\ b \\\\\\ > &\\text subject to \\ && y^TA + s = c \\\\\\ > & \\; s \\in K^ \\\\ > \\end align > MATH Since MATH , it can be removed from the problem. Dual problem of CP > MATH > \\begin align > &\\max y, s \\ && \\; b^Ty \\\\\\ > &\\text subject to \\ && y^TA + s = c \\\\\\ > & \\; s \\in K^ \\\\ > \\end align > MATH If either the primal problem or dual problem is strictly feasible, then strong duality is satisfied. If both the primal problem and dual problem are strictly feasible, then strong duality is satisfied and primal & dual optima exist. Example: semidefinite programming Let's examine the forms of primal & dual problems for SDP and the primal & dual problems for SDP's barrier problem. Primal problem of SDP > MATH >\\begin align > &\\mathop \\text minimize X && tr CX \\\\\\\\ > &\\text subject to && tr A iX = b i, \\phantom 5 i=1,\\dotsc,p \\\\\\\\ > & && X \\succeq 0 ,\\\\\\\\ >\\end align MATH > > MATH Recall: MATH Note: Unlike LP, SDP does not always satisfy strong duality. Dual problem of SDP > MATH >\\begin align > &\\mathop \\text minimize y && b^Ty \\\\\\\\ > &\\text subject to && \\sum i=1 ^m y i A i + S = C \\\\\\\\ > & && S \\succeq 0 .\\\\\\\\ >\\end align > MATH Note: The positive semidefinite cone is a self-dual cone. MATH Primal problem of Barrier problem for SDP > MATH >\\begin align > &\\mathop \\text minimize X && tr CX - \\tau \\log \\big det X \\big \\\\\\\\ > &\\text subject to && tr A iX = b i, \\phantom 5 i=1,\\dotsc,p \\\\\\\\ >\\end align MATH > > MATH Dual problem of Barrier problem for SDP > MATH >\\begin align > &\\mathop \\text minimize y, S && b^Ty + \\tau \\log \\big det S \\big \\\\\\\\ > &\\text subject to && \\sum i=1 ^m y i A i + S = C . >\\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter16/16_03_fenchel_duality/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_primal_dual_interior_point_method",
    "title": "17 Primal-Dual Interior-Point Methods",
    "chapter": "17",
    "order": 1,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this chapter, we will examine the Primal-Dual Interior-Point Method , which improves performance by reducing the centering step of the Barrier method we learned earlier to a single step. The Primal-Dual Interior-Point Method relaxes the constraint that the centering step must be feasible and uses the root finding version of Newton's Method to approximate nonlinear equations with linear equations to find solutions, making it faster and more accurate than the Barrier method. References and further readings S. Boyd and L. Vandenberghe 2004 , “Convex optimization,” Chapter 11 S. Wright 1997 , “Primal-dual interior-point methods,” Chapters 5 and 6 J. Renegar 2001 , “A mathematical view of interior-point methods” Y. Nesterov and M. Todd 1998 , “Primal-dual interior-point methods for self-scaled cones.” SIAM J. Optim.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_primal_dual_interior_point_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_01_barrier_method_duality_optimality_revisited",
    "title": "17-01 Barrier method & duality & optimality revisited",
    "chapter": "17",
    "order": 2,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In Chapter 15, we examined the barrier method, and in Chapters 13 and 16, we looked at duality. Before covering the content of this chapter, we want to briefly review the barrier method and duality. Barrier method When the following primal problem is convex and MATH are differentiable, > MATH \\begin align > &\\min x && f x \\\\ > &\\text subject to &&h i x \\leq 0, i = 1, \\dotsc, m \\\\ > &&& Ax = b \\\\ > \\end align MATH Using the log barrier function, the primal problem can be transformed into a barrier problem as follows: > MATH \\begin align > & \\min x && f x + \\frac 1 t \\phi x & \\qquad & \\min x && tf x + \\phi x \\\\ > & \\text subject to && Ax = b & \\iff \\qquad & \\text subject to && Ax = b \\\\ > & \\text where && \\phi x = - \\sum i=1 ^ m \\log -h i x > \\end align MATH The algorithm starts with MATH satisfying MATH and increases until MATH becomes less than or equal to MATH . At this time, Newton's method is used to find MATH for the initial value MATH , and the process of finding MATH at each step for MATH is repeated. The algorithm can be briefly summarized as follows: 1. Choose MATH and MATH . 2. Solve the barrier problem at MATH to find MATH . 3. While MATH 3-1. Update MATH where MATH 3-2. Initialize Newton's method with MATH warm start Solve the barrier problem at MATH to find MATH . end while For detailed information, see 15-01-02 Log barrier function & barrier method % multilang post url contents/chapter15/21-03-28-15 01 02 log barrier function and barrier method % Duality When the following primal problem is given: > MATH >\\begin align > \\mathop \\text minimize x &\\quad f x \\\\\\\\ > \\text subject to &\\quad f Ax = b \\\\\\\\ > &\\quad h x \\le 0 >\\end align > MATH This can be transformed into Lagrangian form as follows: > MATH >L x,u,v = f x + u^Th x + v^T Ax - b > MATH Using the Lagrangian defined in this way, primal and dual problems can be redefined in the following form. Please refer to Chapter 16 for detailed information. Primal Problem > MATH >\\min x \\mathop \\max u,v u \\geq 0 L x,u,v > MATH Dual problem > MATH >\\mathop \\max u,v u \\geq 0 \\min x L x,u,v > MATH Optimality conditions When MATH are convex and differentiable, and the given problem satisfies strong duality, the KKT optimality conditions for this problem are as follows: > MATH > \\begin array rcl > ∇f x +∇h x u + A^Tv & = & 0 & \\text Stationarity \\\\\\ > Uh x & = & 0 & \\text Complementary Slackness \\\\\\ > Ax & = & b & \\text Primal Feasibility \\\\\\ > u,−h x & ≥ & 0 & \\text Dual Feasibility > \\end array > MATH Here, MATH means MATH , and MATH means MATH . For detailed information, see Chapter 12 KKT conditions % multilang post url contents/chapter12/21-04-02-12 00 KKT conditions % Central path equations The function MATH can be redefined as a barrier problem as follows. In the equation below, MATH is MATH , and by making MATH gradually approach 0 and iteratively finding solutions, we ultimately obtain the solution to the original problem. > MATH >\\begin align >&\\min x && f x + τ\\phi x \\\\\\\\ >& && Ax = b \\\\\\ >& \\text where && \\phi x = −\\sum i=1 ^m \\log −h i x . >\\end align > MATH That is, in the above equation, differences from the primal problem occur depending on MATH , and the trajectory generated according to MATH , i.e., the set of solutions to the barrier problem, is called the central path. And the optimality conditions for this barrier problem are as follows: > MATH > \\begin array rcl > ∇f x +∇h x u + A^Tv & = & 0 \\\\\\ > Uh x & = & −τ\\mathbb 1 \\\\\\ > Ax & = & b \\\\\\ > u,−h x & > & 0 > \\end array > MATH For detailed information, see 16-02 Optimality conditions % multilang post url contents/chapter16/21-03-31-16 02 optimality conditions % The Primal-Dual interior point method introduced in this chapter is a method that defines the first three equations above as residuals and finds solutions by reducing them to MATH . Useful fact The solution MATH has a duality gap of size MATH , i.e., MATH , as follows: > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_01_barrier_method_duality_optimality_revisited/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_02_primal_dual_interior_point_method",
    "title": "17-02 Primal-dual interior-point method",
    "chapter": "17",
    "order": 3,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Like the barrier method, the primal-dual interior-point method also aims to approximately compute points on the central path. However, the two methods have several differences. Differences between Primal-dual interior-point method and barrier method Generally performs one Newton step per iteration. That is, there is no additional loop for the centering step. Does not necessarily need to be feasible . Pushes toward feasible regions through backtracking line search. Generally more effective . Particularly shows superior performance compared to linear convergence under appropriate conditions. Somewhat less intuitive compared to the barrier method.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_02_primal_dual_interior_point_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_02_01_central_path_equations_and_newton_step",
    "title": "17-02-01 Central path equations and Newton step",
    "chapter": "17",
    "order": 4,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "The Primal-dual interior-point method is a method that finds solutions by finding the central path, similar to the barrier method. To do this, it defines perturbed KKT conditions as residual functions and finds solutions that make them zero. This section aims to explain this approach. Central path equations By moving the right-hand side to the left-hand side in the central path equations explained in the previous 17-01 Optimality conditions % multilang post url contents/chapter17/21-05-01-17 01 barrier method duality optimality revisited % , we can organize them as follows. The optimality conditions of central path equations are also called perturbed KKT conditions. > MATH > \\begin array rcl > ∇f x +∇h x u + A^Tv & = & 0 \\\\\\ > Uh x + \\tau\\mathbb 1 & = & 0 \\\\\\ > Ax−b & = & 0 \\\\\\ > u,−h x & > & 0 > \\end array > MATH Note that the complementary slackness and inequality constraints in the KKT conditions for the original problem differ from those in the perturbed KKT conditions. For the original problem, MATH and MATH , but in the perturbed KKT conditions, MATH and MATH . These organized nonlinear equations, the perturbed KKT conditions, can be solved by approximating them as linear equations using the root finding version of Newton's method. Newton step Now let's learn about the method of finding solutions by linearly approximating the perturbed KKT conditions. The perturbed KKT conditions equation can be defined as the following residual function MATH . The reason for naming it residual is that these values must be 0 to be optimal. > MATH r x,u,v := > \\begin bmatrix > ∇f x +∇h x u + A^Tv \\\\\\ > Uh x + τ\\mathbb 1 \\\\\\ > Ax−b > \\end bmatrix , H x = \\text Diag h x MATH To find the roots of the function, approximating MATH with a first-order Taylor expansion gives us the following. This process approximates non-linear equations to linear equations. For detailed information, see 14-02-01 Root finding % multilang post url contents/chapter14/2021-03-26-14 01 newton method % > MATH \\begin align 0 & = r x + \\Delta x, u + \\Delta u, r + \\Delta v \\\\\\\\ & \\approx r x, u, v + \\nabla r x, u, v \\begin pmatrix \\Delta x \\\\\\\\ \\Delta u \\\\\\\\ \\Delta v \\\\\\\\ \\end pmatrix \\\\\\\\ \\end align MATH Accordingly, the function MATH can be organized as follows. > MATH \\begin align \\nabla r x, u, v \\begin pmatrix \\Delta x \\\\\\\\ \\Delta u \\\\\\\\ \\Delta v \\\\\\\\ \\end pmatrix = -r x, u, v \\\\\\\\ \\end align MATH By differentiating MATH with respect to MATH to obtain the Jacobian matrix MATH and substituting the above equation, we get the following. > MATH \\begin bmatrix > \\nabla^2f x + \\sum i u i \\nabla^2h i x & \\nabla h x & A^T \\\\\\ > U \\nabla h x ^T & H x & 0 \\\\\\ > A & 0 & 0 > \\end bmatrix > \\begin bmatrix > \\Delta x \\\\\\ > \\Delta u \\\\\\ > \\Delta v > \\end bmatrix = −r x,u,v MATH > where > MATH r x,u,v := > \\begin bmatrix > \\nabla f x +\\nabla h x u + A^Tv \\\\\\ > Uh x + τ\\mathbb 1 \\\\\\ > Ax−b > \\end bmatrix , H x = \\text Diag h x MATH The solution MATH to this equation is the update direction for the primal and dual variables. The reason why the method introduced in this chapter is called the Primal-Dual interior point method is that it simultaneously updates primal and dual variables using residual functions.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_02_01_central_path_equations_and_newton_step/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_02_02_surrogate_duality_gap_residuals",
    "title": "17-02-02 Surrogate duality gap, residuals",
    "chapter": "17",
    "order": 5,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "To define the Primal-Dual algorithm, let's first define three types of residuals and the surrogate duality gap. Residuals and surrogate duality gap are the objectives to be minimized in the Primal-Dual algorithm. Residuals The dual, central, and primal residuals at MATH are defined as follows. > MATH > MATH > MATH These correspond to each row of the function MATH . The Primal-dual interior point method executes in the direction of satisfying 0 rather than continuously making these three residuals equal to 0. This means that it is not necessary to be feasible during the execution process. The reason MATH is called the dual residual is that, as shown in the equation below, if MATH , it guarantees that MATH are in the domain of MATH , which means dual feasible. > MATH \\begin align & r dual = \\nabla f x +\\nabla h x u + A^Tv = 0 \\\\\\\\ & \\iff \\min x L x,u.v = g u,v \\\\\\\\ \\end align MATH Similarly, satisfying MATH means primal feasible, so MATH is called the primal residual. Surrogate duality gap While the barrier method has a duality gap because it is feasible, the primal-dual interior-point method uses surrogate duality gap because it doesn't necessarily need to be feasible. Surrogate duality gap is defined by the following equation. > MATH If MATH and MATH , then the surrogate duality gap becomes the true duality gap. In other words, if primal and dual feasible, the surrogate duality gap becomes equal to the actual duality gap MATH . Reference Perturbed KKT conditions and parameter t In the perturbed KKT conditions, the parameter t is MATH . For detailed information, see 15-03-01 Perturbed KKT conditions % multilang post url contents/chapter15/21-03-28-15 03 01 perturbed kkt conditions % and 15-03-02 Suboptimality gap % multilang post url contents/chapter15/21-03-28-15 03 02 suboptimality gap % Furthermore, if MATH r x,u,v = 0 MATH \\tau = -\\frac h x ^Tu m MATH In other words, the residual is 0 at points existing on the central path.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_02_02_surrogate_duality_gap_residuals/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_02_03_primal_dual_algorithm",
    "title": "17-02-03 Primal-Dual Algorithm",
    "chapter": "17",
    "order": 6,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "To define the Primal-Dual algorithm, let's first define MATH as follows > MATH For reference, MATH and MATH in the Barrier method are redefined and denoted as MATH and MATH in the Primal-Dual algorithm. > MATH Primal-Dual Algorithm The Primal-Dual algorithm is as follows. > 1. Choose MATH MATH > 2. Choose MATH MATH > 3. Repeat the following steps MATH > MATH Calculate Newton step : > MATH > MATH 계산 > MATH 에 대해 MATH 계산 > MATH Select step length MATH with Backtracking > MATH Primal-Dual update : > MATH > 4. Termination condition : Stop if the conditions MATH and MATH are satisfied The algorithm calculates MATH by executing Newton step at each stage and obtains MATH by performing Primal-Dual updates. However, MATH is selected through Backtracking line search so that the Primal-Dual variables become feasible. The algorithm terminates when the surrogate duality gap and primal and dual residuals become smaller than MATH . Backtracking line search Since the Primal-Dual algorithm executes Newton step only once, it can be viewed as finding the direction of the solution rather than finding the exact solution. Therefore, an appropriate step length must be found so that moving in that direction can enter the feasible set. That is, at each step of the algorithm, MATH is obtained to update the primal-dual variables. > MATH This process has two main objectives. Maintaining the condition MATH Decreasing MATH For this purpose, multi-stage backtracking line search is used. Stage 1: dual feasibility MATH Initially, we start with the largest step MATH that satisfies MATH . > MATH \\begin align &u + \\theta \\Delta u && \\ge 0 \\\\\\\\ \\Leftrightarrow \\quad &u && \\ge -\\theta \\Delta u \\\\\\\\ \\Leftrightarrow \\quad &- u/\\Delta u && \\ge \\theta \\quad \\text such that -\\Delta u \\gt 0 \\\\\\\\ \\end align MATH This is the process of making MATH feasible. Stage 2: primal feasibility MATH Next, with parameters MATH and MATH set to MATH , the following update is performed. Update MATH until MATH h i x^+ This is the process of making MATH feasible. Stage 3 : reduce MATH Update MATH until MATH is satisfied The update equation in Stage 3 is the same as the existing backtracking line search algorithm. The right-hand side of the above equation can be derived as follows. First, we obtain the following result from Newton's method. > MATH \\begin align \\Delta w = \\Delta x, \\Delta u, \\Delta v &\\approx -r^ ' w ^ -1 r w \\\\\\\\ \\Leftrightarrow r w &\\approx -r^ ' w \\Delta w \\\\\\\\ \\end align MATH Since MATH in the above equation, we substitute this into the first-order Taylor approximation below. > MATH \\begin align r w + \\theta \\Delta w & \\approx r w + r^ ' w \\theta \\Delta w \\\\\\\\ &\\approx 1-\\theta r w \\\\\\\\ \\end align MATH As a result, we get MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_02_03_primal_dual_algorithm/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_03_some_history",
    "title": "17-03 Some history",
    "chapter": "17",
    "order": 7,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "Generally, modern state-of-the-art LP Solvers use both Simplex method and interior-point method. Dantzig 1940s : Simplex method, the first method to solve the general form of LP, obtaining exact solutions without iteration. It remains one of the best-known and most studied algorithms for LP to this day. Klee and Minty 1972 : A pathological LP with MATH variables and MATH constraints. Solving with the Simplex method requires MATH iterations. Khachiyan 1979 : A polynomial-time algorithm for LP based on the ellipsoid method of Nemirovski and Yudin 1976 , which is theoretically strong but not so in practice. Karmarkar 1984 : An interior-point polynomial-time LP method that is quite effective and became a breakthrough research. US Patent 4,744,026, expired in 2006 . Renegar 1988 : Newton-based interior-point algorithm for LP. It had the theoretically best computational complexity until the latest research by Lee-Sidford emerged.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_03_some_history/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_04_special_case_linear_programming",
    "title": "17-04 Special case, linear programming",
    "chapter": "17",
    "order": 8,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this section, let's look at an example of the Primal-Dual method for LP linear programming problems. Linear programming Consider the following primal LP problem. > MATH >\\begin align > &\\min x && c^Tx \\\\\\\\ > &\\text subject to && Ax = b \\\\\\\\ > & && x ≥ 0 \\\\\\ >\\end align > MATH > > MATH The dual problem of the above primal LP problem is as follows. > MATH >\\begin align > &\\max y,s && b^Ty \\\\\\\\ > &\\text subject to && A^Ty + s = c \\\\\\\\ > & && s ≥ 0 \\\\\\ >\\end align > MATH Optimality conditions and central path equations The following shows the optimality conditions KKT Conditions for the primal-dual problem of the previous LP. > MATH > \\begin array rcl > A^Ty + s & = & c \\\\\\ > Ax & = & b \\\\\\ > XS\\mathbb 1 & = & 0 \\\\\\ > x,s & \\succeq & 0 > \\end array > MATH Central path equations > MATH > \\begin array rcl > A^Ty + s & = & c \\\\\\ > Ax & = & b \\\\\\ > XS\\mathbb 1 & = & τ\\mathbb 1 \\\\\\ > x,s & > & 0 > \\end array > MATH Primal-dual method vs. barrier method Newton steps for primer-dual method The following is the Newton equation for the primal-dual method for LP problems. > MATH \\begin bmatrix 0 & A^T & I \\\\\\ A & 0 & 0 \\\\\\ S & 0 & X \\end bmatrix \\begin bmatrix ∆x \\\\\\ ∆y \\\\\\ ∆s \\end bmatrix = − \\begin bmatrix A^Ty + s−c \\\\\\ Ax−b \\\\\\ XS\\mathbb 1 −τ\\mathbb 1 \\end bmatrix MATH From the optimal condition, we can know the following relationship. MATH Accordingly, we can obtain optimal conditions for the primal barrier problem by removing MATH , or obtain optimal conditions for the dual barrier problem by removing MATH . Newton steps for barrier problems The following are the primal and dual central path equations for the barrier problem. Left is primal, right is dual > MATH > \\begin array rcr > A^Ty + τX^ −1 1 & = & c & \\qquad \\qquad & A^Ty + s & = & c \\\\\\ > Ax & = & b & \\qquad \\qquad & τAS^ −1 \\mathbb 1 & = & b\\\\\\ > x & > & 0 & \\qquad \\qquad & s & > & 0 > \\end array > MATH > Using the above central path equations, the Newton steps for primal and dual are as follows. Primal Newton step > MATH \\begin bmatrix τX^ −2 & A^T \\\\\\ A & 0 \\end bmatrix \\begin bmatrix ∆x \\\\\\ ∆y \\end bmatrix = − \\begin bmatrix A^Ty + τX^ −1 \\mathbb 1 −c \\\\\\ Ax−b \\end bmatrix MATH Dual Newton step > MATH \\begin bmatrix A^T & I \\\\\\ 0 & τAS^ −2 \\end bmatrix \\begin bmatrix ∆y \\\\\\ ∆s \\end bmatrix = − \\begin bmatrix A^Ty + s −c \\\\\\ τAS^ −1 \\mathbb 1 −b \\end bmatrix MATH Example: barrier versus primal-dual Standard LP : MATH , MATH To verify the performance of the Primal-dual method, let's look at an example of a standard LP problem with MATH variables and MATH equality constraints. Example from B & V 11.3.2 and 11.7.4 The Barrier method used various MATH values 2, 50, 150 while the primal-dual method fixed MATH at 10. Both methods used MATH . Fig1 Duality gap Barrier vs. Primal-dual 1 As can be seen from the graph, primal-dual converges quickly while showing high accuracy. Sequence of problem : MATH and MATH growing. Now let's look at the performance for a series of problems where MATH and MATH gradually increases. The Barrier method used MATH and the outer loop was performed only about 2 times. The duality gap was reduced to MATH The Primal-dual method used MATH and stopped execution when the duality gap and feasibility gap were approximately MATH . Fig2 Newton iteration Barrier vs. Primal-dual 1 As can be seen from the above figure, the Primal-dual method finds solutions with higher accuracy but requires some additional iterations.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_04_special_case_linear_programming/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter17/17_05_optimality_conditions_for_semidefinite_programming",
    "title": "17-05 Optimality conditions for semidefinite programming",
    "chapter": "17",
    "order": 9,
    "owner": "Minjoo Lee",
    "lesson_type": "",
    "content": "In this section, we want to look at an example of the Primal-Dual method for SDP semidefinite programming problems. SDP semidefinite programming The primal problem of SDP is defined as follows. > MATH >\\begin align > &\\min x && C \\cdot X \\\\\\\\ > &\\text subject to && A i \\cdot X = b i, i = 1,...,m \\\\\\\\ > & && X \\succeq 0 >\\end align > MATH The dual problem of SDP is defined as follows. > MATH >\\begin align > &\\max y && b^Ty \\\\\\\\ > &\\text subject to && \\sum^m X i=1 y iA i + S = C \\\\\\\\ > & && S \\succeq 0 >\\end align > MATH For reference, the trace inner product of MATH is denoted as follows. > MATH Optimality conditions for SDP The primal and dual problems of SDP can be defined using linear maps as follows. > MATH >\\begin align > &\\min x && C \\cdot X & \\qquad \\qquad \\qquad & \\max y,S && b^Ty \\\\\\\\ > &\\text subject to && \\mathcal A X = b & \\qquad \\qquad \\qquad & \\text subject to && \\mathcal A ^ ∗ y + S = C \\\\\\\\\\ > & && X \\succeq 0 & \\qquad \\qquad \\qquad & && S \\succeq 0 >\\end align > MATH Here MATH means a linear map. Assuming strong duality is satisfied, MATH and MATH where MATH is a solution are optimal solutions for primal and dual, and vice versa. > MATH > \\begin array rcl > \\mathcal A ^∗ y + S & = & C \\\\\\ > \\mathcal A X & = & b \\\\\\ > XS & = & 0 \\\\\\ > X,S & \\succeq & 0 > \\end array > MATH Central path for SDP Primal barrier problem > MATH >\\begin align > &\\min x && C \\cdot X−τ \\log det X \\\\\\\\ > &\\text subject to && A X = b >\\end align > MATH Dual barrier problem > MATH >\\begin align > &\\max y, S && b^Ty + τ \\log det S \\\\\\\\ > &\\text subject to && \\mathcal A ^∗ y + S = C >\\end align > MATH Primal & dual을 위한 Optimality conditions > MATH > \\begin array rcl > \\mathcal A ^∗ y + S & = & C \\\\\\ > \\mathcal A X & = & b \\\\\\ > XS & = & τI \\\\\\ > X,S & \\succ & 0 > \\end array > MATH Newton step Primal central path equations > MATH > \\begin array rcl > \\mathcal A ^∗ y + \\tau X^ −1 & = & C \\\\\\ > \\mathcal A X & = & b \\\\\\ > X & \\succ & 0 > \\end array > MATH Newton equations > MATH > MATH The central path equation and Newton equation for the dual are similarly defined including MATH . Primal-dual Newton step Primal central path equations > MATH \\begin bmatrix \\mathcal A ^∗ y + S - C \\\\\\ \\mathcal A X - b \\\\\\ XS \\end bmatrix = \\begin bmatrix 0 \\\\\\ 0 \\\\\\ τI \\end bmatrix , X, S \\succ 0 MATH Newton step: > MATH \\begin bmatrix 0 & \\mathcal A ^∗ & I \\\\\\ \\mathcal A & 0 & 0 \\\\\\ S & 0 & X \\end bmatrix \\begin bmatrix \\Delta X \\\\\\ \\Delta y \\\\\\ \\Delta S \\end bmatrix = − \\begin bmatrix \\mathcal A ^∗ y + s−c \\\\\\ \\mathcal A x − b \\\\\\ XS − \\tau I \\end bmatrix MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter17/17_05_optimality_conditions_for_semidefinite_programming/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter18/18_00_Quasi_Newton_methods",
    "title": "18-00 Quasi-Newton methods",
    "chapter": "18",
    "order": 1,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "In the mid-1950s, physicist W.C. Davidon, who was working at Argonne National Laboratory, was solving optimization problems with large computational requirements using coordinate descent methods. Unfortunately, due to the instability of computers at the time, system crashes frequently occurred before computations could be completed, and frustrated by this, Davidon decided to find methods that could improve computational speed. This led to the birth of the first Quasi-Newton algorithm. This became a catalyst for dramatic progress in nonlinear optimization, and subsequently, various follow-up studies on this method emerged over the next 20 years. Ironically, Davidon's research http://www.math.mcgill.ca/dstephens/680/Papers/Davidon91.pdf was not published at the time and remained as a technical report for more than 30 years. It was finally published in the first issue of SIAM Journal on Optimization https://epubs.siam.org/toc/sjope8/1/1 in 1991. Quasi-Newton methods require only the gradient of the objective function at each iteration. This has much less computational burden than Newton methods that require second derivatives, and moreover shows superlinear convergence, making it a sufficiently attractive method 14 . Motivation for Quasi-Newton methods Consider the following unconstrained, smooth optimization problem: > MATH > \\min x \\: f x , \\\\\\\\ > \\text where f \\text is twice differentiable, and dom \\; f = \\mathbb R ^n. > MATH The update methods for x in Gradient descent method and Newton's method for the above problem are as follows: > Gradient descent method: > MATH >x^+ = x - t \\nabla f x > MATH > Newton's method: > MATH >x^+ = x - t \\nabla^2 f x ^ -1 \\nabla f x > MATH Newton's method has the advantage of showing quadratic convergence rate MATH , but there are problems with significantly high computational costs in the following two processes: Computing the Hessian MATH : Computing and storing the Hessian matrix requires MATH memory. This is not suitable performance for handling high-dimensional functions. reference: Hessian matrix https://en.wikipedia.org/wiki/Hessian matrix Use in optimization in Wikipedia Solving the equation MATH : To solve this equation, we must compute the inverse matrix of the Hessian MATH . reference: Computational complexity of Matrix algebra https://en.wikipedia.org/wiki/Computational complexity of mathematical operations Matrix algebra in Wikipedia Quasi-Newton methods use an approximation MATH instead of MATH . > Quasi-Newton method: > MATH >x^+ = x + tp \\\\\\\\ >\\text where Bp = -\\nabla f x . > MATH Here, B should be easy to compute, and it should also be easy to solve the equation MATH . Quasi-Newton Algorithm The Quasi-Newton algorithm is as follows: Pick initial MATH and MATH For MATH Solve MATH Pick MATH and let MATH Update MATH to MATH End for A major characteristic of this method is updating MATH so that we can gradually approach the optimal point. That is, the method of obtaining the next step MATH from MATH will be the main topic of discussion in this chapter. Note: For convenience, we will use the two notations MATH and MATH interchangeably.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter18/18_00_Quasi_Newton_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter18/18_01_Secant_Equation_and_Curvature_Condition",
    "title": "18-01 Secant Equation and Curvature Condition",
    "chapter": "18",
    "order": 2,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Secant Equation As mentioned earlier, MATH is a matrix that approximates MATH . For matrix MATH to have similar properties to the Hessian MATH , it must satisfy a condition called the secant equation. When MATH and MATH is twice differentiable, the first-order Taylor expansion of MATH shows that the true Hessian has the following property. > MATH Here, we call the approximation matrix of MATH as MATH . This matrix satisfies the following equation. > MATH If MATH , then the above equation can be rearranged as follows, and this is called the secant equation. > MATH >B^ k+1 s^k = y^k > MATH The Intuition of Secant Equation MATH axis은 MATH 를, MATH axis은 MATH 를 나타낸다고 할when, MATH 은 MATH and, MATH 를 통and,하는 직선의 기울기and, 같다. Fig1 The intuition of secant equation Conditions to Determine MATH matrix MATH 를 basis,with, computation된 MATH 는 다음의 3가지 condition,을 만족solution야한다. 1. MATH is symmetric: Because it is an approximation of the Hessian. 2. MATH close to MATH : A condition to determine a unique MATH . Since MATH already contains useful information, we choose the matrix closest to MATH among those MATH that satisfy the secant equation. 3. MATH is positive definite MATH is positive definite: To guarantee global optimum, we maintain the convexity of the problem. reference: Analyzing the hessian https://web.stanford.edu/group/sisl/k12/optimization/MO-unit4-pdfs/4.10applicationsofhessians.pdf Curvature Condition The fact that MATH is positive definite and MATH implies the following fact. > MATH reference: positive definite in WikiPedia https://en.wikipedia.org/wiki/Positive-definite matrix Here, MATH is called the curvature condition. If the curvature condition is satisfied, the secant equation MATH always has a solution MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter18/18_01_Secant_Equation_and_Curvature_Condition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter18/18_02_Symmetric_Rank_One_Update_(SR1)",
    "title": "18-02 Symmetric Rank-One Update (SR1)",
    "chapter": "18",
    "order": 3,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "The SR1 update is a method that updates MATH with a rank-1 symmetric matrix so that MATH maintains symmetry and continues to satisfy the secant equation. If a rank-1 symmetric matrix is decomposed as a product of MATH and MATH , the update form would be as follows. > MATH Key Observation MATH and MATH must be chosen so that MATH satisfies the secant equation. Thus, let's substitute the update form introduced above into the secant equation MATH . > MATH Since MATH is a scalar, MATH can also be expressed as a product of MATH and an arbitrary scalar MATH MATH . Let's substitute MATH in 1 with MATH . > MATH The parameters MATH and MATH that satisfy the above equation are as follows. > MATH The Only SR1 Updating Formula Using the information obtained from the key observation, we can derive the unique form of SR1 update 14 section 6.2 . MATH and substituting 2 into MATH . MATH > MATH >B^+ = B + \\frac y-Bs y-Bs ^T y-Bs ^Ts . > MATH > The Update Formula for the Inverse Hessian Approximation To find MATH , we need to compute MATH . > MATH If we can update MATH instead of MATH , wouldn't we be able to reduce the cost of computing MATH every time? Using the Sherman–Morrison formula https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison formula , we can see through the derivation process that MATH can also be updated in the same form. MATH > MATH >H^+ = H + \\frac s-Hy s-Hy ^T s-Hy ^Ty . > MATH Shortcomings of SR1 SR1 has the advantage of being very simple, but it has two critical shortcomings. 1. The update can fail when MATH approaches 0. 2. It cannot maintain the positive definiteness of MATH and MATH . The following sections introduce methods that complement the shortcomings of SR1.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter18/18_02_Symmetric_Rank_One_Update_(SR1)/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter18/18_03_Davidon_Fletcher_Powell_(DFP)_Update",
    "title": "18-03 Davidon-Fletcher-Powell (DFP) Update",
    "chapter": "18",
    "order": 4,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "The DFP update is a method that updates MATH with a rank-2 symmetric matrix. > MATH If MATH computed through the DFP update satisfies the secant equation, then MATH can be expressed as a linear combination of MATH and MATH . reference: by the secant equation, MATH > MATH > > MATH Setting MATH and solving for MATH and MATH , we derive the updating formula for MATH . > MATH > H^+ = H - \\frac Hyy^TH y^THy + \\frac ss^T y^Ts > MATH Similar to the SR1 update, we can derive the updating formula for MATH using the Sherman–Morrison formula https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison formula . > MATH >\\begin align >B^+ &= B + \\frac y-Bs y^T y^Ts + \\frac y y-Bs ^T y^Ts - \\frac y-Bs ^Ts y^Ts ^2 yy^T\\\\\\\\ > &= \\big I - \\frac ys^T y^Ts \\big B \\big I - \\frac sy^T y^Ts \\big + \\frac yy^T y^Ts >\\end align > MATH If MATH is positive definite, then MATH becomes positive semidefinite. In this case, if MATH is positive definite, then MATH is guaranteed to be positive definite. This solves the problem of maintaining positive definiteness that was raised with SR1. DFP Update - Alternate Derivation Recall: if the curvature condition MATH is satisfied, then there exists a symmetric positive definite matrix that satisfies the secant equation. The DFP update can also be derived by solving the problem of minimizing the weighted Frobenius norm between matrix MATH and MATH where MATH 1. satisfies symmetry and 2. satisfies the secant equation. Each different matrix norm corresponds to each different Quasi-Newton method. Among them, the norm that makes it easy to solve this problem while also making it work as a scale-invariant optimization method is the weighted Frobenius norm. >Solve > MATH >\\begin align >& \\min B^+ \\: \\: && \\|W^ 1/2 B^+ - B W^ 1/2 \\| F \\\\\\\\ >& \\text subject to && B^+ = B^+ ^T \\\\\\\\ > &&& B^+s = y \\\\\\\\ >& \\text where && W \\in \\mathbb R ^ n \\; \\times \\;n \\text is nonsingular and such that Wy k = s k. >\\end align \\\\\\\\ > MATH reference : Frobenius norm: The Frobenius norm of matrix MATH is defined as follows. MATH \\| A \\| F \\doteq \\sum i,j A i,j ^2 ^ 1/2 MATH Weighted Frobenius norm: The weighted Frobenius norm of matrix MATH with weight matrix MATH is defined as follows. MATH \\|A\\| W \\doteq \\| W^ 1/2 A W^ 1/2 \\| F MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter18/18_03_Davidon_Fletcher_Powell_(DFP)_Update/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter18/18_04_Broyden_Fletcher_Goldfarb_Shanno_(BFGS)_update",
    "title": "18-04 Broyden-Fletcher-Goldfarb-Shanno (BFGS) update",
    "chapter": "18",
    "order": 5,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "The idea of BFGS is the same as DFP. The only difference is that the roles of B and H are reversed. BFGS is derived by solving the following problem. >Solve > MATH >\\begin align >& \\min H^+ \\: \\: && \\|W^ 1/2 H^+ - H W^ 1/2 \\| F \\\\\\\\ >& \\text subject to && H^+ = H^+ ^T \\\\\\\\ >&&& H^+s = y \\\\\\\\ >& \\text where && W \\in \\mathbb R ^ n \\; \\times \\;n \\text is nonsingular and such that Ws k = y k. >\\end align \\\\\\\\ > MATH The derived updating formulas for MATH and MATH are as follows. > MATH > B^+ = B - \\frac Bss^TB s^TBs + \\frac yy^T y^Ts > MATH and > MATH >\\begin align >H^+ &= H + \\frac s-Hy s^T y^Ts + \\frac s s-Hy ^T y^Ts - \\frac s-Hy ^Ty y^Ts ^2 ss^T\\\\\\\\ > &= \\big I - \\frac sy^T y^Ts \\big H \\big I - \\frac ys^T y^Ts \\big + \\frac ss^T y^Ts >\\end align > MATH BFGS also, DFP처럼 positive definiteness를 유지한다. if, MATH 가 positive definite이고 MATH 이면 MATH 는 positive definite이다. BFGS의 특장점은 self-correcting property를 지니고 있다는 것이다. if, matrix MATH 가 부정확하게 추정되어 iteration의 속도가 느려지게 되면 Hessian approximation이 단 몇 step 만to, 이를 바to,잡는 경향성을 보인다. 반면 DFP는 잘못된 Hessian approximation의 추정about, effect,적with, 바to,잡지 못하므to, 실전at,는 usually, BFGS의 성능이 더 좋은 편이다 14 .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter18/18_04_Broyden_Fletcher_Goldfarb_Shanno_(BFGS)_update/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter18/18_05_The_Broyden_Class",
    "title": "18-05 The Broyden Class",
    "chapter": "18",
    "order": 6,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "The Broyden class generalizes BFGS, DFP, and SR1 with the following formula. Note: MATH and, MATH 는 각각 BFGSand, DFPby, 유도되는 MATH 다. > MATH >B^+ = 1 - \\phi B^+ \\text BFGS + \\phi B^+ \\text DFP , \\text for \\phi \\in \\mathbb R . > MATH MATH 를 MATH to, 정의하면 위 공식은 아래and, 같이 정리된다. > MATH >B^+ = B - \\frac Bss^TB s^TBs + \\frac yy^T y^Ts + \\phi s^TBs vv^T. > MATH Observe: MATH 일when,, 위 update는 BFGSand, 동일solution진다. MATH 일when,, 위 update는 DFPand, 동일solution진다. MATH 일when,, 위 update는 SR1and, 동일solution진다. reference : MATH 의 범위를 MATH to, 제한한 특수한 case,를 restricted Broyden class라 부른다 14 .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter18/18_05_The_Broyden_Class/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter18/18_06_Superlinear_convergence",
    "title": "18-06 Superlinear convergence",
    "chapter": "18",
    "order": 7,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Assumption1: > The Hessian matrix MATH is Lipschitz continuous at MATH , that is, > MATH > for all MATH near MATH , where MATH is a positive constant. Assumption2: Wolfe conditions > Assume MATH is chosen via backtracking so that > MATH > and > MATH > for MATH >\\lim k \\rightarrow \\infty \\frac \\| x^ k+1 - x^\\ast \\| \\| x^k - x^\\ast \\| = 0. > MATH Theorem Dennis-Moré 다음은 Quasi-Newton method의 search direction이 Newton direction을 충분히 잘 approximation하고 있을when,, solutionwith, convergence하는 processat, step length가 Wolfe conditions를 만족함을 보인다. Superlinear convergence를 보이기 for, search direction이 만족solution야하는 condition,이라고도 할 수 있다 14 . > MATH 가 두 번 미분 가능하고 MATH s.t. MATH 이며 MATH 가 positive definite이라고 let's assume. > > MATH > >if, search direction MATH 가 위 condition,을 만족하면, 다음 두 가지 항목을 만족하는 MATH 가 존재한다. > > 1. MATH about, step length MATH 은 Wolfe conditions를 만족한다. > 2. if, MATH about, MATH 이면 MATH 는 superlinear convergence를 보인다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter18/18_06_Superlinear_convergence/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter18/18_07_Limited_Memory_BFGS_(LBFGS)",
    "title": "18-07 Limited Memory BFGS (LBFGS)",
    "chapter": "18",
    "order": 8,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Introduction LBFGS is an example of Limited-memory quasi-Newton methods, and is useful when the cost of computing or storing the Hessian matrix is not reasonable. This method estimates approximates the Hessian matrix by maintaining only a few MATH -dimensional vectors instead of storing a dense MATH Hessian matrix. The LBFGS algorithm is based on BFGS, as its name suggests. The main idea is to use curvature information from the most recent iterations to estimate the Hessian. On the other hand, curvature information from older iterations is not used to save storage space, as it may be somewhat distant from the behavior shown by the Hessian of the current iteration. As a side note, limited-memory versions of other quasi-Newton algorithms e.g., SR1 can also be derived using the same technique 14 . LBFGS LBFGS를 본격적with, 설명하기to, 앞서 BFGS methodabout, 다시 let's look at. 각 stepat, BFGS는 as follows: MATH 를 업데이트 한다. > MATH >x^+ = x - t H \\nabla f, \\\\\\\\ >\\text where t \\text is the step length and H \\text is updated at every iteration by means of the formula, \\\\\\\\ >\\text \\\\\\\\ >H^+ = \\big I - \\frac sy^T y^Ts \\big H \\big I - \\frac ys^T y^Ts \\big + \\frac ss^T y^Ts .\\\\\\\\ > MATH MATH to, about, 업데이트 식을 이용하면 MATH 를 임의의 scalar MATH and, 임의의 vector MATH 를 using, 표현할 수 있다. > MATH >\\begin align >H^+q &= \\big I - \\frac sy^T y^Ts \\big H \\big I - \\frac ys^T y^Ts \\big q + \\frac ss^Tq y^Ts \\\\\\\\ > &= \\big I - \\frac sy^T y^Ts \\big \\underbrace H \\\\big q - \\frac s^T q y^Ts y \\big p + \\underbrace \\frac s^Tq y^Ts \\alpha s\\\\\\\\ > &= \\big I - \\frac sy^T y^Ts \\big p + \\alpha s\\\\\\\\ > &= p - \\underbrace \\frac y^Tp y^Ts \\beta s + \\alpha s \\\\\\\\ > &= p + \\alpha - \\beta s,\\\\\\\\ >& \\text where \\alpha = \\frac s^Tq y^Ts , q^+ = q - \\alpha y, p = Hq, \\beta = \\frac y^Tp y^Ts . >\\end align \\\\\\\\ > MATH MATH 가 k번의 BFGS update를 through, 얻이진다고 할when,, MATH 는 length k의 iteration문 2개to, computation할 수 있다 아래 algorithm reference . 단, 메모리의 효율적인 사용을 for, 가장 최근 MATH 개 iterationsat,의 curvature information만을 이용한다. MATH Algorithm Fig1 The algorithm of LBFGS 3 usually, inverse Hessian approximation MATH 는 dense하며, variable의 개수가 많은 case, 저장 및 operation 비용이 매우 높아지게 된다. 반면 LBFGS는 MATH 을 연속한 vectorsumand, vectorproductwith, obtaining,냄with,써 MATH 의 computation 및 유지를 위한 비용problem를 완화시킬 수 있다. 뿐만 아니라 이 computationto, 사용되는 initial Hessian approximation MATH 는 usually, 실전at, 매우 effect,적with, 작동한다고 검증된 identity matrixto, 어떤 constant를 product한 형태 MATH 를 띄기 because of, 유지 및 computationto, 그다지 큰 비용이 발생하지 않는다 14 의 7.2 . > MATH > H^ 0,k = \\gamma k I, \\\\\\\\ > \\text where \\: \\gamma k = \\frac s^T k-1 y k-1 y^T k-1 y k-1 . > MATH Note: MATH 는 매 iteration마다 다르게 선택될 수 있다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter18/18_07_Limited_Memory_BFGS_(LBFGS)/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_00_Proximal_Newton_Method",
    "title": "19 Proximal Newton Method",
    "chapter": "19",
    "order": 1,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "In this chapter, we will examine Proximal Newton Method , Proximal quasi-Newton method , and Projected Newton method . Reference Papers Proximal Newton method: J. Friedman and T. Hastie and R. Tibshirani 2009 , \"Regularization paths for generalized linear models via coordinate descent\" C.J. Hsiesh and M.A. Sustik and I. Dhillon and P. Ravikumar 2011 , \"Sparse inverse covariance matrix estimation using quadratic approximation\" M. Patriksson 1998 , \"Cost approximation: a unified framework of descent algorithms for nonlinear programs\" J. Lee and Y. Sun and M. Saunders 2014 , \"Proximal Newton-type methods for minimizing composite functions\" P. Tseng and S. Yun 2009 , \"A coordinate gradient descent method for nonsmooth separable minimization\" Projected Newton method: A. Barbero and S. Sra 2011 , \"Fast Newton-type methods for total variation regularization\" D. Bertsekas 1982 , \"Projected Newton methods for optimization problems with simple constraints\" D. Kim and S. Sra. and I. Dhillon 2010 , \"Tackling box-constrained optimization via a new projected quasi-Newton approach\" M. Schmidt and D. Kim and S. Sra 2011 , \"Projected Newton-type methods in machine learning\"",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_00_Proximal_Newton_Method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_01_00_Proximal_Newton_method",
    "title": "19-01 Proximal Newton method",
    "chapter": "19",
    "order": 2,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "In this section, we will review the proximal gradient method and examine how the proximal newton method emerged from it. We will also examine the definition of the proximal newton method and the scaled proximal map that has good properties such as uniqueness and non-expansiveness compared to the general proximal map.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_01_00_Proximal_Newton_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_01_01_Reminder:_proximal_gradient_descent",
    "title": "19-01-01 Reminder - proximal gradient descent",
    "chapter": "19",
    "order": 3,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Before examining the Proximal newton method that we will learn in this chapter, let's first review Proximal gradient descent . For detailed information, see 09 Proximal Gradient Descent and Acceleration % multilang post url contents/chapter09/20-01-08-09 proximal gradient descent and acceleration % see. Proximal gradient descent Proximal gradient descent works on the following problem. > MATH MATH is convex and differentiable. dom MATH MATH is convex and non-differentiable and \"simple\". Algorithm Proximal gradient descent는 시작점 MATH at, 시작solution서 다음 process을 iteration한다. > MATH 여기서 MATH 는 MATH and, association,된 proximal operator 이다. > \\begin align \\text prox t x = \\underset z \\text argmin \\frac 1 2t \\parallel x - z \\parallel 2^2 + h z \\end align Update 식은 generalized gradient MATH 를 using,서 표준화된 형태to, 표현할 수도 있다. > \\begin align > x^ k = x^ k-1 - t k \\cdot G t k x^ k-1 , \\space \\space \\text where \\space G t x = \\frac x-\\text prox t x - t \\nabla g x t \\\\\\\\ > \\end align Performance Proximal gradient descent 의 성능은 MATH according to, 달라질 수 있다. if,, MATH 가 복잡한 function이고 particularly, closed form이 아니라면 minimize할 when, computation을 많이 solution야 하므to, 성능이 매우 떨어질 수 있다. also,, MATH function의 convergence rateand, 같은 convergence 속도를 갖는다. 단, iteration할 when,마다 prox operator를 실행하기 because of, prox computation이 효율적인 case,to,만 유용하다. Motivation Proximal gradient descent at,는 미분 가능한 function MATH 를 Taylor 2difference식with, approximation하고 여기to, 미분이 되지 않는 function인 MATH 를 더하여 목적 functionto, 정의한 후 이를 iteration적with, minimization한다. therefore,, as follows: 2difference 식with, 정리solution 볼 수 있다. 식to, 전개되는 자세한 process은 09-01 Proximal gradient descent % multilang post url contents/chapter09/20-01-08-09 01 proximal gradient descent % reference. > MATH > \\begin align x^+ & = \\underset z \\text argmin \\, \\frac 1 2t \\parallel x - t \\nabla g x - z \\parallel 2 ^2 + h z \\\\\\\\ > & = \\underset z \\text argmin \\ \\nabla g x ^T z - x + \\frac 1 2t \\parallel z - x \\parallel 2 ^2 + h z \\\\\\\\ > \\end align > MATH 두번째 식의 1항and, 2항은 MATH 의 Taylor 2difference approximation식with, from, 유도할 수 있는데, first, constant항 MATH 은 제거하고 gradient descentat,and, 마찬가지to, Hessian MATH 을 MATH spherical curvature to, 대체solution서 구할 수 있다. 다음 그림at,는 proximal gradient descent의 update stepat, MATH 를 2difference approximation식with, minimization 하는 process을 showing,주고 있다. Fig 1 Proximal gradient descent updates 3 Gradient descentand, newton's method의 difference이점는 2difference approximation를 할 when, function의 local hessian인 MATH 를 사용하는지 여부이다. 그렇다면, 위의 식at, MATH instead, MATH 를 사용하면 어떻게 될까? 이것이 바to, 다음 절at, 설명하게 될 proximal newton method 가 나오게 된 background,이다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_01_01_Reminder-_proximal_gradient_descent/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_01_02_Proximal_Newton_method",
    "title": "19-01-02 Proximal Newton method",
    "chapter": "19",
    "order": 4,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "In the previous section, we explained that the proximal newton method is a method that wants to use the local hessian MATH instead of the spherical curvature MATH in the proximal gradient descent formula. The proximal newton method is an old idea that is being studied in statistics under the term local score. Now let's look at how the proximal newton method can be formulated. Algorithm The Proximal gradient descent algorithm consists of the process of finding the direction MATH of the next step and then optimizing the step size MATH . Step 1: Starting from the starting point MATH , iterate the following process. MATH Step 2: Find the direction MATH of the next step. > \\begin align v^ k & = \\underset v \\text argmin \\ \\nabla g x^ k-1 ^T v + \\frac 1 2 v^T H^ k-1 v + h x^ k-1 + v \\end align 여기서 MATH 은 MATH at,의 Hessian이다. 3step : MATH directionwith, step을 이동하기 for, step size를 optimization한다. > \\begin align x^ k & =x^ k-1 + t k v^ k \\end align MATH 는 step sizeto, MATH 이면 pure proximal Newton method이다. Backtracking line search를 through, step size를 optimization하는 process이 있다는 점은 proximal gradient descent methodand, 다른 점이다. Next position view 위의 식을 direction MATH 이 아닌 다음 position인 MATH 의 관점at, 표현하면 as follows:. > MATH > \\begin align > z^ k & = \\underset z \\text argmin \\ \\nabla g x^ k-1 ^T z - x^ k-1 ^T + \\frac 1 2 z - x^ k-1 ^T H^ k-1 z - x^ k-1 + h z \\\\\\\\ > x^ k & =x^ k-1 + t k z^ k - x^ k-1 > \\end align > MATH 직관적with, 첫번째 stepat, 목적 function를 minimization 하는 surrogate point인 MATH 를 구한다. 그런 다음, MATH at, MATH 의 directionwith, 이동but, always, MATH to, 이동하게 되는 것은 아니다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_01_02_Proximal_Newton_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_01_03_Scaled_proximal_map",
    "title": "19-01-03 Scaled proximal map",
    "chapter": "19",
    "order": 5,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Proximal newton method 를 proximal gradient descent and, 같은 형식with, 다시 작성solution 볼 수 있다. Scaled proximal map if, MATH 라고 하면 scaled proximal map 은 as follows: 정의된다. > \\begin align \\text prox t x = \\underset z \\text argmin \\frac 1 2 \\parallel x - z \\parallel H^2 + h z \\end align 여기서 MATH with, MATH 이다. MATH 일 when, 일반적인 unscaled proximal map 이 된다. generally, scaled proximal map 는 usually,의 prox보다 좋은 성질을 갖고 있다. uniqueness : solution가 하나만 존재하는 성질 MATH 이므to, strictly convex optimization problem이기 because of, 만족된다. non-expansiveness : 팽창하지 않는 성질 scaled proximal map이 non-expansive 성질을 갖는 projection operator의 일반화이기 because of, 만족된다. reference Projection operator의 non-expansiveness 두 점 MATH , MATH and, convex set MATH to, about, projection operator MATH about, non-expansiveness란 MATH 를 만족한다는 것을 의미한다. that is,, MATH 는 Lipschitz-1을 만족하며 MATH 가 convex일 case,to,만 만족한다. Fig 1 Projection onto a convex set C 3 Proximal newton update Scaled proximal map 을 using,서 Proximal newton update를 다시 expressing,보면 as follows:. > MATH > \\begin align > z^ + & = \\underset z \\text argmin \\nabla g x ^T z - x ^T v + \\frac 1 2 z - x ^T H z - x + h z \\\\\\\\ > & =\\underset z \\text argmin \\ \\frac 1 2 \\parallel x - H^ -1 \\nabla g x - z \\parallel H^2 + h z > \\end align > MATH 다르게 표현하면 as follows:. > MATH > \\begin align > z^ k & = \\text prox H^ k-1 x^ k-1 - H^ k-1 ^ -1 \\nabla g x^ k-1 \\\\\\\\ > x^ k & =x^ k-1 + t k z^ k - x^ k-1 > \\end align > MATH 직관적with, MATH about,서 newton step을 수행하고, MATH about, scaled prox operator를 applying,서 그 directionwith, 이동한다는 것을 의미한다. 이from, 다음and, 같은 사항을 알 수 있다. MATH 일when, proximal operator는 identity function가 되여 일반적인 Newton update가 된다. MATH 를 MATH to, 대체하고 MATH to, 두면 step size MATH about, proximal gradient update를 구할 수 있다. Prox의 어려움은 MATH 뿐만 아니라 MATH 의 hessian의 구조according to, 달라진다. for example, MATH 가 diagonal이거나 banded이면 dense한 MATH 일 case,to, 비solution problem가 매우 쉬워진다. therefore,, proximal Newton method는 proximal gradient descentand, Newton's method를 둘 다 일반화한 것임을 알 수 있다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_01_03_Scaled_proximal_map/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_02_Backtracking_line_search",
    "title": "19-02 Backtracking line search",
    "chapter": "19",
    "order": 6,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Proximal newton method may not converge in cases with pure step size MATH like newton's method. Therefore, we need to optimize the step size through backtracking line search. Backtracking line search algorithm 1. Initialize parameters. MATH 2. At each iteration, compute the Proximal newton direction as MATH . 3. Initialize MATH . 4. If the condition MATH is satisfied, reduce MATH . Iterate step 4 while this condition is satisfied. MATH 5. Execute the Proximal newton update MATH . 6. If the termination condition is not satisfied, go to step 2. Intuitively, we find a step size MATH such that we move along direction MATH to a position where the linear approximation of function MATH at MATH is reduced by a factor of MATH . Since the MATH part of MATH is not differentiable, we use the discrete derivative MATH . Efficiency of algorithm There are many methods for performing backtracking line search, and here we have introduced one of them. In this method, when computing MATH , the prox operator is computed only once. In the case of proximal gradient descent, the prox operator had to be computed iteratively in the inner loop, which is a distinctly different characteristic. Therefore, this method can perform backtracking line search very efficiently when the computation of the prox operator is complex. reference Method 별 backtracking line search Gradient descent 06-02-02 Backtracking line search % multilang post url contents/chapter06/21-03-20-06 02 02 backtracking line search % Proximal gradient descent 09-02 Convergence analysis % multilang post url contents/chapter09/20-01-08-09 02 convergence analysis % Newton's method 14-04 Backtracking line search % multilang post url contents/chapter14/2021-03-26-14 04 backtracking line search %",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_02_Backtracking_line_search/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_03_When_would_we_use_proximal_Newton",
    "title": "19-03 When would we use proximal Newton?",
    "chapter": "19",
    "order": 7,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "When should we use the proximal newton method? To understand the usefulness of the proximal newton method, let's compare the proximal newton method and proximal gradient descent on the following problem. Problem : MATH Proximal gradient descent vs. proximal newton | Proximal gradient descent | Proximal Newton | | -------- | -------- | | MATH minimization | MATH minimization | | Prox operator가 대부분 closed formwith, 정의됨 | Prox operator가 대부분 closed formwith, 정의되지 않음 | iteration이 저렴 | iteration이 아주 비쌈 newton method보다 비쌈 | | Gradient descent convergence 속도 MATH | Newton's method convergence 속도 MATH | 두 method은 비슷solution 보이지만 실제 매우 다른 일을 한다. therefore,, proximal newton method는 아주 적은 iteration을 기대할 수 있는 scaled prox operator quadratic + MATH to, about, 빠른 inner optimizer를 가질 when, 사용할 수 있다. MATH 가 separable function일 when, inner optimizerto, 가장 많이 사용되는 method이 coordinate descent이다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_03_When_would_we_use_proximal_Newton/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_04_Convergence_analysis",
    "title": "19-04 Convergence analysis",
    "chapter": "19",
    "order": 8,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "To analyze the convergence of the Proximal Newton method, we will follow the proof from Lee 2014 1 . 1 J. Lee and Y. Sun and M. Saunders 2014 , Proximal Newton-type methods for minimizing To prove convergence, we make the following assumptions: MATH , MATH and MATH are convex and MATH is twice differentiable smooth MATH . MATH is Lipschitz with constant MATH MATH can be computed exactly The above three assumptions imply that the function is strictly convex, and assuming that MATH can be computed exactly is because this is not easy to achieve in practice. Convergence Theorem > Proximal Newton method converges globally using backtracking line search. > \\begin align \\parallel x^ k - x^ \\star \\parallel 2 \\le \\frac M 2m \\parallel x^ k-1 - x^ \\star \\parallel 2^2 \\end align This is called local quadratic convergence . After MATH , to satisfy MATH , MATH iterations are needed. However, each iteration uses a scaled prox. Proof sketch To show global convergence , we can show that at any step, the backtracking exit condition for step size MATH is satisfied as follows. > \\begin align t \\le \\min \\left\\\\ 1, \\frac 2m L 1-\\alpha \\right\\\\ \\\\\\\\ \\end align With this equation, we can show that when the global minimum is reached, the update direction converges to 0. To show local quadratic convergence , after sufficiently many iterations, the pure Newton step MATH satisfies the backtracking exit conditions, and the following equation holds. > MATH > \\begin align > \\parallel x^ + - x^ \\star \\parallel 2 & \\le \\frac 1 \\sqrt m \\parallel x^ + - x^ \\star \\parallel H \\\\\\\\ > & = \\frac 1 \\sqrt m \\parallel \\text prox H x - H^ -1 \\nabla g x - \\text prox H x^ \\star - H^ -1 \\nabla g x^ \\star \\parallel H \\\\\\\\ > & \\le \\frac M 2m \\parallel x - x^ \\star \\parallel 2^2 \\\\\\\\ > \\end align > MATH Summarizing this, we get the following: > \\begin align \\parallel x^ + - x^ \\star \\parallel 2 \\ \\le \\ \\frac 1 \\sqrt m \\parallel x^ + - x^ \\star \\parallel H \\ \\le \\ \\frac M 2m \\parallel x - x^ \\star \\parallel 2^2 \\end align The first inequality holds due to the lowest eigenvalue bound, and the equality holds by the fact that MATH becomes the identity at the definition of MATH and global minimum MATH . The second inequality holds due to the nonexpansiveness of the proximal operator, the Lipschitz assumption, and the largest eigenvalue bound.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_04_Convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_05_Notable_examples",
    "title": "19-05 Notable examples",
    "chapter": "19",
    "order": 9,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Glmnet and QUIC Proximal newton method의 매우 유명한 패키지가 두 가지가 있다. glmnet Friedman et al., 2009 : MATH penalized generalized linear modelsto, about, prox Newton를 구현한 패키지. Coordinate descent를 using,서 inner problem을 푼다. QUIC Hsiesh et al., 2011 : graphical lasso problemto, about, prox Newton을 구현한 패키지. Factorization trick을 사용하고 coordinate descent를 using,서 inner problem을 푼다. 두 구현 패키지는 각자의 용도to, 맞춰서 매우 광범위하게 사용되고 있으며 state-of-the-art라고 할 수 있다. Proximal Newton method는 proximal gradient보다 MATH 의 gradient을 덜 자주 computation한다. therefore,, computation 비용이 커질수록 proximal newton이 유리하다. also,, inner solver를 신중하게 선택할수록 좋은 성능을 얻을 수 있다. Example: lasso logistic regression Lee et al. 2012 논문at, 제시된 예제를 let's look at. MATH regularized logistic regressionto,대solution 다음 세가지 methodabout,서 성능을 평가하였다. 1.FISTA : accelerated prox grad 2. spaRSA : spectral projected gradient method 3. PN : proximal Newton Dense hessian X n=5000, p=6000 예시 데이터 수 n = 5000, feature 개수 p = 6000인 dense feature matrix MATH 를 갖는 problemabout, 다음and, 같은 성능을 보였다. Hessian이 dense하기 because of, 매우 challenging한 problem라고 할 수 있다. Fig 1 Dense hessian X n=5000, p=6000 2 오른쪽은 function 호출 기준with,, 왼쪽은 시간 기준with, 평가한 것with,서, function 호출 기준with, 봤을 when,가 PN의 성능이 매우 우세함을 알 수 있다. 여기서 비용은 MATH and, MATH 를 computation하는 시간이 대부분이며 particularly, MATH and, MATH function를 computation하는 시간이 많이 들었다. Sparse hessian X n=542,000, p=47,000 예시 다음의 case,는 MATH 가 sparse하기 because of, MATH and, MATH 를 computation하는 시간이 덜 들었다. Fig 2 Sparse hessian X n=542,000, p=47,000 2 Inexact prox evaluations Proximal Newton methodat, proximal operation을 computation할 when, prox operator가 closed form이 아니기 because of, 정확히 computation하지 못한다. 그럼to,도 불구하고, 매우 높은 정확도를 갖는다면 매우 좋은 성질이 될 수 있다. Lee 2014 at,는 global convergenceand, local superlinear convergence를 보장하는 inner problem의 stopping rule을 제안했다. Three stopping rules Graphical lasso estimation problemto, inner optimizations을 위한 세 가지 stopping rules을 비교하였다. 이when,, 데이터 개수는 n = 72이고 feature 개수는 p = 1255이다. Fig 3 Three stopping rules 2 세 가지 stopping rule은 adaptive, maxiter = 10, exact이다. Maxiter는 inner iteration을 최대 10번to,만 하는 방식이고 exact는 정확한 solution를 구할 when,to, iteration하는 방식이다. Proximal newton method가 quadratic convergence를 만족하므to, exact는 quadratic convergence를 만족한다고 볼 수 있다. Maxiter=10의 case, 최대 10번의 inner iterationwith,는 quadratic convergence를 만족하지 못but, adaptive의 case, quadratic convergence를 만족하며 세 가지 방식 중 가장 빠르다. Stopping rule of usual newton method 일반적인 newton's methodat,는 inner problem은 MATH 의 MATH to, about, quadratic approximation인 MATH 를 minimization한다. and,, MATH 를 choosing,서 다음 condition,을 만족할 when, 중지한다. 이를 forcing sequence라고 한다. > \\begin align \\parallel \\nabla \\tilde g k-1 x^ k \\parallel 2 & \\le \\eta k \\parallel \\nabla g x^ k-1 \\parallel 2 \\\\\\\\ \\end align 이 condition,은 다음 positionat,의 gradient가 현재 positionat,의 gradient보다 MATH 배 만큼 작다는 것을 의미한다. 이when,, Quadratic approximation은 MATH 이다. Stopping rule of proximal gradient method Lee et al. 2012 at,는 proximal gradientat,는 gradient instead, generalized gradient를 사용하는 방식을 제안하였다. > MATH > \\begin align \\parallel G \\tilde f k-1 /M x^ k \\parallel 2 & \\le \\eta k \\parallel G f/M x^ k-1 \\parallel 2 \\end align > MATH 여기서 MATH 이고 MATH 이다. and,, as follows: MATH 를 설정하여 inexact proximal newton이 local superlinear rate를 갖는다는 것을 증명하였다. > MATH > \\begin align > \\eta k \\le \\min \\left\\ \\frac m 2 , \\frac \\parallel G \\tilde f k-2 /M x^ k-1 - G f/M x^ k-1 \\parallel 2 \\parallel G f/M x^ k-2 \\parallel 2 \\right\\ > \\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_05_Notable_examples/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_06_Proximal_quasi_Newton_methods",
    "title": "19-06 Proximal quasi-Newton methods",
    "chapter": "19",
    "order": 10,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "problem가 커질수록 Hessian의 computation 비용이 매우 높아진다. Proximal quasi-Newton method 는 각 stepat, Hessian MATH 를 computation하지 않는 방식with, superlinear or, linear convergence의 convergence 속도를 제공한다. Proximal quasi-Newton method Lee 2014 는 Hessian을 BFGS-styleto, update하는 방식을 제안했다. 이 method은 매우 잘 실행되며 local superlinear convergence의 convergence 속도를 갖는다. Tseng and Yun 2009 은 Hessian을 blockwiseto, approximation하는 방식을 제안했다. 이 method은 MATH at, MATH 가 일부 optimization variableto, 의존하는 부분with, 나뉠 수 있을 when,만 작동한다. Hessian을 blockwiseto, computation하면 computation이 매우 빨라진다. 이 method은 linear convergence의 convergence 속도를 갖는다. Quasi-Newton은 Hessian computation이 힘들when, 뿐 아니라 Hessian이 singular이거나 near singular인 ill-conditionat,도 유용하다. reference 논문 J. Lee and Y. Sun and M. Saunders 2014 , \"Proximal Newton-type methods for minimizing composite functions\" P. Tseng and S. Yun 2009 , \"A coordinate gradient descent method for nonsmooth separable minimization\"",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_06_Proximal_quasi_Newton_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter19/19_07_Projected_Newton_method",
    "title": "19-07 Projected Newton method",
    "chapter": "19",
    "order": 11,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "What's wrong with projected Newton? When MATH is the indicator function MATH of convex set MATH , the problem can be defined as follows: > MATH Therefore, if MATH , then proximal gradient descent becomes projected gradient descent . That is, projected gradient descent is a special case of proximal gradient descent. What about the case of proximal Newton when MATH ? In this case, the update equation is defined as follows: > MATH > \\begin align > z^ + & =\\underset z \\in C \\text argmin \\ \\frac 1 2 \\parallel x - H^ -1 \\nabla g x - z \\parallel H^2 \\\\\\\\ > &= \\underset z \\in C \\text argmin \\ \\nabla g x ^T z - x + \\frac 1 2 z - x ^T H z - x \\\\\\\\ > \\end align > MATH If MATH , then this becomes the result of projecting MATH onto set MATH , but for general MATH , it is not a projection. If MATH , it would be the MATH -norm, so if it were the MATH -norm instead of the H-norm, it would be a projection. Therefore, the projected Newton method is not a special case of the proximal Newton method. Projected Newton for box constraints For the special case of problems with box constraints, projected Newton can be applied. Bertsekas, 1982; Kim et al., 2010; Schmidt et al., 2011 . Let the problem be as follows: > MATH Starting with the initial point MATH of the Projected Newton method and a small constant MATH , we iterate the following steps MATH . step1: Binding set을 정의한다. > \\begin align B k-1 & = \\\\ i : x i^ k-1 \\le l i + \\epsilon \\quad \\text and \\quad \\nabla i g x^ k-1 \\gt 0 \\\\ \\quad \\cup \\quad \\\\ i : x i^ k-1 \\ge u i - \\epsilon \\quad \\text and \\quad \\nabla i g x^ k-1 \\lt 0 \\\\ \\end align optimization stepat, 이 variable들을 box constraint의 경계to, 밀어낸다. 이들을 점점 더 많이 밀어낼수록 목적 function는 줄어든다. step2: Free set MATH 을 정의한다. step3: Free variable을 therefore, Hessian의 주요 submatrix의 inverse를 정의한다. > MATH step4: Fee variable을 따라 Newton step을 실행하고 projection을 한다. > MATH > \\begin align > x k = P l, u \\left x^ k-1 - t k \\begin bmatrix S^ k-1 & 0 \\\\ > 0 & I \\end bmatrix > \\begin bmatrix \\nabla F k-1 g x^ k-1 \\\\ \\nabla B k-1 g x^ k-1 \\end bmatrix > \\right > \\end align > MATH 여기서 MATH 는 MATH to,의 projection이다. matrix식을 보면 free variableabout,서는 Newton step을 실행but, binding variable의 case, 변하지 않는 것을 알 수 있다. also,, projection은 box 범위 밖to, 있는 점들about,서 각 coordinateabout, 적절한 MATH or, MATH 를 지정solution주는 간단한 작업이다. 이 method은 problem가 매우 크고 ex, difference원이 큰 case, 대부분의 variable이 boundary 근처to, 있어서 free set이 매우 작을 when, optimization를 하는 method이다. 어떤 종류의 problem가 box constraint를 갖는가? as follows: 이런 종류의 problem는 매우 많은 것with, informing,져 있다. Nonnegative least squares Support vector machine dual Graphical lasso dual Fused lasso total variation denoising dual Convergence properties Bertsekas 1982 는 적절한 가정하to, projected Newtonwith, 유한번 iteration을 하면 적절한 binding constraints를 찾을 수 있다는 것을 보였다. 그러면, free variableabout, Newton's methodand, 같아진다. Bertsekas 1982 는 also, superlinear convergence를 증명하였다. Kim et al. 2010 , Schmidt et al. 2011 은 BFGS-style update를 사용한 projected quasi-Newton method를 제안했다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter19/19_07_Projected_Newton_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_00_Dual_Methods",
    "title": "20 Dual Methods",
    "chapter": "20",
    "order": 1,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "In this chapter, we will examine methods for solving problems using duality, including dual subgradient method, dual decomposition method, augmented Lagrangian method, and briefly introduce the concept of Alternating Direction Method of Multipliers ADMM . First, we will briefly review the previously learned content on Proximal Newton method and Conjugate function. Review: proximal Newton method Consider the following problem. >\\begin equation \\min x g x + h x \\end equation Here, functions MATH and MATH are convex functions, where MATH is twice differentiable and MATH is simple. The Proximal Newton method starts with initial MATH and finds the optimal vector direction that is good for both functions MATH and MATH as follows >\\begin alignat 1 v^ k & = \\arg \\min v g x^ k-1 ^T v + \\frac 1 2 v^T \\nabla^2 g x^ k-1 v + h x^ k-1 + v \\end alignat Using the direction found above, we update the next MATH as follows. >\\begin equation x^ k = x^ k-1 + t k v^ k , k=1,2,3,\\dots \\end equation Here, MATH is the step size determined by backtracking. We execute the above two processes iteratively. > The above iteration is very expensive computing MATH is generally very difficult However, under appropriate conditions, very few iterations are required to converge, and it has a convergence rate of local quadratic convergence Review: conjugate function For MATH , the conjugate function is defined as follows. >\\begin equation f^ y = \\max x y^Tx - f x \\end equation 1 The conjugate function can be written as follows, and this is a form that frequently appears in dual problems. >\\begin equation -f^ \\ast y = \\min x f x - y^Tx \\end equation 2 If MATH is closed and convex, then MATH . Also, >\\begin equation x \\in \\partial f^ \\ast y \\Longleftrightarrow y \\in \\partial f x \\Longleftrightarrow x \\in \\arg\\min z f z - y^Tz \\end equation Proof first,, MATH 을 증명한다. 1step : MATH > MATH 를 let's assume. 그러면, MATH 는 MATH 를 최대to, 하게 하는 MATH 들의 set MATH to, 속하게 된다, that is, MATH . however,, MATH 이고, MATH . therefore,, MATH 2step : MATH > 위at, 보인것and, 같이, if,, MATH 이면, MATH . 여기서, MATH 이므to, MATH . 위 1, 2 step를 through,, MATH 이 증명되었다. 3step : MATH > 한편, MATH 은 subgradient의 정의from, 자명한 in fact,이다. therefore,, 위 두 증명을 through,, MATH 임이 증명되었다. 3 if, MATH 가 strictly convex이면, > MATH > \\begin equation > \\nabla f^ \\ast y = \\arg\\min z f z - y^T z > \\end equation > MATH Proof > MATH 가 strictly convex이면, MATH 는 최소값을 갖는 유일한 MATH 가 존재하며, >이것은 위 2 to, about, 증명으from, MATH 이어야 한다. 다시 말하면 MATH 가 strictly convex이면 MATH 의 subgradient는 1개이며 gradient가 된다. therefore,, MATH 는 differentiable한 function이다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_00_Dual_Methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_01_00_Dual_(sub)gradient_methods",
    "title": "20-01 Dual (sub)gradient methods",
    "chapter": "20",
    "order": 2,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Even in cases where we cannot find a dual conjugate in closed-form, we can use subgradient or gradient methods based on the dual. For example, consider the following problem. >\\begin equation \\min x f x \\text subject to Ax = b \\end equation The dual problem of the above problem is as follows. Here MATH is the conjugate of MATH . >\\begin equation \\max u -f^ \\ast -A^T u - b^T u \\end equation In this case, if we define MATH as MATH , then the subgradient of MATH is as follows. >\\begin equation \\partial g u = A \\partial f^ \\ast -A^Tu - b \\end equation In the above expression, MATH can be expressed in terms of MATH as follows. >\\begin equation \\partial g u = Ax-b \\quad \\text where \\quad x \\in \\arg\\min z f z + u^T A z \\end equation Dual subgradient method Dual subgradient method 는 dual problem의 목적식을 maximization하기 for, 시작점 MATH at, 시작solution서 MATH about, 다음 step를 iteration한다. > MATH > \\begin alignat 1 > x^ k & \\in \\arg \\min x f x + u^ k-1 ^T A x \\\\ > u^ k & = u^ k-1 + t k A x^ k - b > \\end alignat > MATH 여기서 step size MATH 는 표준적인 방식with, 선택된다. Strictly Convex인 case, if, MATH 가 strictly convex라면 MATH 는 미분가능solution진다. therefore,, algorithm은 MATH about, 다음 step를 iteration하는 dual gradient ascent 가 된다. > MATH > \\begin alignat 1 > x^ k & = \\arg \\min x f x + u^ k-1 ^T A x \\\\ > u^ k & = u^ k-1 + t k A x^ k -b > \\end alignat > MATH before, 방식and, 다른 점은 MATH 가 유일하다는 것이다. MATH and,의 relationship,가 MATH relationship,임을 confirming,보라. 여기서 step size MATH 도 표준적인 방식with, 선택되며 MATH 을 수행할 when, proximal gradient나 acceleration도 평소처럼 적용할 수 있다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_01_00_Dual_(sub)gradient_methods/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_01_01_Convergence_Analysis",
    "title": "20-01-01 Convergence Analysis",
    "chapter": "20",
    "order": 3,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Lipschitz gradients and strong convexity Let's assume MATH is a closed convex function. Then the following equivalence relationship holds. >\\begin equation \\text MATH is strongly convex with parameter MATH MATH Lipschitz with parameter MATH . \\end equation Proof if, MATH 가 strongly convex하고 MATH at, minimize된다고 하면 다음 relationship,가 성립한다. >\\begin equation g y \\geq g x + \\frac d 2 \\lVert y-x \\rVert 2^2, \\text for all y \\end equation 우선, MATH 를 minimization하는 MATH and, MATH 를 minimization하는 MATH 가 있다고 하자. 그러면, 위 식으from, 다음 두 부등식을 얻을 수 있다. > MATH > \\begin alignat 1 > f x v - u^Tx v \\geq f x u - u^T x u + \\frac d 2 \\lVert x u - x v \\rVert 2^2 \\\\ > f x u - v^Tx u \\geq f x v - v^T x v + \\frac d 2 \\lVert x u - x v \\rVert 2^2 > \\end alignat > MATH 위 두 식을 더하면 다음and, 같은 식을 얻을 수 있다. >\\begin equation f x v - u^Tx v + f x u - v^Tx u \\geq f x u - u^T x u + f x v - v^T x v + d \\lVert x u - x v \\rVert 2^2. \\end equation 이 식을 재정렬 후 Cauchy-Schwartz를 적용하면 as follows: 정리된다. > MATH > \\begin align > d \\lVert x u - x v \\rVert 2^2 & \\leq - u^Tx v - v^Tx u + u^T x u + v^T x v \\\\\\\\ > & = u-v ^T x u - x v \\\\\\\\ > & \\leq \\lVert u-v \\rVert 2 \\lVert x u - x v \\rVert 2 > \\end align > MATH therefore,, 다음and, 같은 relationship,를 확인할 수 있다. > MATH 이to,써 MATH Lipschitz with parameter MATH 이 증명되었다. Convergence guarantees 위 result,and, gradient descent를 combining,, dual objective의 optimal solutionto,의 convergence성을 as follows: 설명할 수 있다. if, MATH 가 파라미터 MATH to, strongly convex 하면, step size MATH about,서, dual gradient ascent는 MATH with, converge한다. if, MATH 가 파라미터 MATH to, strongly convex 하고, MATH 는 파라미터 MATH to, Lipschitz하면, step size MATH MATH about,서, dual gradient ascent는 MATH with, converge한다. linear convergence",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_01_01_Convergence_Analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_02_00_Dual_Decomposition",
    "title": "20-02 Dual Decomposition",
    "chapter": "20",
    "order": 4,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "In this section, we examine techniques for decomposing problems using duality.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_02_00_Dual_Decomposition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_02_01_Dual_Decomposition_with_Equality_Constraint",
    "title": "20-02-01 Dual Decomposition with Equality Constraint",
    "chapter": "20",
    "order": 5,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "다음의 problem를 보자. >\\begin equation \\min x \\sum i=1 ^B f i x i \\quad \\text subject to \\quad Ax = b \\end equation if,, variable MATH 를 MATH 개의 블록with, 분할하고, MATH , matrix MATH 역시 MATH 개의 sub-matrix 블록with, as follows: 분할하면, MATH , 위 minimization problem는 as follows: MATH 개의 분리된 problemto, 분solution될 수 있다. > MATH > \\begin alignat 1 > & \\quad x^+ \\in \\arg\\min x \\sum i=1 ^B f i x i + u^T Ax \\\\ > \\Longleftrightarrow & \\quad x i^+ \\in \\arg\\min x i f i x i + u^T A ix i, \\quad i=1,\\dots, B > \\end alignat > MATH Dual decomposition algorithm: > MATH > \\begin alignat 1 > x i^ k & \\in \\arg \\min x i f i x i + u^ k-1 ^T A i x i, \\quad i=1,\\dots,B \\\\ > u^ k & = u^ k-1 + t k \\left \\sum i=1 ^B A i x i^ k - b \\right > \\end alignat > MATH 위 두 step는 아래and, 같이 solution석할 수 있다. > 첫번째 수식은 broadcast stepto,서, MATH 개의 프to,세서의 각각to,게 MATH 를 보낸다. and,, 프to,세서 각각은 병렬to, 자신의 최적 MATH 를 찾는다. 두번째 수식은 gather stepto,서, 각 프to,세서from, MATH 를 모은다. and, global dual variable MATH 를 업데이트 한다. 위 두 step는 MATH about, 계속 iteration한다. Fig 1 Broadcast and Gather",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_02_01_Dual_Decomposition_with_Equality_Constraint/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_02_02_Dual_Decomposition_with_Inequality_Constraint",
    "title": "20-02-02 Dual Decomposition with Inequality Constraint",
    "chapter": "20",
    "order": 6,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "다음의 problem를 생각solution 보자. 앞의 problemand, 다른점은 제약식이 부등식의 relationship,를 갖는 것이다. > MATH > \\begin equation > \\min x \\sum i=1 ^B f i x i \\quad \\text subject to \\quad \\sum i=1 ^B A i x i \\leq b > \\end equation > MATH Dual decomposition projected subgradient method 위 problemat,는 dual variable가 always, MATH 보다 같거나 커야 한다, that is, MATH . therefore,, 다음 스텝의 MATH 값을 computation할 when,, MATH 보다 큰 범위안with, projection을 시켜서 업데이트를 한다. > MATH > \\begin alignat 1 > x i^ k & \\in \\arg \\min x i f i x i + u^ k-1 ^T A i x i, \\quad i=1,\\dots,B \\\\ > u^ k & = u^ k-1 + t k \\left \\sum i=1 ^B A i x i^ k - b \\right + > \\end alignat > MATH 여기서, MATH 는 0보다 큰 MATH 를 의미한다, that is,, MATH . 위 process을 MATH about,서 iteration한다. Price coordination interpretation generally, dual decomposition problem들은 price coordination 관점at, as follows: solution석될 수 있다. Vandenberghe > MATH 개의 독립적인 유닛이 있고, 각 유닛은 자신의 결정 variable MATH 를 결정한다. > 각 constraint은 MATH 개의 유닛이 공유하고 있는 자원to, about, 제약을 의미하며, dual variable MATH 는 자원 MATH 의 가격을 의미한다. > Dual variable는 아래and, 같이 업데이트되며 \\begin equation u j^ + = u j - t s j + , \\quad j=1,\\dots,m \\end equation > > MATH 여기서, MATH 는 슬랙 variableto,써 \\\\ > MATH - MATH \\qquad MATH s j > 0 MATH j MATH u j MATH 를 감소시킨다 \\\\ > MATH - price는 향image 음수가 되지 않도록 한다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_02_02_Dual_Decomposition_with_Inequality_Constraint/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_03_Augmented_Lagrangians",
    "title": "20-03 Augmented Lagrangians",
    "chapter": "20",
    "order": 7,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Dual ascent의 단점은 convergence을 보장하기 for, 강한 condition,이 필요하다는 것이다. convergence을 보장하려면 MATH 가 strongly convexsolution야 했다. 이런 단점은 Augmented Lagrangian method or, Method of multipliers by, 개선될 수 있다. Primal problem를 아래and, 같이 transformation한다. >\\begin equation \\min x f x + \\frac \\rho 2 \\lVert Ax - b \\rVert 2^2 \\quad \\text subject to \\quad Ax = b \\end equation 여기서 MATH 이다. MATH 가 full column rank를 갖는다면 목적식은 strongly convex하다. 이는 원래의 problemand, 정확히 동일한 problem가 된다. Augmented term인 MATH 는 0이 되기 because,이다. Augmented Lagrangian Method Dual gradient ascent : MATH about, 다음을 iteration한다. > MATH > \\begin alignat 1 > x^ k & \\in \\arg\\min x f x + u^ k-1 ^T A x + \\frac \\rho 2 \\lVert Ax - b \\rVert 2^2 \\\\ > u^ k & = u^ k-1 + \\rho A x^ k - b > \\end alignat > MATH 위 dual algorithmat, MATH 는 step size 역할을 한다, that is, MATH 이다. 이것은 next,서 그 reason,를 알 수 있다. MATH 가 step size일 when, optimality 증명 MATH 는 MATH 를 minimization하므to,, 원래 primal problemto, about, stationary condition,according to,, MATH at, 목적식의 subgradient가 아래and, 같이 MATH 을 포함solution야 한다. > MATH > \\begin alignat 1 > 0 & \\in \\partial f x^ k + A^T u^ k-1 + \\rho A x^ k -b \\\\ > & = \\partial f x^ k + A^T u^ k > \\end alignat > MATH 위식at,, MATH to, 동작하게 되면, 적당한 condition,하at, MATH 가 MATH with, 가까워지면서 feasible한 solution를 제공하기 시작하고, 궁극적with, KKT condition,이 만족되고, MATH and, MATH 가 optimalityto, 근접함을 보일 수 있다. Augmented Lagrangian method 의 장점은 훨씬 좋은 convergence성을 갖는다는 것이고, 단점은 problem를 분solution할 수 있는 decomposability를 잃는다는 것이다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_03_Augmented_Lagrangians/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_04_00_A_peak_at_ADMM",
    "title": "20-04 A peak at ADMM",
    "chapter": "20",
    "order": 8,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Lipschitz gradients and strong convexity In this section, we examine an overview of the Alternating Direction Method of Multipliers ADMM technique. While the augmented Lagrangian method previously did not provide decomposability, ADMM is a method that provides decomposability along with convergence properties.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_04_00_A_peak_at_ADMM/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_04_01_ADMM",
    "title": "20-04-01 ADMM",
    "chapter": "20",
    "order": 9,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Alternating Direction Method of Multipliers ADMM The Alternating Direction Method of Multipliers ADMM is a powerful optimization algorithm that combines the benefits of dual decomposition and the method of multipliers. It is particularly effective for solving convex optimization problems that can be decomposed into smaller, more manageable subproblems. Consider the following problem: >\\begin equation \\min x,z f x + g z \\quad \\text subject to \\quad Ax + Bz = c \\end equation As before, we can augment the objective function as follows: >\\begin equation \\min x,z f x + g z + \\frac \\rho 2 \\lVert Ax + Bz - c \\rVert 2^2 \\quad \\text subject to \\quad Ax + Bz = c \\end equation where MATH is the penalty parameter. Augmented Lagrangian The augmented Lagrangian can be defined as: >\\begin equation L \\rho x,z,u = f x + g z + u^T Ax + Bz - c + \\frac \\rho 2 \\lVert Ax + Bz - c \\rVert 2^2 \\end equation ADMM Algorithm ADMM performs the following iterative steps for MATH : > MATH > \\begin alignat 1 > x^ k & = \\arg\\min x L \\rho x,z^ k-1 ,u^ k-1 \\\\ > z^ k & = \\arg\\min z L \\rho x^ k ,z,u^ k-1 \\\\ > u^ k & = u^ k-1 + \\rho Ax^ k + Bz^ k - c > \\end alignat > MATH Key Point: It is crucial that MATH obtained from the first step is used in the second step to compute MATH . This alternating update structure is essential for convergence. Comparison with Method of Multipliers Note that in the general Method of Multipliers , the first two steps are replaced by the following joint minimization: >\\begin equation x^ k , z^ k = \\arg\\min x,z L \\rho x,z,u^ k-1 \\end equation The key advantage of ADMM is that it decomposes this joint minimization into two separate, simpler subproblems that can often be solved more efficiently or even in closed form.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_04_01_ADMM/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_04_02_Converegence_Guarantee",
    "title": "20-04-02 Convergence Guarantee",
    "chapter": "20",
    "order": 10,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Convergence Guarantee for ADMM Under appropriate conditions on MATH and MATH note that MATH and MATH do not need to be full rank , ADMM satisfies the following for all MATH : Residual convergence : As MATH , MATH , meaning the primal iterates approach feasibility. Objective convergence : MATH , where MATH is the optimal primal objective value. Dual convergence : MATH , where MATH is the dual solution. The exact convergence rate is not yet fully understood, and much research is currently ongoing in this area. Roughly speaking, ADMM performs similarly to or slightly better than first-order methods.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_04_02_Converegence_Guarantee/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_04_03_ADMM_in_Scaled_Form",
    "title": "20-04-03 ADMM in Scaled Form",
    "chapter": "20",
    "order": 11,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "ADMM in Scaled Form ADMM can be expressed in scaled form by substituting the dual variable MATH with MATH . The ADMM steps can then be written as follows: > MATH > \\begin alignat 1 > x^ k & = \\arg\\min x f x + \\frac \\rho 2 \\lVert Ax + Bz^ k-1 - c + w^ k-1 \\rVert 2^2 \\\\ > z^ k & = \\arg\\min z g x + \\frac \\rho 2 \\lVert Ax^ k + Bz - c + w^ k-1 \\rVert 2^2 \\\\ > w^ k & = w^ k-1 + Ax^ k + Bz^ k - c > \\end alignat > MATH Equivalence to Original Form We can show that the above equations are equivalent to the original form through the following process: > MATH > \\begin align > x^ k & = \\arg\\min x f x + \\frac \\rho 2 \\lVert Ax + Bz^ k-1 - c + w^ k-1 \\rVert 2^2 \\\\ > & = \\arg\\min x f x + \\frac \\rho 2 \\lVert Ax + Bz^ k-1 - c \\rVert 2^2 + \\rho w^ k-1 T Ax + Bz^ k-1 - c + \\frac \\rho 2 \\lVert w^ k-1 \\rVert 2^2 \\\\ > & = \\arg\\min x f x + \\frac \\rho 2 \\lVert Ax + Bz^ k-1 - c \\rVert 2^2 + u^ k-1 T Ax + Bz^ k-1 - c \\\\ > \\end align > MATH where we used MATH and dropped the constant term. Interpretation of Scaled Variable Here, MATH can be viewed as the sum of residuals up to iteration MATH : > MATH > \\begin equation > w^ k = w^ 0 + \\sum i=1 ^k Ax^ i + Bz^ i - c > \\end equation > MATH This scaled form is often more convenient for implementation and analysis.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_04_03_ADMM_in_Scaled_Form/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_04_04_Example:_Alternating_Projection",
    "title": "20-04-04 Example - Alternating Projection",
    "chapter": "20",
    "order": 12,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Example: Alternating Projection Consider the problem of finding a point in the intersection of convex sets MATH : >\\begin equation \\min x I C x + I D x \\end equation where MATH and MATH are indicator functions for sets MATH and MATH respectively. To reformulate this problem in ADMM form, we express it as follows: > MATH > \\begin equation > \\min x,z I C x + I D x \\quad \\text subject to \\quad x - z = 0 > \\end equation > MATH ADMM Algorithm for Alternating Projection Each ADMM cycle involves two projections: > MATH > \\begin alignat 1 > x^ k & = P C \\left z^ k-1 - w^ k-1 \\right \\\\ > z^ k & = P D \\left x^ k + w^ k-1 \\right \\\\ > w^ k & = w^ k-1 + x^ k - z^ k > \\end alignat > MATH where MATH and MATH denote the projection operators onto sets MATH and MATH respectively. Derivation The update for MATH is derived as follows: > MATH > \\begin alignat 1 > x^ k & = \\arg\\min x I C x + \\frac \\rho 2 \\lVert x - z^ k-1 + w^ k-1 \\rVert 2^2 \\\\ > & = P C \\left z^ k-1 - w^ k-1 \\right > \\end alignat > MATH Similarly, the update for MATH is derived as: > MATH > \\begin alignat 1 > z^ k & = \\arg\\min z I D z + \\frac \\rho 2 \\lVert x^ k - z + w^ k-1 \\rVert 2^2 \\\\ > & = P D \\left x^ k + w^ k-1 \\right > \\end alignat > MATH Comparison with Classical Method This method is similar to the classical alternating projection method but is often more efficient and robust in practice.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_04_04_Example-_Alternating_Projection/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter20/20_05_References",
    "title": "20-05 References",
    "chapter": "20",
    "order": 13,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "References S. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein 2010 , \"Distributed optimization and statistical learning via the alternating direction method of multipliers\" W. Deng and W. Yin 2012 , \"On the global and linear convergence of the generalized alternating direction method of multipliers\" M. Hong and Z. Luo 2012 , \"On the linear convergence of the alternating direction method of multipliers\" F. lutzeler, P. Bianchi, Ph. Ciblat, and W. Hachem 2014 , \"Linear convergence rate for distributed optimization with the alternating direction method of multipliers\" R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. Jordan 2015 , \"A general analysis of the convergence of ADMM\" L. Vandenberghe, Lecture Notes for EE 236C, UCLA, Spring 2011-2012",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter20/20_05_References/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter21/21_00_Alternating_Direction_Method_of_Multipliers",
    "title": "21 Alternating Direction Method of Multipliers",
    "chapter": "21",
    "order": 1,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "This chapter aims to examine ADMM, which was covered in Chapter 20 % multilang post url contents/chapter20/21-03-27-20 00 Dual Methods % , in more detail. The basic concepts are not significantly different in depth from what was covered in Chapter 20, and we will mainly look at application cases. Reference Papers Boyd, Stephen, et al. BPCPE11 \"Distributed optimization and statistical learning via the alternating direction method of multipliers.\" Foundations and Trends® in Machine learning 3.1 2011 : 1-122. Hong, Mingyi, and Zhi-Quan Luo. HL12 \"On the linear convergence of the alternating direction method of multipliers.\" Mathematical Programming 162.1-2 2017 : 165-199. Deng, Wei, and Wotao Yin. DY16 \"On the global and linear convergence of the generalized alternating direction method of multipliers.\" Journal of Scientific Computing 66.3 2016 : 889-916. Iutzeler, Franck, et al. IBCH14 \"Linear convergence rate for distributed optimization with the alternating direction method of multipliers.\" 53rd IEEE Conference on Decision and Control. IEEE, 2014. Nishihara, Robert, et al. NLRPJ15 \"A general analysis of the convergence of ADMM.\" arXiv preprint arXiv:1502.02009 2015 . Parikh, Neal, and Stephen Boyd. NB13 \"Proximal algorithms.\" Foundations and Trends® in Optimization 1.3 2014 : 127-239. Vu, Vincent Q., et al. VCLR13 \"Fantope projection and selection: A near-optimal convex relaxation of sparse PCA.\" Advances in neural information processing systems. 2013. Candès, Emmanuel J., et al. CLMW09 \"Robust principal component analysis?.\" Journal of the ACM JACM 58.3 2011 : 11. Ramdas, Aaditya, and Ryan J. Tibshirani. RT16 \"Fast and flexible ADMM algorithms for trend filtering.\" Journal of Computational and Graphical Statistics 25.3 2016 : 839-858. Wytock, Matt, Suvrit Sra, and Jeremy Z. Kolter. WSK14 \"Fast Newton methods for the group fused lasso.\" UAI. 2014. Barbero, Alvaro, and Suvrit Sra. BS14 \"Modular proximal optimization for multidimensional total-variation regularization.\" arXiv preprint arXiv:1411.0589 2014 . ADMM convergence relation, : BPCPE11 , HL12 , DY16 , IBCH14 , NLRPJ15 Sparse subspace estimation : VCLR13 Sparse plus low rank decomposition : CLMW09 Consensus ADMM : BPCPE11 , NB13 Subprogram parameterization : RT16 , WSK14 , BS14",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter21/21_00_Alternating_Direction_Method_of_Multipliers/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter21/21_01_Last_time_Dual_method,_Augmented_Lagrangian_method,_ADMM,_ADMM_in_scaled_form",
    "title": "21-01 Last time - Dual method, Augmented Lagrangian method, ADMM, ADMM in scaled form",
    "chapter": "21",
    "order": 2,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "before, 20장at, 우리는 Dual methods, ADMMabout, 살펴보았다. 여기선 ADMM의 응용을 살펴보기to, 앞서, Dual methodsand, Augmented Lagrangian method, ADMM, ADMM in scaled formabout, 정리하고자 한다. Dual method 아래의 problem를 let's look at. > MATH >\\begin align >&\\min x &&f x \\\\\\\\ >&\\text subject to &&Ax = b >\\end align > MATH 여기서 MATH 는 strictly convex하고 닫혀있다고 하자. 이 problem의 Lagrangian은 아래and, 같다. > MATH >\\begin align >L x,u = f x +u^ T Ax-b >\\end align > MATH 위 problem의 dual problem는 아래and, 같다. > MATH >\\begin equation >\\max u -f^ \\ast -A^T u - b^T u >\\end equation > MATH 여기at,의 u는 dual variable이다. 이 식to, about, dual gradient ascent는 아래의 식을 iteration적with, computation한다. MATH > MATH >\\begin align >x^ k &=\\underset x \\operatorname argmin L x,u^ k-1 \\\\\\\\ >u^ k &= u^ k-1 +t k Ax^ k -b >\\end align > MATH MATH 는 k번째 iteration의 step size이다. 이 dual methodat,는, primal variable MATH 는 첫번째 식처럼 before, 스텝at, 주어진 MATH at,의 Lagrangian을 minimization하는 MATH 값with, 업데이트되고, dual variable MATH 는 MATH 이 gradient direction인 gradient ascent의 형태to, 업데이트가 된다. 이 method의 장점은 MATH 가 B개의 problemto, 분할이 가능할 when, decomposable , MATH also, B개의 블록with, 분할하고 MATH , matrix A also, B개의 sub-matrix 블록with, decompose가 가능solution서 MATH , 쉽게 병렬화 or, 확장이 가능하여 computation이 용이하다. but, 단점은 convergence성를 보장하기 for, 까다to,운 condition,이 필요하다 ; primal의 feasible을 보장하기 for,, MATH 가 strongly convex하다는 condition,이 필요하다. 20-01-01 % multilang post url contents/chapter20/21-03-27-20 01 01 Convergence Analysis % Augmented Lagrangian method Method of multipliers라고도 불리는 Augmented Lagrangian method는 primal problemto, 추가 항을 더하여 computation한다. 이렇게 하면 iteration을 iteration되면서 점difference KKT의 conditions을 만족하게 된다. Dual methodand, comparing, convergence성to, about, condition, f가 strongly convex 을 완화시킨다. instead of, problem의 분solution decompose 가 불가능solution지는 단점이 있다. Primal problem의 정의는 as follows:. > MATH >\\begin align >&\\min x &&f x +\\frac \\rho 2 ||Ax-b|| 2 ^ 2 &\\\\\\\\ >&\\text subject to &&Ax=b >\\end align > MATH 여기서 MATH 이다. 이 problem의 Lagrangian은 아래and, 같다. > MATH >\\begin align >L \\rho x,u =f x +u^ T Ax-b +\\frac \\rho 2 ||Ax-b|| 2 ^ 2 . >\\end align > MATH Dual gradient ascent는 다음을 iteration한다. MATH > MATH >\\begin align >x^ k &=\\underset x \\operatorname argmin L \\rho x,u^ k-1 \\\\\\\\ >u^ k &= u^ k-1 +\\rho Ax^ k -b >\\end align > MATH 이 method의 장점은 위at, 언급하였듯, dual methodto, 비하여 더 나은 convergence condition,을 가진다. 단점은 제product 항이 추가되는 탓to, 분solution가능한 성질 decomposability 을 잃게 된다. Alternating direction method of multipliers ADMM ADMM은 dual methodand, augmented Lagrangian method의 장점을 섞은 method이다. problem가 아래의 형태to, 정의 되어있다고 하자. > MATH >\\begin align >\\min x f x +g z \\qquad \\text subject to Ax+Bz=c >\\end align > MATH 이 식to, MATH 인 augmented Lagrangian을 정의할 수 있다. > MATH >\\begin align >&L \\rho x,z,u = f x +g z +u^ T Ax+Bz-c +\\frac \\rho 2 ||Ax+Bz-c|| 2 ^ 2 \\\\\\\\ >\\end align > MATH 이어서 아래를 iteration하여 variable를 업데이트한다. > MATH >\\begin align >&\\text for k = 1,2,3,... \\\\\\\\ >&x^ k =\\underset x \\operatorname argmin L \\rho x,z^ k-1 ,u^ k-1 \\\\\\\\ >&z^ k =\\underset z \\operatorname argmin L \\rho x^ k ,z,u^ k-1 \\\\\\\\ >&u^ k =u^ k-1 +\\rho Ax^ k +Bz^ k -c >\\end align > MATH ADMMat,는 primal variable인 MATH 를 함께 업데이트하지 않고, 순difference적with, 각각 업데이트 한다. and, 순difference적with, 업데이트할 when,는 다른 variable는 가장 최근의 값을 이용한다. that is,, k번째 iterationat, MATH 를 업데이트 할when,to,는 before, iteration의 값 MATH 이 아닌 MATH 를 이용하고, u를 업데이트 할when, also, 현재 iterationat, 구한 primal variable MATH 를 바to, 이용한다. Alternating direction method of multipliers ADMM ADMM은 제약식 내의 Aand, B가 full rank라는 가정 없이, MATH and, MATH to, about, 큰 제약 없이 under modeset assumption 모든 MATH about, 다음을 만족한다. Residual convergence: MATH 가 MATH to, 갈 when,, MATH , that is, primal iteration이 feasibilityto, 접근한다. Objective convergence: MATH , 여기서 MATH 는 최적의 primal objective 값이다. Dual convergence: MATH , 여기서 MATH 는 dual solution 이다. Convergence rateabout,서는 아직 generally, informing,지진 않았고, 연구가 이루어지고있다. Convergenceto, about, reference문헌은 21장 소개파트 % multilang post url contents/chapter21/21-03-29-21 00 Alternating Direction Method of Multipliers % to, 서술되어있다. ADMM in scaled form We can express ADMM in scaled form by changing the dual variable MATH to the scaled variable MATH . In summary, the ADMM steps can be represented as follows: > MATH >\\begin align &x^ k = \\underset x \\operatorname argmin f x + \\frac \\rho 2 ||Ax + Bz^ k-1 - c + w^ k-1 || 2^2 \\\\\\\\ &z^ k = \\arg\\min z g x + \\frac \\rho 2 || Ax^ k + Bz - c + w^ k-1 || 2^2 \\\\\\\\ &w^ k = w^ k-1 + Ax^ k + Bz^ k - c \\end align > MATH Here, MATH can also be expressed as the sum of residuals up to the MATH -th iteration as shown below. > MATH >\\begin align w^ k = w^ 0 + \\sum i=1 ^k Ax^ i + Bz^ i - c \\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter21/21_01_Last_time_Dual_method,_Augmented_Lagrangian_method,_ADMM,_ADMM_in_scaled_form/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter21/21_02_Connection_to_proximal_operators",
    "title": "21-02 Connection to proximal operators",
    "chapter": "21",
    "order": 3,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Connection to Proximal Operators Consider an optimization problem with a single variable that separates into two functions: > MATH >\\begin align >\\min x f x +g x >\\end align > MATH This can also be expressed by adding a constraint: > MATH >\\begin align >\\min x, z f x +g z \\qquad \\text subject to x=z >\\end align > MATH ADMM Steps in Proximal Form The ADMM steps for this problem are: > MATH >\\begin align >&x^ k = \\operatorname prox f,\\frac 1 \\rho z^ k-1 -w^ k-1 \\\\\\\\ >&z^ k = \\operatorname prox g,\\frac 1 \\rho x^ k +w^ k-1 \\\\\\\\ >&w^ k =w^ k-1 +x^ k -z^ k >\\end align > MATH where MATH and MATH are the proximal operators of MATH and MATH respectively, with parameter MATH . Proximal Operator Definition Recall that for a convex function MATH , the proximal operator % multilang post url contents/chapter19/21-03-24-19 01 01 Reminder: proximal gradient descent % is defined as: > MATH >\\begin align > \\operatorname prox f, \\lambda v = \\underset x \\operatorname argmin \\left f x +\\frac 1 2\\lambda ||x-v|| 2 ^ 2 \\right . >\\end align > MATH Derivation of Proximal Updates The process of deriving ADMM updates in terms of proximal operators is as follows. Let MATH be the updated values of MATH after one step. Update for x: > MATH >\\begin align >x^ + & = \\underset x \\operatorname argmin f x +\\frac \\rho 2 ||x-z+w||^ 2 2 \\\\\\\\ >& =\\underset x \\operatorname argmin \\frac 1 2\\cdot\\frac 1 \\rho || z-w -x||^ 2 2 +f x \\\\\\\\ >& = \\operatorname prox f,\\frac 1 \\rho z-w >\\end align > MATH Update for z: > MATH >\\begin align >z^ + & = \\underset z \\operatorname argmin g z +\\frac \\rho 2 ||x^ + -z+w||^ 2 2 \\\\\\\\ >& =\\underset z \\operatorname argmin \\frac 1 2\\cdot\\frac 1 \\rho || x^ + +w -z||^ 2 2 +g z \\\\\\\\ >& = \\operatorname prox g,\\frac 1 \\rho x^ + +w >\\end align > MATH Key Insight The original ADMM constraint is MATH , while here the constraint is MATH . That is, when the linear transformation relationship between MATH and MATH is the identity, we can transform the original ADMM updates into proximal updates.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter21/21_02_Connection_to_proximal_operators/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter21/21_03_Example_:_Lasso_regression_and_group_lasso_Regression",
    "title": "21-03 Example - Lasso regression and group lasso Regression",
    "chapter": "21",
    "order": 4,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Lasso regression Let's solve the Lasso regression problem using ADMM. Given MATH , the Lasso problem is: > MATH >\\begin align >\\min \\beta \\frac 1 2 ||y-X\\beta||^ 2 2 +\\lambda||\\beta|| 1 >\\end align > MATH In previous chapters, we have solved the Lasso problem using various methods, including proximal gradient descent ISTA % multilang post url contents/chapter09/20-01-08-09 01 proximal gradient descent % , accelerated proximal gradient descent FISTA % multilang post url contents/chapter09/20-01-08-09 05 03 example FISTA % , barrier method % multilang post url contents/chapter15/21-03-28-15 barrier method % , and primal-dual interior-point method % multilang post url contents/chapter17/21-05-01-17 primal dual interior point method % . As with deriving the dual formulation, the performance of the ADMM algorithm depends on how we set up the auxiliary variables. Among many ways to set auxiliary variables, the following form is known to be one of the most effective: > MATH >\\begin align >&\\min \\beta, \\alpha &&||y-X\\beta||^ 2 2 +\\lambda||\\alpha|| 1 \\\\\\\\ >&\\text subject to &&\\beta-\\alpha= 0. >\\end align > MATH ADMM Updates The ADMM updates for this formulation are derived as follows. The MATH update involves a quadratic function, so we can find the minimum by differentiation. The MATH update is similar to the problem covered in Chapter 7 07-03-04 % multilang post url contents/chapter07/21-03-25-07 03 04 example soft-thresholding % , which has a soft-thresholding solution. > MATH >\\begin align >\\beta^ + &= \\underset \\beta \\operatorname argmin \\frac 1 2 ||y-X\\beta||^ 2 2 +\\frac \\rho 2 ||\\beta-\\alpha+w||^ 2 2 \\\\\\\\ >&= X^ T X+\\rho I ^ -1 X^ T y+\\rho \\alpha-w \\\\\\\\ >\\alpha^ + &= \\underset \\alpha \\operatorname argmin \\lambda||\\alpha|| 1 +\\frac \\rho 2 ||\\beta^ + -\\alpha+w||^ 2 2 \\\\\\\\ >&= S \\frac \\lambda \\rho \\beta^ + +w \\\\\\\\ >w^ + &=w+\\beta^ + -\\alpha^ + >\\end align > MATH Key Properties This result has the following characteristics: The matrix MATH is always invertible regardless of MATH since MATH . If we precompute the factorization typically Cholesky factorization in MATH flops, then the MATH update takes MATH flops. The MATH update applies the soft-thresholding operator MATH , which is identical to the content in 07-03-04 % multilang post url contents/chapter07/21-03-25-07 03 04 example soft-thresholding % . The ADMM steps are \"almost\" equivalent to repeatedly soft-thresholding ridge regression coefficients. Different values of MATH produce different results. Fig 1 Comparison of various algorithms for lasso regression 50 instances with n = 100, p = 20 3 Performance Comparison Fig 1 compares the convergence of various algorithms for the Lasso regression problem. All algorithms have the same computational complexity per iteration. As can be seen from the convergence speed in the graph, ADMM has similar convergence speed to proximal gradient descent black . Accelerated proximal gradient descent red has \"Nesterov ripples\" but shows slightly faster convergence speed. We can also confirm that ADMM shows different convergence speeds according to the MATH value. Coordinate descent green , which will be discussed later in Chapter 23 % multilang post url contents/chapter23/21-03-28-23 Coordinate Descent % , uses more information about the problem and therefore has faster convergence speed compared to other methods. The disadvantage of coordinate descent is that there are conditions required for its application. If the MATH value is set too large, the weight of minimizing the objective function MATH becomes small, and if the MATH value is set too small, feasibility decreases. Therefore, setting an appropriate MATH value is important. For detailed information, see BPCPE discussed in the Chapter 21 reference papers % multilang post url contents/chapter21/21-03-29-21 00 Alternating Direction Method of Multipliers % . Group Lasso Regression Similarly, let's examine solving the Group Lasso regression problem with ADMM. The Group Lasso regression problem is defined as follows for MATH : > MATH >\\begin align >\\min \\beta \\frac 1 2 ||y-X\\beta||^ 2 2 +\\lambda\\sum^ G g=1 c g ||\\beta g || 2 . >\\end align > MATH As with Lasso regression, we can reformulate the problem: > MATH >\\begin align >&\\min \\beta,\\alpha &&\\frac 1 2 ||y-X\\beta||^ 2 2 +\\lambda\\sum^ G g=1 c g ||\\alpha g || 2 \\\\\\\\ >&\\text subject to &&\\beta-\\alpha=0. >\\end align > MATH The ADMM steps are as follows: > MATH >\\begin align >\\beta^ + &= X^ T X+\\rho I ^ -1 X^ T y+\\rho \\alpha-w \\\\\\\\ >\\alpha^ + &= R c g \\frac \\lambda \\rho \\beta^ + g +w g \\qquad \\text g = 1,...G \\\\\\\\ >w^ + &=w+\\beta^ + -\\alpha^ + >\\end align > MATH Properties of Group Lasso ADMM This result has the following characteristics: The matrix MATH is always invertible regardless of MATH since MATH . If we precompute the factorization typically Cholesky factorization in MATH flops, then the MATH update takes MATH flops. The MATH update applies the group soft-thresholding operator MATH , which is defined as follows: >\\begin align >R t x = 1-\\frac x \\lVert x \\rVert 2 + x >\\end align",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter21/21_03_Example_-_Lasso_regression_and_group_lasso_Regression/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter21/21_04_Example_:_Sparse_subspace_estimation_and_sparse_plus_low_rank_decomposition",
    "title": "21-04 Example - Sparse subspace estimation and sparse plus low rank decomposition",
    "chapter": "21",
    "order": 5,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Example: Sparse Subspace Estimation and Sparse Plus Low Rank Decomposition Sparse Subspace Estimation Given MATH , consider the problem of finding a projection that minimizes the Frobenius norm distance between the original MATH and the projected MATH : > MATH >\\begin align >&\\min P &&||X-XP||^ 2 F \\\\\\\\ >&\\text subject to &&\\text rank P =k where P is a projection matrix >\\end align > MATH This problem is non-convex because the set of projection matrices is not a convex set. However, it is known to be equivalent to the following convex problem VCLR13 % multilang post url contents/chapter21/21-03-29-21 00 Alternating Direction Method of Multipliers % . This is also called the subspace estimation problem. > MATH >\\begin align >&\\max Y &&tr SY \\\\\\\\ >&\\text subject to &&Y\\in F k = \\left\\ Y\\in \\mathbb S ^ p : 0 \\preceq Y \\preceq I, tr Y = k \\right\\ >\\end align > MATH VCLR13 discusses solving the sparse version with added L1 norm of the subspace estimation problem. For detailed derivation, please refer to the corresponding paper. > MATH >\\begin align >&\\max Y &&tr SY -\\lambda ||Y|| 1 \\\\\\\\ >&\\text subject to &&Y\\in F k >\\end align > MATH where MATH is the Fantope of order k, as defined in the equation above. When MATH , the above problem is equivalent to standard PCA. This problem has an SDP form and can be solved using interior point methods. However, this approach is complex to implement and becomes very slow as the problem size increases. ADMM Formulation To solve this problem with ADMM, we reformulate it as follows: > MATH >\\begin align >&\\min Y,Z &&-tr SY +I F k Y + \\lambda||Z|| 1 \\\\\\\\ >&\\text subject to &&Y = Z. >\\end align > MATH ADMM Algorithm Summarizing the problem, the ADMM steps are as follows: > MATH >\\begin align >Y^ + &= \\underset Y \\operatorname argmin -tr SY + I F k Y +\\frac \\rho 2 ||Y-Z+W||^ 2 F \\\\\\\\ >&=\\underset Y\\in F k \\operatorname argmin \\frac 1 2 ||Y-Z+W-\\frac S \\rho ||^ 2 F \\\\\\\\ >&=P F k Z-W+\\frac S \\rho \\\\\\\\ >Z^ + & = \\underset Z \\operatorname argmin \\lambda||Z|| 1 +\\frac \\rho 2 ||Y^ + -Z+W||^ 2 F \\\\\\\\ >&=S \\frac \\lambda \\rho Y^ + +W \\\\\\\\ >W^ + &=W+Y^ + -Z^ + . >\\end align > MATH where MATH is the fantope projection operator. This is defined by clipping the eigendecomposition MATH VCLR13 % multilang post url contents/chapter21/21-03-29-21 00 Alternating Direction Method of Multipliers % : > MATH >\\begin align >P F k A = U\\Sigma \\theta U^ T , \\: \\Sigma \\theta = diag \\sigma 1 \\theta ,...\\sigma p \\theta >\\end align > MATH where each MATH and MATH . Sparse plus low rank decomposition Given MATH , the sparse plus low rank decomposition problem is as follows CLMW09 % multilang post url contents/chapter21/21-03-29-21 00 Alternating Direction Method of Multipliers % : > MATH >\\begin align >&\\min L,S &&||L|| tr +\\lambda||S|| 1 \\\\\\\\ >&\\text subject to &&L+S=M >\\end align > MATH Problem Description The goal of this problem is to decompose the observed matrix MATH into a low rank matrix MATH and a sparse matrix MATH . The first term of the objective function is the trace penalty of MATH , which minimizes the sum of singular values of MATH . The second term uses the MATH norm on matrix MATH to induce sparsity in MATH . MATH is a tuning parameter that balances these two terms. Both the trace norm and MATH norm are non-smooth, and generally the trace norm is known to be difficult to optimize. Like the sparse subspace estimation problem, this problem has an SDP form and can be solved using interior point methods, but this is also complex and slow. For this problem, ADMM provides somewhat easier update steps. > MATH >\\begin align >L^ + &= S^ tr \\frac 1 \\rho M-S+W \\\\\\\\ >S^ + &= S^ l 1 \\frac \\lambda \\rho M-L^ + +W \\\\\\\\ >W^ + &= W+M-L^ + -S^ + >\\end align > MATH where MATH is matrix soft-thresholding and MATH is elementwise soft-thresholding. Fig 1 Example of sparse plue low rank decomposition on surveliance camera 3 Application Example Fig 1 shows an example of applying sparse plus low rank decomposition to surveillance camera video analysis. From surveillance cameras that film a fixed area for a long time, we can easily separate the low rank part that shares most frames, and the sparse part extracts characteristic parts from specific frames. For example, in Fig 1 , the middle column represents the low rank part and the right column represents the sparse part. As can be confirmed, the low rank part contains background information that appears in almost all frames, and the sparse part contains only characteristic parts that appear only in specific frames.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter21/21_04_Example_-_Sparse_subspace_estimation_and_sparse_plus_low_rank_decomposition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter21/21_05_Consensus_ADMM",
    "title": "21-05 Consensus ADMM",
    "chapter": "21",
    "order": 6,
    "owner": "Hooncheol Shin",
    "lesson_type": "",
    "content": "Consensus ADMM Basic Consensus ADMM Consider the following problem: > MATH >\\begin align >\\min x \\sum^ B i=1 f i x >\\end align > MATH To solve this problem with ADMM, we need to introduce constraints. Here, we want to transform the equation into a form that is easy to operate in parallel. This approach, called consensus ADMM, reparametrizes the equation as follows: > MATH >\\begin align >&\\min x 1 ,...,x B ,x &&\\sum^ B i=1 f i x i \\\\\\\\ >&\\text subject to &&x i =x, i = 1,...B >\\end align > MATH ADMM Algorithm This allows us to compute decomposable ADMM steps: > MATH >\\begin align >x^ k i &= \\underset x i \\operatorname argmin f i x i +\\frac \\rho 2 ||x i -x^ k-1 +w i ^ k-1 || 2 ^ 2 , i=1,...B\\\\\\\\ >x^ k &=\\frac 1 B \\sum i=1 ^ B x i ^ k +w i ^ k-1 \\\\\\\\ >w i ^ k &=w i ^ k-1 +x i ^ k -x^ k , i=1,...,B >\\end align > MATH Simplified Form Additionally, we can define MATH . With this, we can easily verify that MATH for iterations MATH , and the second equation of the ADMM update simplifies to MATH . Therefore, we can simplify the ADMM update equations as follows: > MATH >\\begin align >x^ k i &= \\underset x i \\operatorname argmin f i x i +\\frac \\rho 2 ||x i -\\overline x ^ k-1 +w i ^ k-1 || 2 ^ 2 , i=1,...B\\\\\\\\ >w i ^ k &=w i ^ k-1 +x i ^ k -\\overline x ^ k , i=1,...,B. >\\end align > MATH Intuition The MATH updates for MATH can be computed in parallel. From the simplified equations, we can gain intuition about consensus ADMM. Each MATH update tries to minimize MATH while simultaneously using MATH regularization to align each MATH with the average MATH . If MATH becomes larger than the average, MATH increases. Therefore, the regularization in the next step will reduce the enlarged MATH . General consensus ADMM General Consensus ADMM Consensus ADMM can be generalized to a more general form. Let's look at the form of problems with affine transformations of MATH and arbitrary function MATH : > MATH >\\begin align >\\min x \\sum i=1 ^ B f i a^ T i x+b i +g x >\\end align > MATH For this equation as well, we reparameterize by adding constraints: > MATH >\\begin align >&\\min x 1 ,..x B ,x &&\\sum^ B i=1 f i a i ^ T x+b +g x \\\\\\\\ >&\\text subject to &&x i = x, i=1,...B >\\end align > MATH We can then derive decomposable ADMM updates: > MATH >\\begin align >x i ^ k &= \\underset x i \\operatorname argmin f i a i ^ T x+b i +\\frac \\rho 2 ||x i -x^ k-1 +w i ^ k-1 ||^ 2 2 +g x \\\\\\\\ >x^ k &=\\underset x \\operatorname argmin \\frac B\\rho 2 ||x-\\overline x ^ k -\\overline w ^ k-1 ||^ 2 2 +g x \\\\\\\\ >w i ^ k &=w i ^ k-1 +x i ^ k -x^ k , i=1,...B >\\end align > MATH Key Differences The differences between generalized consensus ADMM and the consensus ADMM derived above can be summarized as follows: Because the ADMM step equations do not simplify, MATH is no longer satisfied. MATH can be updated in parallel. Each MATH update can be thought of as minimizing the corresponding partial loss with MATH regularization. The MATH update is a proximal operation for the arbitrary function MATH generally a regularizer . Different ADMM algorithms are derived depending on how the reparameterization is done. For more detailed information, see the reference papers % multilang post url contents/chapter21/21-03-29-21 00 Alternating Direction Method of Multipliers % .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter21/21_05_Consensus_ADMM/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter21/21_06_Faster_convergence_with_subprogram_parametrization_:_example_of_the_2d_fused_lasso_problem",
    "title": "21-06 Faster convergence with subprogram parametrization - example of the 2d fused lasso problem",
    "chapter": "21",
    "order": 7,
    "owner": "Hooncheol Shin",
    "lesson_type": "optional",
    "content": "Faster Convergence with Subprogram Parametrization Introduction One very interesting property of ADMM is that when solving problems, if we parametrize the small subproblems in a special way, it can show much faster convergence performance than general methods. In the previous consensus ADMM example, the updates optimize over blocks of variables, which is similar to block coordinate descent. Therefore, ADMM can also achieve fast convergence by updating blocks of variables in nearly orthogonal directions. 2D Fused Lasso Example In this section, we will demonstrate the above concepts through examples, designing auxiliary constraints so that the primal updates are in de-correlated directions. For detailed information, see RT16 , WSK14 , BS14 . Let's examine the 2D fused lasso or 2D total variation denoising problem, which was one of the examples we looked at in Chapter 1 % multilang post url contents/chapter01/21-01-07-01 01 optimization problems % . Given an image MATH , the problem is defined as follows: > MATH >\\begin align >\\min \\Theta \\frac 1 2 ||Y-\\Theta||^ 2 F +\\lambda \\sum i,j |\\Theta i,j -\\Theta i+1,j |+|\\Theta i,j -\\Theta \\Theta i,j+1 | . >\\end align > MATH In this problem, there is a parameter for each pixel of the image, and this parameter matrix is MATH . Fig 1 Interpretation of the penalty term in 2d fussed lasso 3 Fig 1 visually shows the penalty term, which is the second term of the objective function. As can be seen from the defined problem, it aims to reduce the differences between a pixel and its adjacent horizontal and vertical pixels. That is, this penalty term makes the values of neighboring adjacent pixels similar. Vector Form Summarizing the penalty term as an operator, the problem becomes: > MATH >\\begin align >\\min \\theta \\frac 1 2 ||y-\\theta||^ 2 F + \\lambda||D\\theta|| 1 . >\\end align > MATH where MATH is the 2D difference operator corresponding to the original equation. Forms of ADMM updates for the 2d fused lasso problem Forms of ADMM Updates for the 2D Fused Lasso Problem Now we want to create ADMM steps in two ways by applying auxiliary constraints. The first approach is to derive ADMM from the vector form created through the 2D difference operator. > MATH >\\begin align >\\min \\theta, z \\frac 1 2 ||y-\\theta||^ 2 2 +\\lambda||z|| 1 \\qquad \\text subject to z = D\\theta, >\\end align > MATH The ADMM steps are then derived as follows: > MATH >\\begin align >\\theta^ k &= I+\\rho D^ T D ^ -1 y+\\rho D^ T z^ k-1 +w^ k-1 \\\\\\\\ >z^ k &= S \\frac \\lambda \\rho D\\theta^ k -w^ k-1 \\\\\\\\ >w^ k &= w^ k-1 +z^ k-1 -D\\theta ^ k . >\\end align > MATH Computational Complexity - Vector Form Solving for MATH is equivalent to solving the linear system MATH . Here, MATH becomes the Laplacian matrix MATH of the 2D grid, which can be solved in MATH operations. The MATH update also requires MATH operations since it involves applying the soft thresholding operator MATH . Therefore, solving ADMM in vector form takes MATH time. Matrix Form ADMM The second approach is to derive ADMM in matrix form, identical to the original problem definition. > MATH >\\begin align >&\\min \\Theta, Z &&\\frac 1 2 ||Y-\\Theta||^ 2 F +\\lambda\\sum i,j |\\Theta i,j -\\Theta i+1,j +|Z i+1,j -Z i,j+1 | \\\\\\\\ >&\\text subject to &&\\Theta = Z >\\end align > MATH The ADMM steps are as follows: > MATH >\\begin align >\\Theta \\cdot \\\\ , j ^ k &= FL^ 1d \\frac \\lambda 1+\\rho \\bigg \\frac Y+\\rho Z^ k-1 \\cdot \\\\ , j -W \\cdot \\\\ ,j ^ k-1 1+\\rho \\bigg ,\\qquad j=1,...,d\\\\\\\\ >Z i, \\cdot ^ k &= FL^ 1d \\frac \\lambda \\rho \\bigg \\Theta i, \\cdot ^ k + W i, \\cdot ^ k-1 \\bigg , \\qquad j=1,...,d\\\\\\\\ >W^ k &= W^ k-1 + \\Theta^ k - Z^ k \\\\\\\\ >\\end align > MATH where MATH is the 1D fused lasso defined as MATH . Computational Complexity - Matrix Form The matrix form ADMM can also be performed with MATH time complexity. Both MATH and MATH are in the form of 1D fused lasso, which has MATH time complexity. Fig 2 shows how the original penalty term is separated into 1D fused lasso problems. Fig 2 Interpretation of the matrix form ADMM updates for 2d fused lasso 3 Image denoising experiments Image Denoising Experiments Now let's revisit the image denoising problem we examined in Chapter 1. Fig 3 shows the data and denoised images. Fig 4 shows a comparison of the two ADMM methods. The \"specialized\" ADMM in matrix form, which defines constraints by decomposing in vertical/horizontal directions, shows much faster convergence performance than the \"standard ADMM\" derived in vector form. Fig 5 shows the image quality according to ADMM iterations. Fig 3 Data, exact solution image 300x200 image : n = 60,000 . left : original image before denoising, right : the exact solution of denoised image 3 Fig 4 Convergence curves of two ADMM algorithms. black : standard vector form , red : specialized matrix form 3 Fig 5 ADMM iterates visualized after k = 10, 30, 50, 100 iterations 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter21/21_06_Faster_convergence_with_subprogram_parametrization_-_example_of_the_2d_fused_lasso_problem/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter22/22_Conditional_Gradient_Method",
    "title": "22 Conditional Gradient (Frank-Wolfe) Method",
    "chapter": "22",
    "order": 1,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "This chapter will examine the Frank-Wolfe algorithm proposed by Marguerite Frank and Philip Wolfe in 1956. The Frank-Wolfe algorithm is an iterative first-order optimization algorithm for constrained convex optimization, also called the conditional gradient method, reduced gradient method, and convex combination algorithm. This method was originally proposed by Marguerite Frank and Philip Wolfe in 1956. The Frank-Wolfe algorithm considers a linear approximation of the objective function at each iteration and moves toward the minimizer of this linear function. 15 Wikipedia. Frank–Wolfe algorithm https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe algorithm",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter22/22_Conditional_Gradient_Method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter22/22_01_Last_time_ADMM",
    "title": "22-01 Last time: ADMM",
    "chapter": "22",
    "order": 2,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Last time: ADMM Let's consider the following optimization problem > MATH \\begin align > &\\min x,z &&f x + g z \\\\\\\\ > &\\text subject to &&Ax + Bz = c > \\end align MATH Converting this to Augmented Lagrangian form gives us the following. for some MATH > MATH The above equation becomes Strongly Convex with the addition of MATH , and this can be transformed into a form useful for parallel processing as shown in the following equation. For detailed proof, please refer to the content of the previous chapter. ADMM: for MATH > MATH > MATH > MATH ADMM in scaled form Let's change the dual variable MATH to the scaled variable MATH . Here, the ADMM step can be computed as follows. > MATH > MATH > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter22/22_01_Last_time_ADMM/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter22/22_02_Conditional_gradient_method",
    "title": "22-02 Conditional gradient method",
    "chapter": "22",
    "order": 3,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Projected Gradient Descent Let's consider a problem with the following constraints. > MATH We previously saw that if MATH is convex and smooth, and MATH is also convex, we can use the projected gradient descent method. When MATH is the projection operator for set MATH , for the chosen initial value MATH and MATH , the following equation holds. > MATH Projected Gradient Descent can also be represented as a special case of proximal gradient descent, which is essentially motivated by the fact that the MATH value in the local quadratic expansion 2nd Taylor Expansion becomes the next MATH . > MATH For more detailed information about Projected Gradient Descent, please reference 9-4 % multilang post url contents/chapter09/20-01-08-09 04 special cases % . Conditional gradient Frank-Wolfe method Instead of minimizing the quadratic approximation here, let's try something simpler. First, let's examine the point where the value is minimized when we take the inner product of set MATH with MATH . Fundamentally, instead of projection, we can solve problems more conveniently and effectively by minimizing linear functions at points within set MATH . Here, we proceed by applying a line search method using convex combinations between the current point and the minimum point. Let's look at the following formalized method. Choose initial value MATH . MATH > MATH \\begin array rcl > s^ k−1 & ∈ & \\arg\\min s ∈ C ∇f x^ k−1 ^Ts \\\\\\ > x^ k & = & 1 − γ k x^ k−1 + γ ks^ k−1 > \\end array MATH reference > MATH > MATH > MATH 여기서, before,and, 다르게 Projection process을 거치지 않고 업데이트를 할 떄, 제약 condition, set MATH to, 있는 점을 using, problem를 풀어나간다. 기본적with, step size는 MATH with, 설정된다. 임의의 MATH at, convexityby, MATH 임을 보인다. also, 다음and, 같은 식with, 업데이트가 진행되기도 한다. > MATH that is,, algorithm 수행됨according to, 선형 minimizer directionwith, 점difference적with, 조금씩 덜 이동하게 된다. 대부분의 case,, co-ordinate descent의 스페셜 케이스인 Ball L1about,서 sub gradient 방식을 사용하는 것이 projection 방식을 사용하는 것 보다 problem를 solution결하기 더 쉽다. reference 흥미to,운 in fact,은, Frankand, Wolfe는 Tuckerand, 함께 일하던 post-doc 였다고 informing,져 있으며. 그들은 first, 첫번째to, 이 algorithm을 2 difference functionto, 제안했다고 한다. and, 그 algorithm은 1956년to, 출판되고, 후to, 논문with,도 발표되었다. and, 이 후to, 오랫during, 더 이image 이to, about, 후속 논문은 전혀 나오지 못했다. however, 지난 몇년 during, Jaggi의 통찰력to, 힘임어 세imageto, 소개되면서 다시 주목을 받게 되었다. Fig 1 Conditional Gradient Frank-Wolfe method From Jaggi 2011 3 Norm constraints norm MATH about, MATH 일 when, 무슨일이 발생할까? 다음을 let's look at > MATH \\begin align > s &∈ \\arg\\min \\|s\\|≤t ∇f x^ k−1 ^Ts \\\\\\ > &= −t · \\arg\\max \\|s\\|≤1 ∇f x^ k−1 ^Ts \\\\\\ > &= −t · ∂ \\| ∇f x^ k−1 \\| ∗ > \\end align MATH 여기서 MATH 는 dual norm을 의마한다. 다시 말solution, dual norm의 subgradient를 computation하는 method을 안다면, Frank-Wolfe step를 쉽게 수행 할 수 있다는 뜻이다. Frank-Wolfe의 핵심은 MATH to, projection method을 사용하는 것보다 더 간단하거나 낮은 비용with, 구할 수 있으며, also, when,to,는 MATH 의 prox operator보다도 간단하거나 더 낮은 비용을 요한다는 것이다. Example: MATH regularization 다음은 MATH -regularized 이다. > MATH 앞선 공식대to, 전개하면, MATH 를 얻을 수 있다. Frank-Wolfe method은 다음의 process을 through, 업데이트 된다. > MATH \\begin array rcl > i k−1 & ∈ & \\arg\\max i=1,...p ∇ i f x^ k−1 \\\\\\ > x^ k & = & 1 − γ k x^ k−1 − γ kt · sign ∇ i k−1 f x^ k−1 · e i k−1 > \\end array MATH 이것은 coordinate descent의 일종이다 coordinate descentabout,서는 나중to, 자세히 let's look at . Note : 두 가지 모두 MATH 의 복잡도가 필요but, MATH ballto, projection 하는 것보다 훨씬 간단하다. Example: MATH regularization 다음은 MATH -regularized problem다. > MATH MATH at, p가 q의 dual일 when, MATH 이다. that is,, MATH 이다. that is, as follows: 선택할 수 있다. > MATH 여기서 MATH 는 MATH and, 같은 constant이고, Frank-Wolfe 업데이트도 동일하다. Note: 일반 MATH 의 case, p Ballto, Projection 하는 것보다 훨씬 간단하다. 특별한 case, MATH 를 제외하고 이러한 projection은 직접 computation할 수 없다 optimizationto, 처리되어야 함 . Example: trace norm regularization trace-regularized problem를 let's look at > MATH MATH 이다. as follows: MATH 를 선택할 수 있다. > MATH 여기서 MATH 는 MATH 의 왼쪽, 오른쪽 singular vector이고, Frank-Wolfe 업데이트는 평소and, 같다. Note: 이 method은 특이 값 분solution SVD 가 가능하면, trace norm ballto, projection 하는 것보다 훨씬 간단하고 효율적with, solution를 구할 수 있는 method이다. Constrained and Lagrange forms 제약 condition,이 있는 problem의 solution을 다시 한번 image기solution보자 > MATH 다음의 Lagrange problem는 위 식and, equivalence이다. > MATH 튜닝 파라미터 MATH and, MATH 는 0,∞ 구간at, 변한다. also, MATH 의 Frank-Wolfe 업데이트를 MATH 의 proximal 오퍼레이터and, comparing,야 한다. • MATH norm : Frank-Wolfe method은 gradient의 최댓값을 스캔하여 업데이트 한다. proximal operator soft-threshold를 진행하면서 업데이트 한다. 두 step 모두 MATH flops을 사용 한다. • MATH norm : 프랭크-울프 Frank-Wolfe 업데이트는 gradient의 각 항목마다 제product하고 모두 sum산하여 MATH flopwith, 증가시킨다. proximal operator는 generally, 직접 computation할 수 없다. • Trace norm : 프랭크-울프 Frank-Wolfe 업데이트는 gradient의 image단 왼쪽 및 오른쪽 singular vector를 computation한다. proximal operatorat,는 soft-thresholds gradient step을 진행하며, 특이값 분solution SVD 를 필요to, 한다. 다른 많은 regularizer들이 효율적인 Frank-Wolfe update를 도출하였다. 예를 들면, special polyhedra or, cone constraints, sum-of-norms group-based regularization, atomic norms. 같은 것들이다. Constrained Lassoto, about, projected gradient techniqueand, conditional gradient technique을 활용했을 when, 성능을 비교하면 as follows:. 여기서 MATH Fig 2 Comparing projected and conditional gradient for constrained lasso problem 3 프랭크-울프 Frank-Wolfe method이 first-order method의 convergence율and, 비슷한 양image을 띠고 있는 것을 확인할 수 있을 것이다. however, actually,는 높은 정확도to, convergence하기 for,서는 속도가 더 느려질 수 있다. reference: 여기서 fixed step size를 사용but,, line search를 using, convergence 속도를 향image시킬 수도 있다. Duality gap 프랭크-울프 Frank-Wolfe iteration processat, 자연스럽게 duality gap 이 발생되며, 이는 actually, suboptimality gap을 의미한다. > MATH 이것은 MATH 의 upper bound 이다. Proof convexity의 first-order condition을 using, 증명할 수 있다. > MATH 모든 MATH about, 양쪽을 minimization 한다. > MATH 최종적with,, 다시 정리하여 다음 식은 duality gap이 upper bound임을 showing, 준다. > MATH Note therefore, 이 quantity는 Frank-Wolfe 업데이트at, 직접 나온 것이다. 왜 우리는 이를 “duality gap”이라 부를까? original problem을 다시 써보면 아래and, 같이 쓸 수있다. > MATH 여기서 MATH 는 MATH 의 indicator function을 의미한다. dual problem는 아래and, 같다. > MATH MATH 가 MATH 의 support function을 의미한다. Indicator function의 conjugate는 support function 이 됨을 앞서 살펴보았다. Recall > MATH > I C X = > \\begin cases > +& \\infty &if &x &\\notin; C \\\\\\ > & 0 &if &x &\\in; C > \\end cases > MATH > MATH > \\begin align > I C^ &= \\max x \\ - I C x \\ \\\\ > &= \\max x \\in C \\\\ > &= \\text Support function of C \\text at S > \\end align > MATH MATH 일 when,, MATH at, 발생하는 duality gap은 as follows:. 13-04 Fenchel's inequality % multilang post url contents/chapter13/21-04-05-13 04 Conjugate function % from, 유도되기도 한다. > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter22/22_02_Conditional_gradient_method/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter22/22_03_Convergence_analysis",
    "title": "22-03 Convergence analysis",
    "chapter": "22",
    "order": 4,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Convergence analysis To find out the convergence characteristics of the Frank-Wolfe method, it is necessary to define the curvature constant of MATH for MATH as follows. Jaggi 2011 > MATH > MATH M을 through,서 actually, function가 선형 approximation linear approximation from, 얼마나 먼 경향을 가지고 있는지를 측정할 수도 있다. 여기서 MATH 은 MATH 가 선형임을 나타낸다. MATH 는 MATH by, 정의 된 Bregman divergence 라 부른다. > Theorem: 고정 스텝 사이즈 MATH 를 이용한 condition,부 그레디언드 method conditional gradient method 은 다음을 만족한다. > MATH MATH 를 만족하기 for, 필요한 iteration 횟수는 MATH 이다. 이제 이 이론은 귀납법with, proving,보고자 한다. however, 바to, 증명with, 넘어가기전 짚고 넘어가야 할 개념을 하나 소개하고자 한다. Basic inequality Frank-Wolfe convergence 속도를 증명하는 데 사용되는 key inequality 는 as follows:. > MATH 여기서 MATH 는 앞서 논의한 duality gap 을 의미하며, 귀납법according to, 이 비율은 inequality를 따르게 된다. Proof Basic inequality를 증명하기 for, MATH 를 지정한다. and, as follows: 정리한다. > MATH \\begin align > f x^+ &= f\\bigl x + γ s − x \\bigr \\\\\\ > &≤ f x + γ∇f x ^T s − x + \\frac γ^2 2 M \\\\\\ > &= f x − γg x + \\frac γ^2 2 M > \\end align MATH 위 수식at, 두 번째 줄은 MATH 의 정의를 사용했고, 세 번째 줄은 MATH 의 정의를 사용하였다. 이제, basic inequality를 using,, 우리는 convergence rate theorem을 증명하기 for, 귀납법을 사용한다. MATH 의 case,, theorem이 만족함을 쉽게 확인할 수 있다. and, 임의의 MATH 일 case,, MATH 를 만족함을 가정한다. 앞서 언급한 duality gap MATH 를 다시 떠올려 보자. > MATH > MATH and, 이제 basic inequalityto, applying, 보자. > MATH > MATH > MATH 이 증명 된 convergence 속도는 ∇f가 립시츠 Lipschitz 일 when, projected gradient descent의 informing,진 속helping, 일치한다. 이제 이 가정 들을 comparing, 보자. in fact, if, MATH 가 constant MATH 을 가지는 Lipschitz라면 MATH 일 when, MATH 이다. 이를 확인하기 for, constant MATH 을 가지는 MATH Lipschitz 아래and, 같다는 것을 image기할 필요가 있다. > MATH 모든 MATH 를 maximizing, MATH 를 product하면 as follows:. > MATH M의 경계가 결정되었다. 기본적with, 경계가 있는 곡률이 proximal gradientabout, 가정한 곡률보다 크지 않다고 가정한다. Affine invariance 앞서 배운 개념들을 다시 생각solution 보자. Gradient Descent: MATH Pure Newton’s Method: MATH Gradient descent는 affine invariant하지 않다. that is,, coordinate들을 스케일링 함with, gradient descent의 성능은 향image 된다. 반면, Newton’s method는 affine invariant하다. that is,, 이 algorithm은 variable의 모든 affine transformationat, 동일하게 동작한다. and, Conditional gradient method는 gradient descentand, 비슷but, affine invariant 하다. Frank-Wolfe의 중요한 속성 : 업데이트는 affine invariant 하다. Nonsingular MATH 가 주어지면, MATH 를 정의할 수 있다. 그러면 MATH at,의 Frank-Wolfe는 아래and, 같이 computation 가능하다. > MATH \\begin array rcl > s' & = & \\arg\\min z∈A^ −1 C ∇h x' ^Tz \\\\\\ > x' ^+ & = & 1 − γ x' + γs' > \\end array MATH MATH to, product하면 MATH at, 수행되는 것and, 동일한 Frank-Wolfe 업데이트가 나타난다. 심지어 convergence analysis은 affine invariant이다. MATH 의 곡률 constant MATH 은 as follows:. > MATH MATH 이기 because of, MATH and, 일치한다. however,, affine invariance는 M의 경계at, 직관적이지 않다. > MATH 주어진 C의 diameter이 affine invariance이 아니라면, 이것은 고민solution 볼 가치가 있다. Inexact updates 정확하지 않은 Frank-Wolfe 업데이트를 분석하였다. Jaggi 2011 MATH 를 선택한다. > MATH MATH 는 부정확한 파라미터이다. 이를 using, 기본적with, 다음and, 같은 비율을 얻게 된다. > Theorem: 고정 스텝 magnitude MATH 및 부정확한 파라미터 δ≥0을 이용한 Conditional gradient method을 using,, 다음을 만족한다. > MATH Note: MATH step의 optimization 오difference는 MATH 이다. 여기서 MATH 이므to, 시간이 지날수록 오difference가 사라지는 것을 의도to, 한다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter22/22_03_Convergence_analysis/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter22/22_04_Properties_and_variants",
    "title": "22-04 Properties and variants",
    "chapter": "22",
    "order": 5,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Some variants Let's look at some variant conditional gradient methods: • Line search : Instead of fixing MATH , we use exact line search for the step size at each MATH . > MATH Backtracking can also be used. • Fully corrective : Direct update according to the following equation. > MATH This method can achieve much better progress, but the cost is high. Fig 3 Away step motivation 3 Another variant: away steps For a faster solution, let's look at the minimization problem in Fig 3 . Here, the optimal solution is 0,0 . The conditional descent method becomes difficult to move from the initial point 0,1 . However, due to away step movement, conditional gradient descent not only moves to promising points but also moves away from unpromising points. Let's assume a convex hull MATH for atoms set MATH We can explicitly represent MATH as a convex combination of elements belonging to MATH . > MATH Conditional gradient with away steps: \\\\ MATH \\\\ MATH \\\\ MATH MATH MATH MATH \\\\ MATH Linear convergence Let's consider the following unconstrained problem. > MATH Here, MATH is µ-strongly convex and MATH is L-Lipschitz. By iterating gradient descent MATH with MATH , the following is satisfied. > MATH Now let's also consider the following constrained problem. > MATH Theorem Lacoste-Julien & Jaggi 2013 Assume that MATH is µ-strongly convex, MATH is L-Lipschitz, and MATH is finite. With appropriate MATH , the iteration steps generated by the conditional gradient algorithm always satisfy the following. > MATH > MATH If the polytope is planar, MATH is small and the algorithm converges slowly. Path following Let's look at the following given norm constrained problem > MATH The Frank-Wolfe algorithm can be used for path following . In other words, it means that it can generate a approximate solution path MATH . Starting with MATH and MATH , fix parameters MATH and then iterate for MATH . Compute MATH and set MATH for all MATH . At MATH , execute Frank-Wolfe to compute MATH and terminate when the duality gap is MATH . This is a method that simplifies existing strategies. Giesen et al. 2012 Through this path following strategy, we can guarantee the following for all visited MATH : > MATH That is, it generates a case of suboptimality gap that is uniformly bounded by MATH for all MATH . As shown in the equation below, the Frank-Wolfe duality gap can be redefined as follows: > MATH This is a linear function with respect to MATH . Therefore, if MATH , we can increase MATH to MATH using the following equation. > MATH That is, the duality gap is maintained at MATH between MATH and MATH for the same MATH .",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter22/22_04_Properties_and_variants/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter23/23_Coordinate_Descent",
    "title": "23 Coordinate Descent",
    "chapter": "23",
    "order": 1,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Coordinate descent is an optimization algorithm that iteratively moves along each coordinate axis to find the minimum of the objective function. At each iteration, according to a coordinate selection rule, it determines a coordinate axis coordinate or coordinate block, then minimizes the function along the direction of that axis while keeping the unselected coordinate axes or coordinate blocks fixed exactly or inexactly . Coordinate descent can be utilized not only with gradient-based methods but also with gradient-free methods. Additionally, depending on the case, line search can be used to determine appropriate step sizes for each axis 16 . Coordinate descent is very simple and easy to implement, and shows very good performance when carefully implemented for appropriate problems. Examples: lasso regression, lasso GLMs under proximal Newton , SVMs, group lasso, graphical lasso applied to the dual , additive modeling, matrix completion, regression with nonconvex penalties References and Further readings Early coordinate descent in optimization: D. Bertsekas and J. Tsitsiklis 1989 , “Parallel and distributed domputation: numerical methods” Z. Luo and P. Tseng 1992 , “On the convergence of the coordinate descent method for convex differentiable minimization” J. Ortega and W. Rheinboldt 1970 , “Iterative solution of nonlinear equations in several variables” P. Tseng 2001 , “Convergence of a block coordinate descent method for nondifferentiable minimization” 35 Early coordinate descent references in statistics and ML: I. Daubechies and M. Defrise and C. De Mol 2004 , “An iterative thresholding algorithm for linear inverse problems with a sparsity constraint” J. Friedman and T. Hastie and H. Hoefling and R. Tibshirani 2007 , “Pathwise coordinate optimization” W. Fu 1998 , “Penalized regressions: the bridge versus the lasso” T. Wu and K. Lange 2008 , “Coordinate descent algorithms for lasso penalized regression” A. van der Kooij 2007 , “Prediction accuracy and stability of regresssion with optimal scaling transformations” Coordinate descent의 응용: O. Banerjee and L. Ghaoui and A. d’Aspremont 2007 , “Model selection through sparse maximum likelihood estimation” J. Friedman and T. Hastie and R. Tibshirani 2007 , “Sparse inverse covariance estimation with the graphical lasso” J. Friedman and T. Hastie and R. Tibshirani 2009 , “Regularization paths for generalized linear models via coordinate descent” C.J. Hsiesh and K.W. Chang and C.J. Lin and S. Keerthi and S. Sundararajan 2008 , “A dual coordinate descent method for large-scale linear SVM” R. Mazumder and J. Friedman and T. Hastie 2011 , “SparseNet: coordinate descent with non-convex penalties” J. Platt 1998 , “Sequential minimal optimization: a fast algorithm for training support vector machines” 37 Recent theory for coordinate descent: A. Beck and L. Tetruashvili 2013 , “On the convergence of block coordinate descent type methods” Y. Nesterov 2010 , “Efficiency of coordinate descent methods on huge-scale optimization problems” J. Nutini, M. Schmidt, I. Laradji, M. Friedlander, H. Koepke 2015 , “Coordinate descent converges faster with the Gauss- Southwell rule than random selection” A. Ramdas 2014 , “Rows vs columns for linear systems of equations—randomized Kaczmarz or coordinate descent?” P. Richtarik and M. Takac 2011 , “Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function” A. Saha and A. Tewari 2013 , “On the nonasymptotic convergence of cyclic coordinate descent methods” S. Wright 2015 , “Coordinate descent algorithms” 38 Screening rules and graphical lasso references: L. El Ghaoui and V. Viallon and T. Rabbani 2010 , “Safe feature elimination in sparse supervised learning” R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R. J. Tibshirani 2011 , “Strong rules for discarding predictors in lasso-type problems” R. Mazumder and T. Hastie 2011 , “The graphical lasso: new insights and alternatives” R. Mazumder and T. Hastie 2011 , “Exact covariance thresholding into connected components for large-scale graphical lasso” J. Wang, P. Wonka, and J. Ye 2015 , “Lasso screening rules via dual polytope projection” D. Witten and J. Friedman and N. Simon 2011 , “New insights and faster computations for the graphical lasso” Convergence analysis: Coordinate descent의 convergence analysisto, about, 연구 흐름을 간략히 소개하겠다. Convergence of coordinatewise minimization for solving linear systems, the Gauss-Seidel method, is a classic topic. E.g., see Golub and van Loan 1996 , or Ramdas 2014 for a modern twist that looks at randomized coordinate descent Nesterov 2010 considers randomized coordinate descent for smooth functions and shows that it achieves a rate O 1/ε under a Lipschitz gradient condition, and a rate O log 1/ε under strong convexity Richtarik and Takac 2011 extend and simplify these results, considering smooth plus separable functions, where now each coordinate descent update applies a prox operation Saha and Tewari 2013 consider minimizing l1 regularized functions of the form g β + λ∥β∥1, for smooth g, and study both cyclic coordinate descent and cyclic coordinatewise min. Under very strange conditions on g, they show both methods dominate proximal gradient descent in iteration progress Beck and Tetruashvili 2013 study cyclic coordinate descent for smooth functions in general. They show that it achieves a rate O 1/ε under a Lipschitz gradient condition, and a rate O log 1/ε under strong convexity. They also extend these results to a constrained setting with projections Nutini et al. 2015 analyze greedy coordinate descent called Gauss-Southwell rule , and show it achieves a faster rate than randomized coordinate descent for certain problems Wright 2015 provides some unification and a great summary. Also covers parallel versions even asynchronous ones General theory is still not complete; still unanswered questions e.g., are descent and minimization strategies the same?",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter23/23_Coordinate_Descent/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter23/23_01_Coordinate_descent",
    "title": "23-01 Coordinate Descent",
    "chapter": "23",
    "order": 2,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "In this chapter, we introduce a method called coordinate descent that is extremely simple, efficient, and highly scalable. First, let's start with some simple questions and answers. Q1. When function MATH is convex and differentiable, if the point where MATH is minimized along each coordinate axis is MATH , is this MATH a global minimizer? A1: Yes. Since MATH , MATH is a global minimizer of MATH . The above question is equivalent to asking whether MATH is satisfied for all MATH when MATH is the MATH -th standard basis vector. That is, since we cannot make MATH smaller by moving in any coordinate axis direction from MATH , the partial derivatives in all axis directions become 0. MATH \\nabla f x = \\big \\frac \\partial f \\partial x 1 x , \\dots, \\frac \\partial f \\partial x n x \\big = 0, \\dots, 0 = 0 MATH MATH Q2. Then, when MATH is convex but 'non-differentiable' function, is the point MATH where MATH is minimized along each coordinate axis always a global minimizer? A2: No. In this case, we cannot assert that MATH is a global minimizer of MATH . Counterexample: Fig2 below Looking at the contour lines on the right side of the counterexample below, we can see that although the marked point is not the global minimum, MATH cannot be made smaller by moving in any coordinate axis direction. To make MATH smaller, one must be able to move inside the contour lines. This is because at this position, all inner regions of the contour lines are contained within the two tangent lines parallel to the coordinate axes. On the other hand, when MATH is a differentiable convex function, only one tangent line exists at any point on the contour lines, so this phenomenon does not occur. MATH Q3. When MATH can be expressed as the sum of a differentiable convex function MATH and a convex function MATH , is the point MATH where MATH is minimized along each coordinate axis always a global minimizer? That is, MATH A3. Yes. This is because it satisfies the following for any MATH . MATH \\begin align f y - f x &\\ge \\nabla g x ^T y-x + \\sum i=1 ^ n \\big h i y i - h i x i \\big \\\\\\\\ &= \\sum i=1 ^ n \\big \\underbrace \\nabla i g x y i - x i + h i y i - h i x i \\ge 0 \\big \\ge 0 \\end align MATH Proof: >Let MATH . MATH means viewing only the MATH -th element of MATH as a variable, and the rest as fixed values. > > MATH > \\begin align > & \\: 0 \\in \\partial F i x i \\\\\\\\ > \\Leftrightarrow & \\: 0 \\in \\ \\nabla i g x \\ + \\partial h i x i \\\\\\\\ > \\Leftrightarrow & \\: - \\nabla i g x \\in \\partial h i x\\ i > \\end align > MATH By the definition of subgradient % multilang post url contents/chapter07/21-03-25-07 01 subgradient % , > MATH > \\begin align > & h i y i \\ge h i x i - \\nabla i g x y i - x i \\\\\\\\ > \\Leftrightarrow & \\nabla i g x y i - x i + h i y i - h i x i \\ge 0. > \\end align > MATH MATH Conclusion The minimizer of MATH with MATH convex, differentiable and MATH convex can be found using coordinate descent . Coordinate descent iterates the following cycle. Assume that an appropriate initial value MATH is set. > Coordinate Descent: > MATH For MATH , > > MATH >\\begin align >x 1^ k &\\in \\text arg \\min x 1 \\: f x 1, x 2^ k-1 , x 3^ k-1 , \\dots, x n^ k-1 \\\\\\\\ >x 2^ k &\\in \\text arg \\min x 2 \\: f x 1^ k , x 2, x 3^ k-1 , \\dots, x n^ k-1 \\\\\\\\ >x 3^ k &\\in \\text arg \\min x 3 \\: f x 1^ k , x 2^ k , x 3, \\dots, x n^ k-1 \\\\\\\\ >& \\dots\\\\\\\\ >x n^ k &\\in \\text arg \\min x n \\: f x 1^ k , x 2^ k , x 3^ k , \\dots, x n >\\end align > MATH Notes: The process of obtaining MATH uses the newly obtained MATH in the MATH -th cycle. The order of coordinate axes in the cycle can be arbitrarily specified. Two or more coordinate axes can be grouped together and processed as blocks. The coordinate descent introduced above corresponds to exact coordinatewise minimization. Another approach is inexact coordinatewise minimization using gradients. Assuming MATH is a differentiable convex function > Coordinate Descent inexact coordinatewise minimization : > MATH For MATH , > > MATH >\\begin align >x 1^ k &= x 1^ k-1 - t k,1 \\cdot \\nabla 1 f x 1^ k-1 , x 2^ k-1 , x 3^ k-1 , \\dots, x n^ k-1 \\\\\\\\ >x 2^ k &= x 2^ k-1 - t k,2 \\cdot \\nabla 2 f x 1^ k , x 2^ k-1 , x 3^ k-1 , \\dots, x n^ k-1 \\\\\\\\ >x 3^ k &= x 3^ k-1 - t k,3 \\cdot \\nabla 3 f x 1^ k , x 2^ k , x 3^ k-1 , \\dots, x n^ k-1 \\\\\\\\ >& \\dots\\\\\\\\ >x n^ k &= x n^ k-1 - t k,n \\cdot \\nabla n f x 1^ k , x 2^ k , x 3^ k , \\dots, x n^ k-1 >\\end align > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter23/23_01_Coordinate_descent/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter23/23_02_Example_linear_regression",
    "title": "23-02 Example: linear regression",
    "chapter": "23",
    "order": 3,
    "owner": "Jinwoo Park",
    "lesson_type": "",
    "content": "Let's define the linear regression problem as follows. > MATH > MATH When MATH are fixed values, let's find MATH that minimizes the given objective function. MATH means the remaining terms excluding MATH . - In the case of MATH , the remaining columns excluding the MATH -th column. MATH \\begin align 0 &= \\nabla i f \\beta \\\\\\\\ &= X i^T X\\beta - y \\\\\\\\ &= X i^T X i \\beta i + X -i \\beta -i - y \\\\\\\\ \\Rightarrow\\\\\\\\ &\\beta i = \\frac X i^T y - X -i \\beta -i X i^T X i \\end align MATH Through coordinate descent, we iterate and update MATH for MATH . Experiment: Convergence speed comparison - GD vs AGD vs CD The graph below shows the convergence speeds of coordinate descent, gradient descent, and accelerated gradient descent for a linear regression problem with MATH . The k on the horizontal axis represents one step GD, AGD or one cycle CD . Fig1 GD vs AGD vs CD 3 According to the above results, coordinate descent shows significantly better convergence speed than AGD, which is optimal among first-order methods. Why can this phenomenon occur? To conclude, coordinate descent can achieve performance that far surpasses AGD because it utilizes more information than first-order methods. This is because coordinate descent uses the latest information updated in the previous step at each step within one cycle. That is, CD is not a first-order method. Q. Then, is it fair to compare one cycle of CD with one step of GD in the above experiment? A. Yes. The CD update formula introduced earlier can be modified to have a time complexity of MATH per step. Then, the time complexity of one cycle for CD becomes MATH , which has the same time complexity as one step of GD. Gradient descent update: MATH , the time complexity of the MATH operation becomes MATH flops.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter23/23_02_Example_linear_regression/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter23/23_03_Example_lasso_regression",
    "title": "23-03 Example: lasso regression",
    "chapter": "23",
    "order": 4,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Lasso regression problem를 아래and, 같이 nonsmooth part가 분리되어있는 objective function의 형태to, 정by,보겠다. > MATH > \\min \\beta \\frac 1 2 \\| y - X\\beta \\| 2^2 + \\lambda \\|\\beta\\| 1 > MATH Note: MATH MATH 가 고정된 값일when,, 주어진 objective function를 minimization시키는 MATH 를 let's find. MATH \\begin align &0 = \\nabla i f \\beta = X i^T X i \\beta i + X i^T X -i \\beta -i - y + \\lambda s i,\\\\\\\\ &\\text where s i \\in \\partial |\\beta i| \\Rightarrow \\beta i = S \\lambda / \\|X i\\| 2^2 \\big \\frac X i^T y-X -i \\beta -i X i^TX i \\big \\end align MATH Solution은 thresholding level이 MATH 인 soft-thresholding functionand,도 같다. Coordinate descent를 through, MATH for MATH 를 iteration하며 업데이트 한다. 실험: convergence속도 비교 - PG vs AGD vs CD 아래 그래프는 MATH 인 lasso regression problemabout, proximal gradient descent, accelerated gradient descent, coordinate descent의 convergence속도를 comparing, showing,준다. 가to,axis의 k는 한 step PD, AGD or, 한 cycle CD 을 나타낸다. Fig1 PD vs AGD vs CD 3 Linear regression의 예시 % multilang post url contents/chapter23/21-03-28-23 02 Example linear regression % at,and, 마찬가지to, lasso regression problemat,도 coordinate descent는 월등한 convergence속도를 보인다. First-order method보다 더 많은 정보를 활용한다. Note: 위 실험at,의 모든 methods는 각 iteration당 MATH flops의 시간복잡도를 보인다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter23/23_03_Example_lasso_regression/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter23/23_04_Example_pathwise_coordinate",
    "title": "23-04 Example: Pathwise coordinate descent for lasso",
    "chapter": "23",
    "order": 5,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "In this section, Pathwise coordinate descent for lassoto, about, 개요를 간단히 소개하도록 한다 Friedman et al. 2007 https://arxiv.org/pdf/0708.1485.pdf Friedman et al. 2009 https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf . > Lasso regression problem: > MATH > \\min \\beta \\frac 1 2 \\| y - X\\beta \\| 2^2 + \\lambda \\|\\beta\\| 1 > MATH 07-03-03 Example: Lasso Optimality Condition % multilang post url contents/chapter07/21-03-25-07 03 03 example lasso optimality condition % at, lasso regression problemto, about, optimality condition을 유도solution 보았다. 위 problemto, about, optimal solution는 다음의 condition,을 만족한다. > > MATH > \\begin align > X 1^T y-X\\beta &= \\lambda v 1\\\\ > X 2^T y-X\\beta &= \\lambda v 2\\\\ > \\dots\\\\ > X p^T y-X\\beta &= \\lambda v p > \\end align > MATH Note: MATH 는 주어진 matrix MATH 의 MATH 번째 열 column 데이터를 의미한다. 여기서 MATH 는 MATH to, about, subgradient다. > MATH v i, i \\in \\ 1,2,\\dots,p \\ = \\begin cases \\ 1 \\ &\\text if MATH \\\\ \\ -1 \\ &\\text if MATH $의 순서를 따라 optimal solution를 computation한다. Tuning parameter MATH at, computation된 result,를 MATH to, about, coordinate descent algorithm을 초기화하는데 사용한다. warm start Inner loop active set strategy : 하나 or, 적은 수의 coordinate cycle을 시행한다. and, 0이 아닌 MATH 의 element를 active set MATH to, 기록한다. MATH to, 기록된 element들about,서만 convergence할 when,to, coordinate cycle을 시행한다. MATH 의 모든 element들about, optimality condition을 확인한다. condition,을 만족하지 않는 element가 있으면 MATH to, 추가하고 step 1with, 다시 돌아간다. Notes 통image적with, pathwise strategy는 problemat, 주어진 MATH to, about, solution를 바to, 구하는 것보다 훨씬 효율적with, 동작한다. Active set strategy는 sparsityabout, 이점이 있다. 이 because of, coordinate descent는 ridge regression보다 lasso regressionat, 훨씬 더 빠르게 동작한다. reference: ridge regressionand, lasso regression의 경향성 분석 https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/ Pathwise coordinate descent for lasso는 lasso regression problemabout, 가장 빠르다고 informing,진 다른 algorithm들to, 비견될만큼 빠르게 동작한다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter23/23_04_Example_pathwise_coordinate/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter24/24_Mixed_integer_programming",
    "title": "24 Mixed Integer Programming (part I)",
    "chapter": "24",
    "order": 1,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "This chapter introduces the definition, relationships, and examples of Mixed Integer Programming, and presents methods for finding optimal solutions by indirectly utilizing relaxation to find solutions for Integer programming.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter24/24_Mixed_integer_programming/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter24/24_01_Definition",
    "title": "24-01 Definition",
    "chapter": "24",
    "order": 2,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "This section aims to explain the basic concepts for solving optimization problems through the mixed integer program approach. Problem definition When some variables in an optimization model have the constraint of being integers, this is called an integer program. > MATH > \\begin align > &\\min x && f x \\\\\\\\ > &\\text subject to && x \\in C \\\\\\\\ > &&&x j \\in \\mathbb Z , j \\in J > \\end align > MATH > MATH > \\begin align > \\text where f: \\mathbb R ^ n \\rightarrow \\mathbb R , \\quad C \\subseteq \\mathbb R ^ n \\quad and \\quad J \\subseteq 1, \\dotsc, n . > \\end align > MATH In the above expression, if MATH satisfies the following, it is called a pure integer program. > MATH \\ MATH \\ Let us assume that both MATH and MATH discussed in this section are convex. Binary variables Looking at some representative examples of integer programs, we can mention yes/no decision problems or logical values. In this case, we define the problem using binary variables and solve the problem to find values of 0 or 1 for the conditions. The combinatorial optimization to be introduced next is directly associated with integer programming. This is because by utilizing binary variables, we can transform existing problems and solve them as new problems. Combinatorial optimization problems are defined using the triple MATH representation. > MATH is a finite ground set MATH is the set of feasible solutions MATH is the cost function The ultimate goal is to solve the following equation through the triple MATH . > MATH > \\begin align \\quad \\min S \\in \\mathcal F & \\sum i \\in S c i \\\\ > \\end align > MATH 많은 결sum optimization combinatorial optimization problem는 binary integer program들to, being used,질 수있다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter24/24_01_Definition/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter24/24_02_Example_of_integer_programs",
    "title": "24-02 Examples of integer programs",
    "chapter": "24",
    "order": 3,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "In this section, let's examine various examples corresponding to Integer programming and learn how they are utilized. Knapsack problem The Knapsack problem is a traditional combinatorial optimization problem where the volume that can be put into the knapsack is limited, constraining the total magnitude of items that can fit inside the knapsack. When this constraint exists, the problem aims to select items with maximum value. This problem can be expressed using binary variables MATH , where MATH takes a value of 0 or 1 depending on whether the MATH -th item is selected or not. > MATH > \\begin align > &\\max x && c^\\intercal x \\\\\\\\ > &\\text subject to && a^\\intercal x \\leq b \\\\\\\\ > &&&x j \\in 0, 1 , j = 1, \\dotsc , n > \\end align > MATH MATH represent the value and magnitude volume of the MATH -th item, respectively. Assignment problem Let's assume there are MATH people and MATH tasks. Each person can be assigned to exactly one task. Here, MATH represents the cost required for person MATH to perform task MATH . The Assignment problem aims to assign MATH people to MATH tasks with the minimum cost. To optimize these conditions, the mathematical formulation is as follows: > MATH > \\begin align > &\\min x &&\\sum i = 1 ^ n \\sum j = 1 ^ n c ij x ij \\\\\\\\ > &\\text subject to &&\\sum i = 1 ^ n x ij = 1, j = 1 \\dotsc n \\\\\\\\ > &&&\\sum j = 1 ^ n x ij = 1, i = 1 \\dotsc n \\\\\\\\ > &&&x ij \\in \\lbrace 0, 1\\rbrace \\quad i = 1 \\dotsc n, \\quad j = 1 \\dotsc n > \\end align > MATH Facility location problem The Facility location problem aims to minimize transportation costs from specific facilities to customers. Let's assume there are depots MATH and customers MATH . The fixed cost MATH is associated with using depot MATH . The transportation cost MATH is the cost incurred when goods delivered to customer MATH are transported from depot MATH . The decisions to be made here are which depots should be operational and which customers should receive deliveries from each depot, with the goal of minimizing both fixed costs and transportation costs by deriving and solving the mathematical formulation. > MATH > \\begin align > &\\min x, y && \\sum i = 1 ^ n f j y j + \\sum i = 1 ^ m \\sum j = 1 ^ n c ij x ij \\\\ > &\\text subject to && \\sum j = 1 ^ n x ij = 1, \\quad i = 1 \\dotsc m \\\\ > &&& x ij \\leq y j , \\quad i = 1 \\dotsc m, \\quad j = 1 \\dotsc n \\\\ > &&& x ij \\in \\lbrace 0, 1\\rbrace \\quad i = 1 \\dotsc m, \\quad j = 1 \\dotsc n \\\\ > &&& y j \\in \\lbrace 0, 1\\rbrace \\quad j = 1 \\dotsc n \\\\ > \\end align > MATH The first constraint means that each customer can receive goods from one depot. The second constraint states that depot MATH must be operational for customer MATH to receive goods from there. Since both MATH and MATH are binary, we can consider MATH constraints. This can also be expressed in a \"marginalized\" form as the following constraint: > MATH Reflecting this, it can be replaced with the following mathematical formulation: > MATH > \\begin align > &\\min x, y &&\\sum i = 1 ^ n f j y j + \\sum i = 1 ^ m \\sum j = 1 ^ n c ij x ij \\\\ > &\\text subject to &&\\sum j = 1 ^ n x ij = 1, \\quad i = 1 \\dotsc n \\\\ > &&& \\sum i = 1 ^ n x ij \\leq m y j , \\quad j = 1 \\dotsc n \\\\ > &&& x ij \\in \\lbrace 0, 1\\rbrace \\quad i = 1 \\dotsc n, \\quad j = 1 \\dotsc n \\\\ > &&& y j \\in \\lbrace 0, 1\\rbrace \\quad j = 1 \\dotsc n \\\\ > \\end align > MATH K-means and K-medoids clustering Clustering is the process of dividing data into similar groups. The K-means algorithm aims to find K clusters by finding MATH center values centroids that minimize the average distance between data points within clusters. The goal is to find a partition MATH for the given data. In this case, the following formula is minimized: MATH where MATH , MATH represents the centroid of cluster MATH . A method that is more robust to outliers than computing centroids by averaging K-means is K-medoids clustering, which sets the center value as the data point closest to the cluster center instead of computing the center values of K clusters using arithmetic mean. That is, K-medoids clustering is a method that considers each data point MATH as a center point and designates the data point that yields the minimum value when computed as the centroid. > MATH > MATH This problem can be transformed and represented as an integer program. First, we define MATH and define the following two binary variables: MATH \\begin align &w i =\\begin cases 1 && \\text if choose x^ i \\text as a centroid \\\\\\\\ 0 && \\text otherwise. \\end cases \\\\\\\\ &z ji =\\begin cases 1 && \\text if x^ j \\text in the cluster with centroid x^ i \\\\\\\\ 0 && \\text otherwise. \\end cases \\end align MATH The K-medoids problem can be defined as an optimization problem as follows: > MATH > \\begin align > &\\min w, z && \\sum i = 1 ^ n \\sum j = 1 ^ n d ij z ji \\\\\\\\ > &\\text subject to && z ji \\leq w i \\\\\\\\ > &&& \\sum i = 1 ^ n w i = k \\\\\\\\ > &&& w ij \\in 0, 1 \\quad i = 1 \\dotsc n \\\\\\\\ > &&& z ji \\in 0, 1 \\quad j, i = 1 \\dotsc n > \\end align > MATH The first constraint means that after the centroid is first determined, we will determine whether MATH belongs to MATH or not. Best subset selection When the conditions MATH are given, the Best subset selection problem is as follows: > MATH > \\begin align > &\\min \\beta &&\\frac 1 2 \\| y - X\\beta \\|^ 2 \\\\\\\\ > &\\text subject to &&\\| \\beta \\| \\leq k\\\\\\\\ > \\end align MATH > MATH \\begin align > \\text where \\| \\beta \\| 0 : = \\text the number of nonzero entries of \\beta. > \\end align > MATH Since MATH is a non-convex constraint, the problem can be solved more easily by transforming it using Integer programming. > MATH > \\begin align > &\\min \\beta, z && \\frac 1 2 \\| y - X\\beta \\|^ 2 \\\\\\\\ > &\\text subject to && | \\beta i | \\leq Mz i \\quad i = 1 \\dotsc n \\\\\\\\ > &&&z ji \\in \\lbrace 0, 1 \\rbrace \\quad i = 1 \\dotsc n \\\\\\\\ > &&&\\sum i = 1 ^ p z i \\leq k > \\end align > MATH Least median of squares regression When the conditions MATH , and MATH are given, if we define MATH , the Least median of squares regression problem is as follows: > MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter24/24_02_Example_of_integer_programs/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter24/24_03_Solving_integer_programs",
    "title": "24-03 Solving integer programs",
    "chapter": "24",
    "order": 4,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "After transforming the mathematical formulation of Integer programming, techniques such as relaxation are needed to solve the problem. Let's examine the constraints that appear in integer programs and what approaches are taken to address the problem. Hardness of integer programs Solving Integer program problems is much more difficult than solving convex optimization problems. General Integer programming is NP-hard https://en.wikipedia.org/wiki/NP-hardness , requiring at least polynomial time without even knowing the possibility of solvability. In this case, by removing constraints on integer constraints and performing convex relaxation, we can obtain a lower bound that approaches the optimal value. When solving problems using convex relaxation, the following limitations may occur: Finding a feasible integer solution can become difficult. The optimal solution obtained under relaxation conditions may be distant from the optimal solution obtained with integer programming. The value after approximation rounding may differ from the optimal value. Algorithmic template for solving integer programs When MATH is convex and includes integrality constraints, the integer program is as follows: > MATH Unlike convex optimization, there are no direct \"optimality conditions\" that prove a feasible point MATH is optimal. Instead, we can use a method that finds approximations of the optimal by finding lower bound MATH and upper bound MATH while approaching MATH . Algorithmic template Observing the decreasing sequence of upper bounds, > MATH Observing the increasing sequence of lower bounds, > MATH For any MATH , the value of MATH is determined within the range where MATH . Primal bounds According to the previous MATH formula, for any feasible MATH , MATH always holds, and in this case, MATH is an upper bound. However, since we cannot always find a feasible MATH , if the MATH value is included in the corresponding set, the problem can be solved easily, but this may not always be the case. Dual bounds Usually also called lower bounds, their values are found through relaxation. Detailed explanations are added in the next section.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter24/24_03_Solving_integer_programs/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter24/24_04_Relaxations",
    "title": "24-04 Relaxations",
    "chapter": "24",
    "order": 5,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "For relaxation, specific conditions must be satisfied, and Convex relaxation and Lagrangian relaxation methods can be utilized. Let's examine the detailed content. Conditions for Relaxations If a general optimization problem is defined as follows: > MATH The relaxation of this problem is defined as follows when represented as an arbitrary optimization problem: > MATH \\begin align > &\\min x \\in Y \\: g x \\\\\\\\ > &\\text such that \\\\\\\\ > &\\text ① X \\subset Y \\quad \\text and \\\\\\\\ > &\\text ② g x \\leq f x \\text for all x \\in X > \\end align MATH If the objective functions MATH and MATH are different, both conditions must be satisfied, and if they are the same, only condition ① needs to be satisfied. By these two conditions, the optimal value of the relaxation becomes a lower bound of the optimal value of the original problem. Convex relaxations When the given problem is as follows: > MATH > \\begin align > &\\min x &&f x \\\\\\\\ > &\\text subject to && x \\in C \\\\\\\\ > &&&x j \\in \\mathbb Z , \\quad j \\in J \\\\\\\\ >\\end align MATH > MATH \\begin align > \\text where f is convex f : \\mathbb R ^ n \\rightarrow \\mathbb R , \\quad C \\in \\mathbb R ^n > \\quad \\text and \\quad J \\in \\lbrace 1 \\dotsc n \\rbrace \\\\ > \\end align > MATH convex relaxation can be expressed as follows: > MATH > \\begin align > &\\min x && f x \\\\\\\\ > &\\text subject to && x \\in C \\\\\\\\ >\\end align MATH > MATH \\begin align >\\text where f is convex f: \\mathbb R ^ n \\rightarrow \\mathbb R , \\quad C \\in \\mathbb R ^n >\\text and \\quad J \\in \\lbrace 1 \\dotsc n \\rbrace \\\\ >\\end align > MATH Lagrangian relaxations MATH 가 convex and, integer constraints를 모두 포함할 when,, as follows: problem를 정의 할 수 있다. > MATH > \\begin align > &\\min x &&f x \\\\\\\\ > &\\text subject to &&Ax \\leq b \\\\\\\\ > &&& x j \\in \\mathbb Z \\quad x \\in X > \\end align > MATH 이 when,, constraints를 objectiveto, 더하여, 어떤 MATH to, about, Lagrangian relaxation을 하면, as follows:. > MATH > \\begin align > L u = &\\min x &&f x + u^ \\top Ax-b \\\\\\\\ > &\\text subject to &&x \\in X > \\end align > MATH Lagrangian form을 through,서 constraint set이 확장되었고, feasible MATH about, MATH 을 만족하므to,, always, MATH 이 성립한다. therefore, MATH 는 임의의 MATH about,서 lower bound이고, 최선의 lower bound는 dual problem MATH 을 solution결함with,써 obtaining,낼 수 있다. MATH 는 convex function의 point-wise minimization이기 because of, concave optimization problem이 된다는 것을 기억하자. 앞서 언급되었던 Facility location problemto, Lagrangian relaxation을 applying, 보면, unconstrained MATH about, 다음 식을 푸는 problemto, 변형된다. > MATH > \\begin align > L u = &\\min x && \\sum i = 1 ^ n f j y j + \\sum i = 1 ^ m \\sum j = 1 ^ n c ij - v i x ij + \\sum i = 1 ^ m v i \\\\\\\\ > &\\text subject to && x ij \\leq y j \\quad i = 1 \\dotsc m, \\quad j = 1 \\dotsc n \\\\\\\\ > &&& x ij , y j \\in \\lbrace 0, 1 \\rbrace \\quad i = 1 \\dotsc m, \\quad j = 1 \\dotsc n > \\end align > MATH 각각의 MATH about, Lagrange relaxation MATH 는 쉽게 풀릴 수 있다 : > MATH > MATH 이는 lower bound MATH and, heuristic primal solution을 도출 할 수 있도록 한다. also, MATH 의 부분미분 subdifferential 을 사용한다면 computation도 쉬워진다. subgradient method를 using, MATH 를 MATH to, transformation시켜서 problem를 풀어갈 수 있다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter24/24_04_Relaxations/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter24/24_05_Branch_and_bound_algorithm",
    "title": "24-05 Branch and bound algorithm (B&B)",
    "chapter": "24",
    "order": 6,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Let's find out the method of solving Integer programs through Branch and bound algorithm and Convex relaxation. Definition and properties Branch and bound algorithm is the most common method for solving integer programs. It is mainly a divide and conquer approach that breaks the original problem into several smaller problems sub-problems to approach the correct answer. When the constraint set MATH is a union of partitions consisting of each MATH , > MATH We can find the optimal solution by partitioning the region and finding the minimum. Any feasible solution of a sub-problem can be set as the upper bound MATH . To obtain the lower bound, we find the lower bound MATH of each sub-problem. Then, if MATH , we exclude the sub-problem MATH corresponding to this part. The Integer Programming problem IP is defined as follows: > MATH > \\begin align > &\\min x &&f x \\\\\\\\ > &\\text subject to && x \\in C \\\\\\\\ > &&&x j \\in \\mathbb Z , \\quad j \\in J \\\\\\\\ > \\end align MATH > MATH \\begin align > \\text where f is convex f : \\mathbb R ^ n \\rightarrow \\mathbb R , \\quad C \\in \\mathbb R ^n \\quad \\text and \\quad J \\in \\lbrace 1 \\dotsc n \\rbrace \\\\ > \\end align > MATH And when the Convex Relaxation CR problem is as follows: > MATH > \\begin align > &\\min x &&f x \\\\\\\\ > &\\text subject to &&x \\in C \\\\\\\\ > \\end align MATH > MATH \\begin align > \\text where f is convex f : \\mathbb R ^ n \\rightarrow \\mathbb R , \\quad C \\in \\mathbb R ^n \\quad \\text and \\quad J \\in \\lbrace 1 \\dotsc n \\rbrace \\\\ > \\end align > MATH The problem is solved recursively. If the constraint set is trivial, solve the CR problem. If the solution is less than the current upper bound, update the upper bound. Stop. If CR is infeasible, then IP is also infeasible. Stop. If the solution MATH of CR is also feasible for IP , then MATH becomes the solution. Stop. Find the lower bound of the problem. If the solution MATH of CR is infeasible for IP , update the lower bound of IP . If the lower bound is greater than the current upper bound, Stop. Split the constraint set and solve each sub-problem recursively. After branching After branching, solve each subproblem. If the lower bound of a subproblem is greater than the current upper bound, there is no need to consider the subproblems below it. The most reliable method for computing the lower bound is through convex relaxation, but other methods e.g., Lagrangian relaxation are also used.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter24/24_05_Branch_and_bound_algorithm/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_Mixed_integer_programming",
    "title": "25 Mixed Integer Programming (part II)",
    "chapter": "25",
    "order": 1,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "This chapter examines the cutting plane algorithm, which can be considered the most core algorithm in Integer Programming IP , and the branch and cut algorithm, which is its practical implementation. We will also examine examples of Integer Programming such as best subset selection and Least mean squares. Reference Belotti, Kirches, Ley er, Linderoth, Luedke, and Mahajan 2012 , \"Mixed-integer nonlinear optimization\" Bertsimas and Mazumder 2016 , \"Best subset selection via a modern optimization lens\" Bertsimas, King, and Mazumder 2014 , \"Least quantile regression via modern optimization\" Conforti, Cornuejols, and Zambelli 2014 , \"Integer programming\" Wolsey 1998 , \"Integer programming\"",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_Mixed_integer_programming/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_01_Cutting_planes",
    "title": "25-01 Cutting Planes",
    "chapter": "25",
    "order": 2,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "The cutting plane method is an approach that changes an integer linear program to a convex problem and finds a solution. If this solution is not included in the original feasible set, it uses cuts to progressively guide the newly obtained solution to be included in the original feasible set by cutting out the region where the solution exists. Here, a cut is a line or hyperplane that cuts the feasible set, also called a cutting plane. Concept of cutting plane Conceptually, it can be thought of as a method that draws a line between the original feasible set and the feasible set to cut out regions that are not part of the original feasible set, as shown in the figure below. Fig1 Cutting Plane Red region: feasible set of the original integer linear program Blue region: feasible set of the convex relaxation problem Green line: cutting plane the cutting plane exists between the blue and red regions The detailed algorithm will be introduced again in the main text. A bit of history on cutting planes It took a very long time for the cutting plane method to develop from theory to a practical method. In 1954, Dantzig, Fulkerson, and Johnson first proposed the cutting plane method to solve the TSP traveling salesman problem , and in 1958, mathematician Gomory proposed a general cutting plane method that could solve arbitrary integer linear programs. However, for about 30 years after that, Gomory cuts remained buried in an impractical state for solving real problems. In 1990, Sebastian Ceria at CMU successfully implemented the cutting plane method using the branch and bound algorithm, which is called branch and cut. Since then, cutting planes have become a core component of commercial optimization solvers.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_01_Cutting_planes/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_01_01_Convexification",
    "title": "25-01-01 Convexification",
    "chapter": "25",
    "order": 3,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Transforming an integer program into an equivalent convex problem is called convexification. When convexification is performed, the feasible set becomes a polyhedron, making it easy to find valid cutting planes for the cutting plane algorithm. Convexification To convexify an integer program, the objective function must be linear. In this case, the constraints of the integer program consist of a convex set MATH and an integer set MATH . > MATH > \\begin align > \\min x & \\quad c^ T x \\\\ > \\text subject to & \\quad x \\in C \\\\ > & \\quad x j \\in \\mathbb Z , \\quad j \\in J \\\\ > \\end align > MATH In this case, the feasible set can be redefined as the convex hull MATH . Using the feasible set defined by this convex hull MATH , we can define a convex problem equivalent to the original problem as follows. This process is called convexification. > MATH > \\begin align > \\min x & \\quad c^ T x \\\\ > \\text subject to & \\quad x \\in S \\\\ > \\end align > MATH In the figure below, the blue region is MATH , the red points are MATH , and the convex hull MATH formed by these two sets is the red region. Fig1 Cutting Plane 출처: https://commons.wikimedia.org/wiki/File:Cutting plane algorithm2.png The reason these two formulations are equivalent is because the objective function is linear. Special case: integer linear programs Let's apply the above convexification process to the following integer linear program. > MATH > \\begin align > \\min x & \\quad c^ T x \\\\ > \\text subject to & \\quad Ax \\le b \\\\ > & \\quad x j \\in \\mathbb Z , \\quad j \\in J \\\\ > \\end align > MATH The convex hull MATH of the integer linear program is defined as follows: > Theorem : If MATH are rational numbers, then the following set is a polygon. MATH So is an integer linear program a linear program? Of course it is. However, in this case, the polyhedron MATH can become a very complex polygon with an exponentially large number of inequalities. Therefore, generally, we need to solve the problem using different methods than those used to solve linear programs.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_01_01_Convexification/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_01_02_Cutting_plane_algorithm",
    "title": "25-01-02 Cutting plane algorithm",
    "chapter": "25",
    "order": 4,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "In this section, we will examine the cutting plane algorithm that can solve integer linear programs. Valid Inequality To define cutting planes, let's first look at what valid inequalities are. An inequality MATH is said to be valid for set MATH if it satisfies the following condition. That is, if a set MATH is contained in the halfspace defined by the inequality MATH , then this inequality can be considered valid for MATH . > MATH for all MATH An inequality must be valid to become a cutting plane. Cutting plane algorithm 이제 다음and, 같은 integer programming이 있을 when, cutting plane algorithm을 let's examine. > MATH > \\begin align > \\min x & \\quad c^ T x \\\\ > \\text subject to & \\quad x \\in C \\\\ > & \\quad x j \\in \\mathbb Z , \\quad j \\in J \\\\ > \\end align > MATH MATH 이다. Cutting plane algorithm 다음 algorithmat, Convex Problem은 CPto, Integer Program은 IPto, 표기한다. 1. MATH with, 두고 MATH 를 computation 2. for MATH MATH if MATH 가 IP feasible이면 MATH 는 optimal solution이므to, Stop함 MATH else MATH MATH about, valid하면서 MATH 를 잘라내는 부등식 MATH , MATH 을 찾음 MATH MATH MATH MATH MATH end if end for 이and, 같은 valid inequality를 cutting plane or, cut 이라고 한다. algorithm의 1step는 convex relaxation을 하여 CP problem를 푸는 step이다. 이떄 feasible set은 MATH 이다. algorithm 2stepat,는 구한 solution가 IPat, feasible하다면 이를 solutionto, 본다. if, feasible하지 않다면 solution인 MATH and, set MATH 를 나누는 valid inequality를 finding, MATH 의 범위를 줄인다. and,, MATH to, 재정의된 CP problem를 풀고 algorithm 2step를 iteration하게 된다. 아래 그림at, polygon은 set MATH 를 representing,며 CP의 solution는 검정색 점with, 표시되어 있다. 이when,, valid inequality는 solution를 잘라내서 set MATH 의 범위를 줄이게 된다. Fig1 Valid Inequality 3 이and, 같이 set MATH 의 범위를 계속solution서 reducing,나가면 IP problem의 convex hull feasible set인 set MATH and, 만나게 되어 IPto, feasible한 solution를 구할 수 있게 된다.",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_01_02_Cutting_plane_algorithm/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_01_03_Gomory_cuts",
    "title": "25-01-03 Gomory cuts (1958)",
    "chapter": "25",
    "order": 5,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Mathematician Gomory devised a method to easily find valid inequalities based on the following fact: > if MATH and MATH is an integer then MATH . That is, if a is an integer, then even if b is rounded down, the relationship that a is less than or equal to b is maintained. Gomory fractional cut Let's say that the feasible set MATH defined by the convex hull of the IP problem mentioned earlier is as follows: MATH In this case, the Gomory fractional cut is defined as follows: MATH There are many ideas that extend this concept. For example, there are Chvatal cuts, split cuts, lift-and-project cuts, etc. The derivation process of Gomory fractional cut is detailed on Wikipedia, so please refer to it. For detailed information, see Cutting-plane method https://en.wikipedia.org/wiki/Cutting-plane method",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_01_03_Gomory_cuts/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_01_04_Branch_and_cut_algorithm",
    "title": "25-01-04 Branch and cut algorithm",
    "chapter": "25",
    "order": 6,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "In 1990, Sebastian Ceria at CMU successfully implemented the cutting plane method using the branch and bound algorithm, which is called branch and cut. Since then, cutting planes have become a core component of commercial optimization solvers. Branch and cut algorithm 다음and, 같은 integer programming problem가 있다고 하자. 이when, MATH 이고 MATH 는 convex이며 MATH 이다. > MATH > \\begin align > \\min x & \\quad f x \\\\ > \\text subject to & \\quad x \\in C \\\\ > & \\quad x j \\in \\mathbb Z , \\quad j \\in J \\\\ > \\end align > MATH Branch and cut algorithm algorithmat, Convex Problem은 CPto, Integer Program은 IPto, 표기한다. 1. 다음 convex relaxation problem를 푼다. > MATH > \\begin align > \\min x & \\quad f x \\\\ > \\text subject to & \\quad x \\in C \\\\ > & \\quad x j \\in \\mathbb Z , \\quad j \\in J \\\\ > \\end align > MATH 2. CR infeasible MATH IP infeasible 3. CR 의 solution MATH 이 IP feasible MATH MATH 는 IP 의 solution 4. CR 의 solution MATH 이 IP infeasible하면 다음 두 가지 중to, 선택 MATH 4.1 cut을 추가하고 step 1to, 간다. MATH 4.2 branchsolution서 iteration적with, subproblem을 푼다. Branch and cut algorithm은 branch and bound and, cutting plane method를 결sum한 algorithmwith,서, step 4at, branch-and-bound를 할지, cut을 할지 선택할 수 있다. Integer programming technology Gurobi, CPLEX, FICOand, 같은 state-of-the-art solver들은 매우 효율적인 simplex, interior-point method 등의 algorithm 구현을 포함하고 있다. particularly,, mixed integer optimization의 case, 대부분의 solver들은 branch and cut algorithm을 사용하고 있으며 이들은 convex relaxationand, warm start의 이점을 많이 활용하고 있다. 약 30년 전to, 비하면 Integer programming의 성능 향image은 매우 비약적이다. therefore,, 그during, 풀지 못했던 실생활의 많은 problem들이 최근to, Integer programming을 through, solution결되고 있으며 computing power가 향image됨according to, 더욱 적극적with, 활용될 전망이다. Algorithmat,의 속도 향image 1990-2016 : over MATH Hardwareat,의 속도 향image 1990-2016 : over MATH Total speedup over MATH billion = MATH",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_01_04_Branch_and_cut_algorithm/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_02_Two_extended_examples",
    "title": "25-02 Two extended examples",
    "chapter": "25",
    "order": 7,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "In this section, we will present two examples of Mixed Integer Programming. Best subset selection Least mean squares",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_02_Two_extended_examples/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_02_01_Best_subset_selection",
    "title": "25-02-01 Best subset selection",
    "chapter": "25",
    "order": 8,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "Best subset selection, one of the representative examples of Integer Programming, is a problem of selecting k entries from MATH entries. Best subset selection When MATH and MATH , the best subset selection problem is as follows: > MATH > \\begin align > \\min \\beta & \\quad \\frac 1 2 \\parallel y - X\\beta \\parallel^ 2 \\\\ > \\text subject to & \\quad \\parallel \\beta \\parallel 0 \\ \\leq k \\\\ > \\end align > MATH Here, MATH is the number of nonzero entries in MATH . Previously in earlier chapters, we defined this type of problem as a Lasso problem and made MATH sparse using the MATH norm. In this problem, it is defined as a problem that constrains the number of non-zero entries using the MATH norm, but since the constraint condition MATH is non-convex, the problem cannot be solved with the convex optimization techniques we have learned so far. Integer programming formulation Then let's reformulate this problem with Integer programming. > MATH > \\begin align > \\min \\beta, z & \\quad \\frac 1 2 \\parallel y - X\\beta \\parallel^ 2 \\\\ > \\text subject to & \\quad \\left\\vert \\beta i \\right\\vert \\leq M i \\cdot z i \\quad i = 1 \\dotsc p \\\\ > & \\quad \\sum i = 1 ^ p z i \\leq k \\\\ > & \\quad z ji \\in \\lbrace 0, 1 \\rbrace \\quad i = 1 \\dotsc p \\\\ > \\end align > MATH Binary variable MATH 를 introducing,서 MATH 의 sum이 MATH 보다 작게 만듦with,써 위의 problemand, 동일solution지게 만들었다. MATH 는 사전to, 알고 있는 MATH 의 image한 값with, MATH and, MATH 를 사전processing,서 computation할 수 있는 값이다. 이제 problem를 Integer Programmingwith, 정의했으므to, 지금from, Integer Programming techniquewith, 풀 수 있다. A clever way to get good feasible solutions problem를 generalizing,서 algorithm을 explaining,보자. Objective function MATH 이 smooth convex이고 MATH 가 L-Lipschitz이라고 하자. > MATH Best subset selection의 case, MATH 이다. Observation as follows: 정의된 MATH function를 through, MATH at, 가장 큰 k개 entry를 구할 수 있다. > MATH 이when,, MATH function는 hard thresholding을 한다. also,, MATH 를 set MATH to, projection한 것with, 볼 수도 있다. Discrete first-order algorithm 이제 gradient descentand, function MATH 를 using,서 algorithm을 정by,보자. 1. MATH with, 시작 2. for MATH MATH end for 위의 process을 iteration하면 MATH to, convergence하게 된다. 이는 위의 minimization problemto, about, local solution이라고 할 수 있다. > MATH result,적with, 이 algorithm은 proximal gradient algorithmwith, 볼 수 있다. 왜냐하면 function MATH 가 proximal operator 역할을 하고 있기 because,이다. Computational results Mixed integer programming gap 아래 그림at, Subset selection problem의 실험 result,를 let's look at. 왼쪽 그래프at, upper bound는 바to, optimal이 되었지만 lower bound는 천천히 올라오다가 upper boundand, 만나는 지점at,야 optimal임을 알게 된다. 왜냐하면 linear programat,는 solution이 optimal인지 체크할 method이 없으며 upper boundand, lower bound가 같아졌을 when, optimal임을 알 수 있게 된다. referenceto, upper boundand, lower bound의 difference를 mixed integer programming gap이라고 한다. 오른쪽 그림은 동일한 실험 result,를 mixed integer programming gap을 작아지는 모습with, showing,주고 있다. 주황색 그래프는 upper boundand, lower bound의 difference이인 mixed integer programming gap을 representing,며 점점 줄어들고 있다. MATH Cold and Warm Starts 다음 그림at, warm start가 cold start보다 전체적with, 성능이 매우 우수함을 showing,주고 있다. Fig2 Cold and Warm Starts 3 Sparsity Detection 다음 그림at,는 MIP Mixed Integer Programming and, Lasso, Step regression, Sparsenet의 sparsity를 비교하고 있다. result,적with, MIP가 가장 sparse한 result,내고 있음을 알 수 있다. Fig3 Sparsity Detection synthetic database 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_02_01_Best_subset_selection/",
    "lang": "en"
  },
  {
    "id": "/contents/en/chapter25/25_02_02_Least_mean_squares",
    "title": "25-02-02 Least mean squares",
    "chapter": "25",
    "order": 9,
    "owner": "YoungJae Choung",
    "lesson_type": "",
    "content": "So far, we have solved regression problems by minimizing the MATH norm or MATH norm of residuals. Is there a more robust method than these methods? When performing regression to minimize the median of residuals, we can achieve more robust regression. This is called Least Median of Squares , and it is robust enough that the estimator does not get corrupted even if about 50% of the data is corrupted. However, this problem is also an NP-Hard problem! This section introduces how to solve the Least Quantile of Squares problem, which generalizes the Least Median of Squares problem, using Integer programming. Least mean squares Let MATH and MATH . And when MATH , let MATH . Observe Least squares LS : MATH Least absolute deviation LAD : MATH Least Median of Squares LMS > MATH Least quantile regression Least Median of Squares problem를 일반화한 Least Quantile of Squareproblem는 as follows: 정의할 수 있다. 여기서 MATH 는 MATH 번째 ordered absolute residual이다. Least Quantile of Squares LQS > MATH Key step in the formulation 이제 Least Quantile of Squareproblem를 Integer Programmingwith, 재정by,보자. 이when,, MATH 의 각 entry MATH about, 다음and, 같은 binary variable을 사용한다. > MATH Integer programming formulation MATH and, MATH 은 thresholdto, 각각의 개수는 MATH 개, MATH 개이다. > MATH > \\begin align > \\min \\beta, \\mu, \\bar \\mu , z, \\gamma & \\quad \\gamma \\\\ > \\text subject to & \\quad \\gamma \\le \\lvert r i \\rvert + \\bar \\mu i , \\quad i = 1, ..., n \\\\ > & \\quad \\gamma \\le \\lvert r i \\rvert - \\mu i , \\quad i = 1, ..., n \\\\ > & \\quad \\bar \\mu i \\le M \\cdot z i , \\quad i = 1, ..., n \\\\ > & \\quad \\mu i \\le M \\cdot 1-z i , \\quad i = 1, ..., n \\\\ > & \\quad \\sum^ p i=1 z i = q \\\\ > & \\quad \\mu i , \\bar \\mu i \\ge 0, \\quad i = 1, ..., n \\\\ > & \\quad z i \\in \\ 0, 1\\ , \\quad i = 1, ..., n \\\\ > \\end align > MATH 이 problemat, 첫번째and, 두번쨰 constraint을 보면 residual의 절대값 MATH 이 포함되어 있어서 convex relaxationwith, 풀 수가 없다. therefore,, 첫번째and, 두번쨰 constraint을 convex functionwith, converting, 주어야 한다. First-order algorithm MATH 는 다음and, 같은 형태to, convex function MATH to, 재정의할 수 있다. > MATH 이when, MATH 는 as follows: 정의된다. > MATH > \\begin align > H q \\beta = \\sum^ n i=q \\lvert y i - x^ T i \\beta \\rvert = & > \\max w \\sum^ n i=1 w i \\lvert y i - x^ T i \\beta \\rvert \\\\ > & \\text subject to \\sum^ n i=1 w i = n − q + 1 \\\\ > &0 \\le w i \\le 1, i = 1, ..., n \\\\ > \\end align > MATH MATH 는 앞서 정의된 MATH 을 작은것from, 큰 순with, 나열할 when,, MATH 번째 이image의 모든 residual의 sum이다. therefore,, MATH 번째 이image의 residual의 sumat, MATH 번째 이image의 residual의 sum을 빼면 MATH 번째의 residual 된다는 것을 알 수 있다. Subgradient algorithmwith, MATH 의 local minimum을 구할 수 있다. For detailed information, see 논문 LEAST QUANTILE REGRESSION VIA MODERN OPTIMIZATION https://arxiv.org/pdf/1310.8625.pdf see Computational results 위의 논문at, Least Quantile of Squareproblem를 실험한 result,는 다음 그래프at, 볼 수 있다. Mixed integer programming gap Fig1 Mixed integer programming gap 3 Cold vs Warm Starts Fig2 Cold vs Warm Starts 3",
    "url": "/optimization-for-data-science-iuh-2025/contents/en/chapter25/25_02_02_Least_mean_squares/",
    "lang": "en"
  }
]