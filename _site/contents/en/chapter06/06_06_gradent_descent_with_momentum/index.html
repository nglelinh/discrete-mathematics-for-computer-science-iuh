<!DOCTYPE html>
<html lang="en">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      06-06 Gradient descent with momentum &middot; Optimization in Data Science
    
  </title>

  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/poole.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/syntax.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/lanyon.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/github-markdown.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/multilang.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/search.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/content-boxes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap">
  
  <!-- Lunr.js for search functionality -->
  <script src="https://unpkg.com/lunr/lunr.js"></script>

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="http://localhost:4000/optimization-for-data-science-iuh-2025/public/logo.png">
  <link rel="shortcut icon" href="http://localhost:4000/optimization-for-data-science-iuh-2025/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/optimization-for-data-science-iuh-2025/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Optimization in Data Science</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/">Home</a>

    

    
    
    
    <!-- Hiển thị các chương có sẵn cho ngôn ngữ hiện tại -->
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter00/">
              00. Basic Mathematical Concepts
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter01/">
              01. Introduction
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter02/">
              02. Convex Sets
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter03/">
              03. Convex Functions
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter04/">
              04. Convex Optimization Basis
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter05/">
              05. Canonical Problems
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter06/">
              06. Gradient Descent
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter07/">
              07. Subgradient
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter08/">
              08. Subgradient Method
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter09/">
              09. Proximal Gradient Descent and Acceleration
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter10/">
              10. Duality in Linear Programs
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter11/">
              11. Duality in General Programs
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter12/">
              12. KKT Conditions
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter13/">
              13. Duality uses and correspondences
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter14/">
              14. Newton's Method
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter16/">
              16. Duality Revisited
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter18/">
              18. Quasi-Newton Methods
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter19/">
              19. Proximal Netwon Method
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
    
    <!-- Nếu không có nội dung cho ngôn ngữ hiện tại, hiển thị thông báo -->
    
    
    <!-- Hiển thị thông tin về tình trạng dịch thuật -->
    
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/optimization-for-data-science-iuh-2025/" title="Home">Optimization in Data Science</a>
            <small></small>
          </h3>
          <!-- Header Actions: Language Toggle and GitHub Link -->
          <div class="header-actions">
            <div class="language-toggle">
              <a href="/optimization-for-data-science-iuh-2025/contents/vi/chapter06/06_06_gradent_descent_with_momentum/" class="language-switch" title="Switch to Vietnamese">Switch to Vietnamese</a>
            </div>
            <a class="github-logo__wrapper" target="_blank" href="https://github.com/nglelinh/optimization-for-data-science-iuh-2025" title="Github">
             <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
            </a>
          </div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">
    06-06 Gradient descent with momentum
    
      
        <span class="lesson-badge required large">Required</span>
      
    
  </h1>
  <script src="https://d3js.org/d3.v7.min.js"></script>

<h2 id="the-problem-with-vanilla-gradient-descent">The Problem with Vanilla Gradient Descent</h2>

<p>Imagine you’re rolling a ball down a valley. Standard gradient descent is like a ball with no memory - at each step, it only considers the current slope and moves accordingly. This can lead to several problems:</p>

<ol>
  <li><strong>Slow convergence in ravines</strong>: When the function has steep gradients in some directions and shallow gradients in others</li>
  <li><strong>Oscillations</strong>: The algorithm may zigzag back and forth across the valley instead of making steady progress</li>
  <li><strong>Getting stuck in poor local minima</strong>: Without momentum, the algorithm may settle in suboptimal solutions</li>
</ol>

<p><strong>Question</strong>: What if our ball could “remember” its previous direction and maintain some velocity?</p>

<h2 id="momentum-adding-memory-to-gradient-descent">Momentum: Adding Memory to Gradient Descent</h2>

<p>Gradient descent with momentum is inspired by physics - specifically, the motion of a ball rolling down a hill with friction. The key insight is to accumulate a velocity vector that combines the current gradient with the previous momentum.</p>

<h3 id="the-momentum-algorithm">The Momentum Algorithm</h3>

<p><strong>Update rules:</strong>
\(\begin{align}
v^{(k)} &amp;= \beta v^{(k-1)} + (1-\beta) \nabla f(x^{(k-1)}) \\
x^{(k)} &amp;= x^{(k-1)} - t v^{(k)}
\end{align}\)</p>

<p>where:</p>
<ul>
  <li>\(v^{(k)}\) is the momentum (velocity) at iteration \(k\)</li>
  <li>\(\beta \in [0,1)\) is the momentum coefficient (typically 0.9 or 0.99)</li>
  <li>\(t &gt; 0\) is the learning rate</li>
  <li>\(v^{(0)} = 0\) (initial velocity is zero)</li>
</ul>

<h3 id="alternative-formulation-nesterov-style">Alternative Formulation (Nesterov-style)</h3>

<p>Some implementations use a slightly different form:
\(\begin{align}
v^{(k)} &amp;= \beta v^{(k-1)} + \nabla f(x^{(k-1)}) \\
x^{(k)} &amp;= x^{(k-1)} - t v^{(k)}
\end{align}\)</p>

<p><strong>Key insight</strong>: The momentum term \(v^{(k)}\) is an exponentially weighted moving average of past gradients.</p>

<h2 id="understanding-the-momentum-coefficient-beta">Understanding the Momentum Coefficient \(\beta\)</h2>

<p>The momentum coefficient \(\beta\) controls how much “memory” the algorithm has:</p>

<ul>
  <li><strong>\(\beta = 0\)</strong>: No momentum, reduces to standard gradient descent</li>
  <li><strong>\(\beta = 0.9\)</strong>: Moderate momentum, commonly used in practice</li>
  <li><strong>\(\beta = 0.99\)</strong>: High momentum, used in some deep learning applications</li>
  <li><strong>\(\beta \to 1\)</strong>: Maximum momentum, but may cause instability</li>
</ul>

<h3 id="exponentially-weighted-moving-average">Exponentially Weighted Moving Average</h3>

<p>The momentum \(v^{(k)}\) can be expanded as:
\(v^{(k)} = (1-\beta) \sum_{i=0}^{k-1} \beta^i \nabla f(x^{(k-1-i)})\)</p>

<p>This shows that momentum gives exponentially decreasing weights to older gradients.</p>

<h2 id="interactive-visualization-gradient-descent-vs-momentum">Interactive Visualization: Gradient Descent vs Momentum</h2>

<div id="momentum-comparison" style="margin: 20px 0;">
    <!-- Parameter Controls -->
    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px; margin-bottom: 15px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; background-color: #f9f9f9;">
        <div>
            <label for="momentum-lr">Learning Rate: </label>
            <input type="range" id="momentum-lr" min="0.01" max="0.3" step="0.01" value="0.1" />
            <span id="momentum-lr-value">0.1</span>
        </div>
        <div>
            <label for="momentum-beta">Momentum (β): </label>
            <input type="range" id="momentum-beta" min="0" max="0.99" step="0.01" value="0.9" />
            <span id="momentum-beta-value">0.9</span>
        </div>
        <div>
            <label for="momentum-speed">Animation Speed: </label>
            <input type="range" id="momentum-speed" min="1" max="10" step="1" value="5" />
            <span id="momentum-speed-value">5</span>
        </div>
    </div>
    
    <!-- Control Buttons -->
    <div style="display: flex; gap: 10px; margin-bottom: 15px; flex-wrap: wrap;">
        <button id="start-vanilla-gd" style="background-color: #4CAF50; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">Start Vanilla GD</button>
        <button id="start-momentum-gd" style="background-color: #FF9800; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">Start Momentum GD</button>
        <button id="start-both" style="background-color: #2196F3; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">Compare Both</button>
        <button id="reset-momentum" style="background-color: #f44336; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">Reset</button>
    </div>
    
    <!-- Visualization Canvas -->
    <div id="momentum-canvas" style="border: 1px solid #ccc; border-radius: 5px; background-color: white; position: relative; overflow: hidden;">
        <svg width="800" height="500" id="momentum-svg"></svg>
    </div>
    
    <!-- Algorithm Status -->
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 15px;">
        <div style="padding: 10px; border: 1px solid #4CAF50; border-radius: 5px; background-color: #f1f8e9;">
            <h4 style="margin: 0 0 5px 0; color: #4CAF50;">Vanilla Gradient Descent</h4>
            <p id="vanilla-gd-info" style="margin: 0; font-size: 14px;">Ready to start</p>
        </div>
        <div style="padding: 10px; border: 1px solid #FF9800; border-radius: 5px; background-color: #fff8e1;">
            <h4 style="margin: 0 0 5px 0; color: #FF9800;">Momentum Gradient Descent</h4>
            <p id="momentum-gd-info" style="margin: 0; font-size: 14px;">Ready to start</p>
        </div>
    </div>
</div>

<script>
document.addEventListener('DOMContentLoaded', function() {
    // Momentum visualization implementation
    const svg = d3.select("#momentum-svg");
    const width = 800, height = 500;
    const margin = {top: 20, right: 20, bottom: 40, left: 40};
    
    // Function to optimize: f(x,y) = 0.5*x^2 + 5*y^2 (elongated bowl)
    function f(x, y) {
        return 0.5 * x * x + 5 * y * y;
    }
    
    function gradient(x, y) {
        return [x, 10 * y];
    }
    
    // Scale setup
    const xScale = d3.scaleLinear()
        .domain([-6, 6])
        .range([margin.left, width - margin.right]);
    
    const yScale = d3.scaleLinear()
        .domain([-3, 3])
        .range([height - margin.bottom, margin.top]);
    
    // Create contour plot
    function createContours() {
        // Create a simple grid-based contour visualization
        const gridSize = 20;
        const contourLevels = [1, 4, 9, 16, 25, 36, 49];
        
        svg.selectAll(".contour").remove();
        
        // Draw contour ellipses for the function f(x,y) = 0.5*x^2 + 5*y^2
        contourLevels.forEach(level => {
            // For f(x,y) = 0.5*x^2 + 5*y^2 = level
            // This is an ellipse: x^2/(2*level) + y^2/(level/5) = 1
            const a = Math.sqrt(2 * level); // semi-major axis in x direction
            const b = Math.sqrt(level / 5); // semi-minor axis in y direction
            
            if (a <= 6 && b <= 3) { // Only draw if within our domain
                svg.append("ellipse")
                    .attr("class", "contour")
                    .attr("cx", xScale(0))
                    .attr("cy", yScale(0))
                    .attr("rx", xScale(a) - xScale(0))
                    .attr("ry", yScale(0) - yScale(b))
                    .attr("fill", "none")
                    .attr("stroke", "#ddd")
                    .attr("stroke-width", 1)
                    .attr("opacity", 0.6);
            }
        });
    }
    
    createContours();
    
    // Add global minimum marker
    svg.append("circle")
        .attr("cx", xScale(0))
        .attr("cy", yScale(0))
        .attr("r", 8)
        .attr("fill", "red")
        .attr("stroke", "white")
        .attr("stroke-width", 2)
        .attr("opacity", 0.8);
    
    svg.append("text")
        .attr("x", xScale(0))
        .attr("y", yScale(0) - 15)
        .attr("text-anchor", "middle")
        .attr("font-size", "12px")
        .attr("font-weight", "bold")
        .attr("fill", "red")
        .text("Global Minimum");
    
    // Add axes
    svg.append("g")
        .attr("transform", `translate(0,${height - margin.bottom})`)
        .call(d3.axisBottom(xScale));
    
    svg.append("g")
        .attr("transform", `translate(${margin.left},0)`)
        .call(d3.axisLeft(yScale));
    
    // Add axis labels
    svg.append("text")
        .attr("x", width / 2)
        .attr("y", height - 5)
        .attr("text-anchor", "middle")
        .attr("font-size", "14px")
        .text("x");
    
    svg.append("text")
        .attr("x", 15)
        .attr("y", height / 2)
        .attr("text-anchor", "middle")
        .attr("font-size", "14px")
        .attr("transform", `rotate(-90, 15, ${height / 2})`)
        .text("y");
    
    // Add title
    svg.append("text")
        .attr("x", width / 2)
        .attr("y", 15)
        .attr("text-anchor", "middle")
        .attr("font-size", "16px")
        .attr("font-weight", "bold")
        .text("f(x,y) = 0.5x² + 5y² - Optimization Paths Comparison");
    
    // Variables for animation
    let vanillaPath = [], momentumPath = [];
    let vanillaRunning = false, momentumRunning = false;
    let vanillaInterval, momentumInterval;
    
    // Algorithm implementations
    function runVanillaGD() {
        const lr = parseFloat(document.getElementById('momentum-lr').value);
        let x = -4, y = 2; // Starting point
        let iteration = 0;
        const maxIterations = 500;
        vanillaPath = [{x: x, y: y}];
        vanillaRunning = true;
        
        vanillaInterval = setInterval(() => {
            const grad = gradient(x, y);
            x -= lr * grad[0];
            y -= lr * grad[1];
            vanillaPath.push({x: x, y: y});
            iteration++;
            
            updateVisualization();
            updateStatus();
            
            // Stop if converged or max iterations reached
            if ((Math.abs(grad[0]) < 0.01 && Math.abs(grad[1]) < 0.01) || iteration >= maxIterations) {
                clearInterval(vanillaInterval);
                vanillaRunning = false;
            }
        }, 1000 / parseFloat(document.getElementById('momentum-speed').value));
    }
    
    function runMomentumGD() {
        const lr = parseFloat(document.getElementById('momentum-lr').value);
        const beta = parseFloat(document.getElementById('momentum-beta').value);
        let x = -4, y = 2; // Starting point
        let vx = 0, vy = 0; // Initial velocity
        let iteration = 0;
        const maxIterations = 500;
        momentumPath = [{x: x, y: y}];
        momentumRunning = true;
        
        momentumInterval = setInterval(() => {
            const grad = gradient(x, y);
            vx = beta * vx + (1 - beta) * grad[0];
            vy = beta * vy + (1 - beta) * grad[1];
            x -= lr * vx;
            y -= lr * vy;
            momentumPath.push({x: x, y: y});
            iteration++;
            
            updateVisualization();
            updateStatus();
            
            // Stop if converged or max iterations reached
            if ((Math.abs(grad[0]) < 0.01 && Math.abs(grad[1]) < 0.01) || iteration >= maxIterations) {
                clearInterval(momentumInterval);
                momentumRunning = false;
            }
        }, 1000 / parseFloat(document.getElementById('momentum-speed').value));
    }
    
    function updateVisualization() {
        // Remove existing paths
        svg.selectAll(".vanilla-path").remove();
        svg.selectAll(".momentum-path").remove();
        svg.selectAll(".vanilla-point").remove();
        svg.selectAll(".momentum-point").remove();
        
        // Draw vanilla GD path
        if (vanillaPath.length > 1) {
            const line = d3.line()
                .x(d => xScale(d.x))
                .y(d => yScale(d.y));
            
            svg.append("path")
                .datum(vanillaPath)
                .attr("class", "vanilla-path")
                .attr("d", line)
                .attr("fill", "none")
                .attr("stroke", "#4CAF50")
                .attr("stroke-width", 2);
            
            // Current point
            const current = vanillaPath[vanillaPath.length - 1];
            svg.append("circle")
                .attr("class", "vanilla-point")
                .attr("cx", xScale(current.x))
                .attr("cy", yScale(current.y))
                .attr("r", 5)
                .attr("fill", "#4CAF50");
        }
        
        // Draw momentum GD path
        if (momentumPath.length > 1) {
            const line = d3.line()
                .x(d => xScale(d.x))
                .y(d => yScale(d.y));
            
            svg.append("path")
                .datum(momentumPath)
                .attr("class", "momentum-path")
                .attr("d", line)
                .attr("fill", "none")
                .attr("stroke", "#FF9800")
                .attr("stroke-width", 2);
            
            // Current point
            const current = momentumPath[momentumPath.length - 1];
            svg.append("circle")
                .attr("class", "momentum-point")
                .attr("cx", xScale(current.x))
                .attr("cy", yScale(current.y))
                .attr("r", 5)
                .attr("fill", "#FF9800");
        }
    }
    
    function updateStatus() {
        // Update vanilla GD status
        if (vanillaPath.length > 0) {
            const current = vanillaPath[vanillaPath.length - 1];
            const fValue = f(current.x, current.y);
            document.getElementById('vanilla-gd-info').innerHTML = 
                `Iteration: ${vanillaPath.length - 1}<br>Position: (${current.x.toFixed(3)}, ${current.y.toFixed(3)})<br>f(x,y): ${fValue.toFixed(4)}`;
        }
        
        // Update momentum GD status
        if (momentumPath.length > 0) {
            const current = momentumPath[momentumPath.length - 1];
            const fValue = f(current.x, current.y);
            document.getElementById('momentum-gd-info').innerHTML = 
                `Iteration: ${momentumPath.length - 1}<br>Position: (${current.x.toFixed(3)}, ${current.y.toFixed(3)})<br>f(x,y): ${fValue.toFixed(4)}`;
        }
    }
    
    // Event listeners
    document.getElementById('start-vanilla-gd').addEventListener('click', () => {
        if (!vanillaRunning) {
            vanillaRunning = true;
            runVanillaGD();
        }
    });
    
    document.getElementById('start-momentum-gd').addEventListener('click', () => {
        if (!momentumRunning) {
            momentumRunning = true;
            runMomentumGD();
        }
    });
    
    document.getElementById('start-both').addEventListener('click', () => {
        if (!vanillaRunning && !momentumRunning) {
            vanillaRunning = true;
            momentumRunning = true;
            runVanillaGD();
            runMomentumGD();
        }
    });
    
    document.getElementById('reset-momentum').addEventListener('click', () => {
        clearInterval(vanillaInterval);
        clearInterval(momentumInterval);
        vanillaRunning = false;
        momentumRunning = false;
        vanillaPath = [];
        momentumPath = [];
        updateVisualization();
        document.getElementById('vanilla-gd-info').innerHTML = 'Ready to start';
        document.getElementById('momentum-gd-info').innerHTML = 'Ready to start';
    });
    
    // Update display values
    document.getElementById('momentum-lr').addEventListener('input', function() {
        document.getElementById('momentum-lr-value').textContent = this.value;
    });
    
    document.getElementById('momentum-beta').addEventListener('input', function() {
        document.getElementById('momentum-beta-value').textContent = this.value;
    });
    
    document.getElementById('momentum-speed').addEventListener('input', function() {
        document.getElementById('momentum-speed-value').textContent = this.value;
    });
});
</script>

<h2 id="advantages-of-momentum">Advantages of Momentum</h2>

<h3 id="1-faster-convergence">1. <strong>Faster Convergence</strong></h3>
<p>Momentum helps the algorithm build up speed in consistent directions, leading to faster convergence especially in:</p>
<ul>
  <li>Functions with valleys or ravines</li>
  <li>Ill-conditioned problems (high condition number)</li>
  <li>Functions with many local minima</li>
</ul>

<h3 id="2-reduced-oscillations">2. <strong>Reduced Oscillations</strong></h3>
<p>In directions where the gradient changes sign frequently, momentum helps smooth out the oscillations by averaging past gradients.</p>

<h3 id="3-escape-from-local-minima">3. <strong>Escape from Local Minima</strong></h3>
<p>The accumulated momentum can help the algorithm “roll through” small local minima and continue toward better solutions.</p>

<h2 id="variants-and-extensions">Variants and Extensions</h2>

<h3 id="1-nesterov-accelerated-gradient-nag">1. <strong>Nesterov Accelerated Gradient (NAG)</strong></h3>
<p>Instead of computing the gradient at the current position, NAG computes it at the “look-ahead” position:</p>

\[\begin{align}
v^{(k)} &amp;= \beta v^{(k-1)} + \nabla f(x^{(k-1)} - \beta v^{(k-1)}) \\
x^{(k)} &amp;= x^{(k-1)} - t v^{(k)}
\end{align}\]

<p><strong>Intuition</strong>: “Look before you leap” - check the gradient at where momentum would take you.</p>

<h4 id="the-problem-with-regular-momentum">The Problem with Regular Momentum</h4>

<p>While momentum helps the ball overcome local minima, there’s a limitation we can observe: when approaching the target, momentum still takes considerable time before stopping. The reason is precisely because of the accumulated velocity.</p>

<h4 id="the-key-insight">The Key Insight</h4>

<p>The fundamental idea is to <strong>predict the future direction</strong> - essentially looking ahead one step! Specifically, if we use the momentum term \(\beta v^{(k-1)}\) for updating, we can approximate the next position as \(x^{(k-1)} - \beta v^{(k-1)}\) (we don’t include the gradient term here as we’ll use it in the final step).</p>

<p>Instead of using the gradient at the current position, NAG takes a step forward and uses the gradient at the anticipated next position.</p>

<h4 id="visual-comparison">Visual Comparison</h4>

<p><strong>With regular momentum</strong>: The update is the sum of two vectors:</p>
<ul>
  <li>Momentum vector (from previous step)</li>
  <li>Gradient at the current position</li>
</ul>

<p><strong>With Nesterov momentum</strong>: The update is the sum of two vectors:</p>
<ul>
  <li>Momentum vector (from previous step)</li>
  <li>Gradient at the <strong>look-ahead</strong> position (where momentum would take us)</li>
</ul>

<p>This “look-ahead” approach allows NAG to make more informed corrections and often leads to faster convergence.</p>

<h3 id="2-adaptive-moment-estimation-adam">2. <strong>Adaptive Moment Estimation (Adam)</strong></h3>
<p>Adam combines momentum with adaptive learning rates for each parameter:</p>

\[\begin{align}
m^{(k)} &amp;= \beta_1 m^{(k-1)} + (1-\beta_1) \nabla f(x^{(k-1)}) \\
v^{(k)} &amp;= \beta_2 v^{(k-1)} + (1-\beta_2) (\nabla f(x^{(k-1)}))^2 \\
x^{(k)} &amp;= x^{(k-1)} - t \frac{m^{(k)}}{\sqrt{v^{(k)}} + \epsilon}
\end{align}\]

<h2 id="practical-implementation-tips">Practical Implementation Tips</h2>

<h3 id="1-choosing-the-momentum-coefficient">1. <strong>Choosing the Momentum Coefficient</strong></h3>
<ul>
  <li><strong>Start with \(\beta = 0.9\)</strong>: A good default for most problems</li>
  <li><strong>Increase to \(\beta = 0.99\)</strong>: For very smooth optimization landscapes</li>
  <li><strong>Decrease to \(\beta = 0.5-0.7\)</strong>: For noisy or non-smooth functions</li>
</ul>

<h3 id="2-learning-rate-adjustment">2. <strong>Learning Rate Adjustment</strong></h3>
<p>When using momentum, you may need to reduce the learning rate compared to vanilla gradient descent, as momentum amplifies the effective step size.</p>

<h3 id="3-warm-up-period">3. <strong>Warm-up Period</strong></h3>
<p>Consider starting with lower momentum and gradually increasing it, as momentum needs time to build up effective velocity.</p>

<h2 id="mathematical-analysis">Mathematical Analysis</h2>

<h3 id="convergence-properties">Convergence Properties</h3>

<p>For strongly convex functions with Lipschitz gradients, momentum gradient descent achieves:</p>
<ul>
  <li><strong>Linear convergence</strong>: \(f(x^{(k)}) - f^* \leq C \rho^k\) for some \(\rho &lt; 1\)</li>
  <li><strong>Improved condition number</strong>: Effective condition number can be improved from \(\kappa\) to \(\sqrt{\kappa}\)</li>
</ul>

<h3 id="heavy-ball-method-connection">Heavy Ball Method Connection</h3>

<p>Momentum gradient descent is closely related to the heavy ball method from classical mechanics:
\(mx'' + \gamma x' + \nabla f(x) = 0\)</p>

<p>This differential equation, when discretized, leads to the momentum update rules.</p>

<h2 id="comparison-summary">Comparison Summary</h2>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Vanilla GD</th>
      <th>Momentum GD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Memory</strong></td>
      <td>No</td>
      <td>Yes (exponential decay)</td>
    </tr>
    <tr>
      <td><strong>Convergence</strong></td>
      <td>Can be slow</td>
      <td>Often faster</td>
    </tr>
    <tr>
      <td><strong>Oscillations</strong></td>
      <td>More prone</td>
      <td>Reduced</td>
    </tr>
    <tr>
      <td><strong>Local minima</strong></td>
      <td>May get stuck</td>
      <td>Better escape</td>
    </tr>
    <tr>
      <td><strong>Hyperparameters</strong></td>
      <td>Learning rate</td>
      <td>Learning rate + momentum</td>
    </tr>
    <tr>
      <td><strong>Computational cost</strong></td>
      <td>Low</td>
      <td>Slightly higher</td>
    </tr>
  </tbody>
</table>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li><strong>Momentum adds memory</strong>: It remembers the direction of previous steps</li>
  <li><strong>Faster convergence</strong>: Especially effective for functions with valleys or ravines</li>
  <li><strong>Reduced oscillations</strong>: Smooths out zigzag behavior</li>
  <li><strong>Widely used</strong>: Foundation for many modern optimization algorithms</li>
  <li><strong>Tunable</strong>: The momentum coefficient \(\beta\) allows fine-tuning for different problems</li>
</ol>

<p><strong>Bottom line</strong>: Momentum is a simple yet powerful enhancement to gradient descent that has stood the test of time and remains relevant in modern machine learning applications.</p>

</div>

<!-- Back to Chapter Home Link -->

  
  
  <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff;">
    <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter06/" style="text-decoration: none; color: #007bff; font-weight: bold;">
      ← Back to Chapter Home
    </a>
  </div>













<div class="related">
  <ul class="related-posts">
    
      
    
      
    
      
    
      
    
      
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_04_gradient_boosting/">
              06-04 Gradient boosting
            </a>
          </h3>
        </li>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <li>
        <h2>Next Post</h2>
        <h3>
          <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_07_regularization_and_loss_functions/">
            06-07 Regularization and Loss Functions
          </a>
        </h3>
      </li>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
  </ul>
</div>



<script src="https://utteranc.es/client.js"
        repo="convex-optimization-for-all/convex-optimization-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/optimization-for-data-science-iuh-2025/public/js/script.js'></script>
    <script src='/optimization-for-data-science-iuh-2025/public/js/multilang.js'></script>
    <script src='/optimization-for-data-science-iuh-2025/public/js/search.js'></script>
  </body>
</html>
