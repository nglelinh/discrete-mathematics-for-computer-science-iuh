<!DOCTYPE html>
<html lang="en">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      06-01 Gradient Descent &middot; Optimization in Data Science
    
  </title>

  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/poole.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/syntax.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/lanyon.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/github-markdown.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/multilang.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/search.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/content-boxes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap">
  
  <!-- Lunr.js for search functionality -->
  <script src="https://unpkg.com/lunr/lunr.js"></script>

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="http://localhost:4000/optimization-for-data-science-iuh-2025/public/logo.png">
  <link rel="shortcut icon" href="http://localhost:4000/optimization-for-data-science-iuh-2025/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/optimization-for-data-science-iuh-2025/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Optimization in Data Science</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/">Home</a>

    

    
    
    
    <!-- Hiển thị các chương có sẵn cho ngôn ngữ hiện tại -->
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter00/">
              00. Basic Mathematical Concepts
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter01/">
              01. Introduction
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter02/">
              02. Convex Sets
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter03/">
              03. Convex Functions
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter04/">
              04. Convex Optimization Basis
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter05/">
              05. Canonical Problems
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter06/">
              06. Gradient Descent
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter07/">
              07. Subgradient
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter08/">
              08. Subgradient Method
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter09/">
              09. Proximal Gradient Descent and Acceleration
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter10/">
              10. Duality in Linear Programs
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter11/">
              11. Duality in General Programs
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter12/">
              12. KKT Conditions
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter13/">
              13. Duality uses and correspondences
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter14/">
              14. Newton's Method
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter16/">
              16. Duality Revisited
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter18/">
              18. Quasi-Newton Methods
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter19/">
              19. Proximal Netwon Method
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
    
    <!-- Nếu không có nội dung cho ngôn ngữ hiện tại, hiển thị thông báo -->
    
    
    <!-- Hiển thị thông tin về tình trạng dịch thuật -->
    
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/optimization-for-data-science-iuh-2025/" title="Home">Optimization in Data Science</a>
            <small></small>
          </h3>
          <!-- Header Actions: Language Toggle and GitHub Link -->
          <div class="header-actions">
            <div class="language-toggle">
              <a href="/optimization-for-data-science-iuh-2025/contents/vi/chapter06/06_01_gradient_descent/" class="language-switch" title="Switch to Vietnamese">Switch to Vietnamese</a>
            </div>
            <a class="github-logo__wrapper" target="_blank" href="https://github.com/nglelinh/optimization-for-data-science-iuh-2025" title="Github">
             <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
            </a>
          </div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">
    06-01 Gradient Descent
    
      
        <span class="lesson-badge required large">Required</span>
      
    
  </h1>
  <p>Gradient descent is the simplest algorithm for solving unconstrained convex and differentiable optimization problems.</p>

<blockquote>
  <p>\(\min_x f(x),\)
where \(f\) is differentiable and \(dom(f) = \mathbb{R}^n\).</p>
</blockquote>

<p>The optimal value is \(f^* = \min_x f(x)\), and the optimizer is \(x^*\).</p>

<h2 id="why-gradient-descent-matters-in-data-science">Why Gradient Descent Matters in Data Science</h2>

<p><strong>Gradient descent is the workhorse of machine learning!</strong> It’s the algorithm behind:</p>

<ul>
  <li><strong>Training neural networks</strong>: Backpropagation uses gradient descent to update weights</li>
  <li><strong>Linear regression</strong>: Finding optimal coefficients to minimize MSE</li>
  <li><strong>Logistic regression</strong>: Optimizing parameters for classification</li>
  <li><strong>Deep learning</strong>: Training complex models with millions of parameters</li>
  <li><strong>Recommendation systems</strong>: Learning user preferences and item features</li>
</ul>

<p><strong>Key insight</strong>: Every time you see “training” or “learning” in ML, gradient descent (or its variants) is likely involved!</p>

<h2 id="gradient-descent-for-single-variable-functions">Gradient Descent for Single Variable Functions</h2>

<p>For functions of a single variable \(f: \mathbb{R} \to \mathbb{R}\), gradient descent simplifies significantly. The gradient becomes the derivative, and the update rule becomes:</p>

<blockquote>
\[x^{(k)} = x^{(k-1)} - t f'(x^{(k-1)}), \quad k = 1, 2, 3, ...\]
</blockquote>

<p>where \(f'(x)\) is the derivative of \(f\) at point \(x\).</p>

<h3 id="geometric-interpretation">Geometric Interpretation</h3>

<p>In the single variable case, the derivative \(f'(x)\) represents the slope of the tangent line at point \(x\):</p>
<ul>
  <li>If \(f'(x) &gt; 0\), the function is increasing, so we move left (subtract a positive value)</li>
  <li>If \(f'(x) &lt; 0\), the function is decreasing, so we move right (subtract a negative value)</li>
  <li>If \(f'(x) = 0\), we have reached a critical point (potential minimum)</li>
</ul>

<h3 id="learning-rate-t">Learning rate \(t\)</h3>

<p>The learning rate \(t\) (also called step size) is a crucial hyperparameter that can be set by the algorithm designer. It controls how large steps we take in the direction of the negative gradient.</p>

<p><strong>Impact of Learning Rate:</strong></p>

<ul>
  <li>
    <p><strong>Too small (\(t \ll 1\))</strong>: The algorithm will update very slowly, requiring many iterations to converge to the optimal solution. While this ensures stability, it can be computationally expensive.</p>
  </li>
  <li>
    <p><strong>Too large (\(t \gg 1\))</strong>: The algorithm may overshoot the minimum and potentially diverge, oscillating around the optimal point or even moving away from it.</p>
  </li>
  <li>
    <p><strong>Just right</strong>: The algorithm converges quickly and smoothly to the optimal solution.</p>
  </li>
</ul>

<p><strong>Common strategies for choosing learning rate:</strong></p>
<ol>
  <li><strong>Fixed step size</strong>: Use a constant value throughout the optimization</li>
  <li><strong>Exact line search</strong>: At each iteration, choose \(t\) to minimize \(f(x^{(k-1)} - t\nabla f(x^{(k-1)}))\)</li>
  <li><strong>Backtracking line search</strong>: Start with a large step size and reduce it until sufficient decrease is achieved</li>
  <li><strong>Adaptive methods</strong>: Adjust the learning rate based on the optimization progress (e.g., Adam, RMSprop)</li>
</ol>

<h3 id="example-quadratic-function">Example: Quadratic Function</h3>

<p>Consider the quadratic function \(f(x) = \frac{1}{2}(x - 2)^2 + 1\) with derivative \(f'(x) = x - 2\).</p>

<p>The gradient descent update becomes:</p>
<blockquote>
\[x^{(k)} = x^{(k-1)} - t(x^{(k-1)} - 2)\]
</blockquote>

<p>Starting from \(x^{(0)} = 0\) with step size \(t = 0.1\):</p>
<blockquote>
  <p>\(x^{(1)} = 0 - 0.1(0 - 2) = 0.2 \\\) 
\(x^{(2)} = 0.2 - 0.1(0.2 - 2) = 0.38\)
\(x^{(3)} = 0.38 - 0.1(0.38 - 2) = 0.542\)
…</p>
</blockquote>

<p>The sequence converges to \(x^* = 2\), which is the global minimum.</p>

<h3 id="step-size-selection">Step Size Selection</h3>

<p>The choice of step size \(t\) is crucial:</p>
<ul>
  <li><strong>Too small</strong>: Convergence is very slow</li>
  <li><strong>Too large</strong>: The algorithm may overshoot and diverge</li>
  <li><strong>Optimal</strong>: For quadratic functions \(f(x) = \frac{1}{2}ax^2 + bx + c\) with \(a &gt; 0\), the optimal step size is \(t = \frac{1}{a}\)</li>
</ul>

<h3 id="interactive-visualization">Interactive Visualization</h3>

<div id="single-var-gradient-descent" style="margin: 20px 0;">
    <div style="margin-bottom: 15px;">
        <label for="step-size-slider">Step Size (t): <span id="step-size-value">0.1</span></label><br />
        <input type="range" id="step-size-slider" min="0.01" max="0.5" step="0.01" value="0.1" style="width: 300px;" />
    </div>
    
    <div style="margin-bottom: 15px;">
        <label for="start-point-slider">Starting Point: <span id="start-point-value">-3</span></label><br />
        <input type="range" id="start-point-slider" min="-5" max="5" step="0.1" value="-3" style="width: 300px;" />
    </div>
    
    <div style="margin-bottom: 15px;">
        <button id="start-animation">Start Animation</button>
        <button id="reset-animation">Reset</button>
        <button id="step-once">Single Step</button>
    </div>
    
    <canvas id="gradient-canvas" width="600" height="400" style="border: 1px solid #ccc; display: block; margin: 0 auto;"></canvas>
    
    <div id="iteration-info" style="text-align: center; margin-top: 10px; font-family: monospace;">
        Iteration: 0, x = -3.000, f(x) = 13.500, f'(x) = -5.000
    </div>
</div>

<script>
class SingleVarGradientDescent {
    constructor() {
        this.canvas = document.getElementById('gradient-canvas');
        this.ctx = this.canvas.getContext('2d');
        this.stepSizeSlider = document.getElementById('step-size-slider');
        this.startPointSlider = document.getElementById('start-point-slider');
        this.stepSizeValue = document.getElementById('step-size-value');
        this.startPointValue = document.getElementById('start-point-value');
        this.iterationInfo = document.getElementById('iteration-info');
        
        // Animation state
        this.isAnimating = false;
        this.currentX = -3;
        this.iteration = 0;
        this.history = [];
        this.animationId = null;
        
        // Function parameters: f(x) = 0.5 * (x - 2)^2 + 1
        this.a = 0.5;
        this.b = 2;
        this.c = 1;
        
        this.setupEventListeners();
        this.reset();
    }
    
    setupEventListeners() {
        this.stepSizeSlider.addEventListener('input', (e) => {
            this.stepSizeValue.textContent = e.target.value;
        });
        
        this.startPointSlider.addEventListener('input', (e) => {
            this.startPointValue.textContent = e.target.value;
            if (!this.isAnimating) {
                this.reset();
            }
        });
        
        document.getElementById('start-animation').addEventListener('click', () => {
            this.startAnimation();
        });
        
        document.getElementById('reset-animation').addEventListener('click', () => {
            this.reset();
        });
        
        document.getElementById('step-once').addEventListener('click', () => {
            this.singleStep();
        });
    }
    
    // Function: f(x) = 0.5 * (x - 2)^2 + 1
    f(x) {
        return this.a * Math.pow(x - this.b, 2) + this.c;
    }
    
    // Derivative: f'(x) = (x - 2)
    fprime(x) {
        return 2 * this.a * (x - this.b);
    }
    
    // Convert x coordinate to canvas coordinate
    xToCanvas(x) {
        const xMin = -5, xMax = 5;
        return (x - xMin) / (xMax - xMin) * this.canvas.width;
    }
    
    // Convert y coordinate to canvas coordinate
    yToCanvas(y) {
        const yMin = 0, yMax = 15;
        return this.canvas.height - (y - yMin) / (yMax - yMin) * this.canvas.height;
    }
    
    // Convert canvas x to actual x
    canvasToX(canvasX) {
        const xMin = -5, xMax = 5;
        return xMin + (canvasX / this.canvas.width) * (xMax - xMin);
    }
    
    drawFunction() {
        this.ctx.strokeStyle = '#2196F3';
        this.ctx.lineWidth = 2;
        this.ctx.beginPath();
        
        for (let canvasX = 0; canvasX <= this.canvas.width; canvasX += 2) {
            const x = this.canvasToX(canvasX);
            const y = this.f(x);
            const canvasY = this.yToCanvas(y);
            
            if (canvasX === 0) {
                this.ctx.moveTo(canvasX, canvasY);
            } else {
                this.ctx.lineTo(canvasX, canvasY);
            }
        }
        this.ctx.stroke();
    }
    
    drawAxes() {
        this.ctx.strokeStyle = '#666';
        this.ctx.lineWidth = 1;
        
        // X-axis
        const yZero = this.yToCanvas(0);
        this.ctx.beginPath();
        this.ctx.moveTo(0, yZero);
        this.ctx.lineTo(this.canvas.width, yZero);
        this.ctx.stroke();
        
        // Y-axis
        const xZero = this.xToCanvas(0);
        this.ctx.beginPath();
        this.ctx.moveTo(xZero, 0);
        this.ctx.lineTo(xZero, this.canvas.height);
        this.ctx.stroke();
        
        // Labels
        this.ctx.fillStyle = '#666';
        this.ctx.font = '12px Arial';
        this.ctx.textAlign = 'center';
        
        // X-axis labels
        for (let x = -4; x <= 4; x += 2) {
            const canvasX = this.xToCanvas(x);
            this.ctx.fillText(x.toString(), canvasX, yZero + 15);
        }
        
        // Y-axis labels
        this.ctx.textAlign = 'right';
        for (let y = 2; y <= 14; y += 2) {
            const canvasY = this.yToCanvas(y);
            this.ctx.fillText(y.toString(), xZero - 5, canvasY + 4);
        }
    }
    
    drawCurrentPoint() {
        const canvasX = this.xToCanvas(this.currentX);
        const canvasY = this.yToCanvas(this.f(this.currentX));
        
        // Current point
        this.ctx.fillStyle = '#F44336';
        this.ctx.beginPath();
        this.ctx.arc(canvasX, canvasY, 6, 0, 2 * Math.PI);
        this.ctx.fill();
        
        // Tangent line
        const derivative = this.fprime(this.currentX);
        const tangentLength = 1;
        const x1 = this.currentX - tangentLength;
        const x2 = this.currentX + tangentLength;
        const y1 = this.f(this.currentX) + derivative * (x1 - this.currentX);
        const y2 = this.f(this.currentX) + derivative * (x2 - this.currentX);
        
        this.ctx.strokeStyle = '#FF9800';
        this.ctx.lineWidth = 2;
        this.ctx.beginPath();
        this.ctx.moveTo(this.xToCanvas(x1), this.yToCanvas(y1));
        this.ctx.lineTo(this.xToCanvas(x2), this.yToCanvas(y2));
        this.ctx.stroke();
    }
    
    drawHistory() {
        if (this.history.length < 2) return;
        
        this.ctx.strokeStyle = '#4CAF50';
        this.ctx.lineWidth = 2;
        this.ctx.setLineDash([5, 5]);
        this.ctx.beginPath();
        
        for (let i = 0; i < this.history.length; i++) {
            const x = this.history[i];
            const canvasX = this.xToCanvas(x);
            const canvasY = this.yToCanvas(this.f(x));
            
            if (i === 0) {
                this.ctx.moveTo(canvasX, canvasY);
            } else {
                this.ctx.lineTo(canvasX, canvasY);
            }
        }
        this.ctx.stroke();
        this.ctx.setLineDash([]);
        
        // Draw history points
        this.ctx.fillStyle = '#4CAF50';
        for (let i = 0; i < this.history.length - 1; i++) {
            const x = this.history[i];
            const canvasX = this.xToCanvas(x);
            const canvasY = this.yToCanvas(this.f(x));
            
            this.ctx.beginPath();
            this.ctx.arc(canvasX, canvasY, 3, 0, 2 * Math.PI);
            this.ctx.fill();
        }
    }
    
    draw() {
        // Clear canvas
        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        
        // Draw components
        this.drawAxes();
        this.drawFunction();
        this.drawHistory();
        this.drawCurrentPoint();
        
        // Update iteration info
        this.iterationInfo.textContent = 
            `Iteration: ${this.iteration}, x = ${this.currentX.toFixed(3)}, ` +
            `f(x) = ${this.f(this.currentX).toFixed(3)}, f'(x) = ${this.fprime(this.currentX).toFixed(3)}`;
    }
    
    singleStep() {
        if (Math.abs(this.fprime(this.currentX)) < 1e-6) {
            return; // Already at minimum
        }
        
        const stepSize = parseFloat(this.stepSizeSlider.value);
        const newX = this.currentX - stepSize * this.fprime(this.currentX);
        
        this.history.push(this.currentX);
        this.currentX = newX;
        this.iteration++;
        
        this.draw();
    }
    
    startAnimation() {
        if (this.isAnimating) {
            this.stopAnimation();
            return;
        }
        
        this.isAnimating = true;
        document.getElementById('start-animation').textContent = 'Stop Animation';
        
        const animate = () => {
            if (!this.isAnimating) return;
            
            if (Math.abs(this.fprime(this.currentX)) > 1e-6 && this.iteration < 100) {
                this.singleStep();
                this.animationId = setTimeout(animate, 500);
            } else {
                this.stopAnimation();
            }
        };
        
        animate();
    }
    
    stopAnimation() {
        this.isAnimating = false;
        document.getElementById('start-animation').textContent = 'Start Animation';
        if (this.animationId) {
            clearTimeout(this.animationId);
            this.animationId = null;
        }
    }
    
    reset() {
        this.stopAnimation();
        this.currentX = parseFloat(this.startPointSlider.value);
        this.iteration = 0;
        this.history = [];
        this.draw();
    }
}

// Initialize when DOM is loaded
document.addEventListener('DOMContentLoaded', function() {
    new SingleVarGradientDescent();
});
</script>

<h2 id="gradient-descent-method-for-multivariables-function">Gradient Descent Method for Multivariables function</h2>

<p>Gradient descent starts from an initial point \(x^{(0)} \in \mathbb{R}^n\) and iteratively updates as follows until a stopping criterion is met:</p>
<blockquote>
  <p>\(x^{(k)} = x^{(k-1)} - t \nabla f(x^{(k-1)}), \quad k = 1, 2, 3, ...\), \(t &gt; 0\)</p>
</blockquote>

<p>Pseudocode:</p>
<blockquote>
  <p><strong>Given a starting point</strong> \(x \in dom(f)\) <br />
<strong>Repeat</strong>  <br /></p>
  <ol>
    <li>Determine descent direction \(\Delta x = -\nabla f(x)\). <br /></li>
    <li>Line search: choose step size \(t &gt; 0\). <br /></li>
    <li>Update \(x = x + t \Delta x\). <br />
<strong>Until</strong> stopping criterion is satisfied <br /></li>
  </ol>
</blockquote>

<h3 id="examples">Examples</h3>

<p>The figure below shows gradient descent on a convex function. In this case, the local minimum is also the global minimum.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/optimization-for-data-science-iuh-2025/img/chapter_img/chapter06/06_01_gradientdescent1.png" alt="gradientdescent1" width="80%" height="80%" />
  <figcaption style="text-align: center;">[Fig 1] Gradient descent in convex functions[3]</figcaption>
</p>
</figure>

<p>The next figure shows gradient descent on a non-convex function. Here, the initial point determines which local minimum is reached.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/optimization-for-data-science-iuh-2025/img/chapter_img/chapter06/06_01_gradientdescent2.png" alt="gradientdescent2" width="80%" height="80%" />
  <figcaption style="text-align: center;">[Fig 2] Gradient descent in non-convex functions[3]</figcaption>
</p>
</figure>

<h2 id="gradient-descent-interpretation">Gradient Descent Interpretation</h2>
<p>Gradient descent can be interpreted as choosing the next point by minimizing a quadratic approximation of the function.</p>

<p>For a function \(f\), the second-order Taylor expansion around \(x\) is:</p>

<blockquote>
\[f(y) = f(x) + \nabla f(x)^T (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f(x) (y - x) + \text{higher-order terms}.\]
</blockquote>

<blockquote>
\[f(y) \approx f(x) + \nabla f(x)^T (y - x) +  \frac{1}{2} \nabla^2 f(x)  \|y - x\|_2^2\]
</blockquote>

<p>If we approximate the Hessian \(\nabla^2 f(x)\) by \(\frac{1}{t}I\), then:</p>
<blockquote>
  <p>\(f(y) \approx f(x) + \nabla f(x)^T (y - x) +  \frac{1}{2t}  \|y - x\|_2^2\)
where \(t\) is the step size.</p>
</blockquote>

<p>Thus, in gradient descent, the function is approximated by a quadratic function whose Hessian matrix has eigenvalues equal to the reciprocal of the step size. The term \(f(x) + \nabla f(x)^T (y - x)\) represents a linear approximation of \(f\), and \(\frac{1}{2t}  \|y - x\|_2^2\) serves as a proximity term indicating how close \(y\) is to \(x\).</p>

<p>The next position is chosen as the minimum of this approximated quadratic function. Setting the gradient of \(f(y)\) to zero to find the next position \(y = x^+\) leads to:</p>

<blockquote>
\[x^+ = x - t \nabla f(x)\]
</blockquote>

<p>In the illustration below, the blue dot represents the current position \(x\), and the red dot represents the next position \(y\). The curve below is the actual function \(f\), and the curve above is the quadratic approximation of \(f\). Hence, the red dot indicates the minimum of the quadratic approximation.</p>

<figure class="image" style="align: center;">
<p align="center">
  <img src="/optimization-for-data-science-iuh-2025/img/chapter_img/chapter06/06_01_gradientdescent3.png" alt="gradientdescent3" width="80%" height="80%" />
  <figcaption style="text-align: center;">$$ \text{[Fig 3] Gradient descent algorithm : red dot is } x^+ \text{ and blue dot } x \text{ [3]} $$</figcaption>
</p>
</figure>

<p>The proximity of the next position \(y\) to the current position \(x\) is influenced by the weight of the proximity term \(\frac{1}{2t}\). A smaller \(t\) results in a larger weight for the proximity term, leading to smaller steps. This process can be expressed as:</p>

<blockquote>
  <p>\begin{align}
x^+ = \underset{y}{\arg \min} \ f(x) + \nabla f(x)^T (y - x) + \frac{1}{2t} \parallel y - x \parallel_2^2
\end{align}</p>
</blockquote>

</div>

<!-- Back to Chapter Home Link -->

  
  
  <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff;">
    <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter06/" style="text-decoration: none; color: #007bff; font-weight: bold;">
      ← Back to Chapter Home
    </a>
  </div>













<div class="related">
  <ul class="related-posts">
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_00_gradient_descent/">
              06 Gradient Descent
            </a>
          </h3>
        </li>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <li>
        <h2>Next Post</h2>
        <h3>
          <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter06/06_02_how_to_choose_step_sizes/">
            06-02 How to choose step sizes
          </a>
        </h3>
      </li>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
  </ul>
</div>



<script src="https://utteranc.es/client.js"
        repo="convex-optimization-for-all/convex-optimization-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/optimization-for-data-science-iuh-2025/public/js/script.js'></script>
    <script src='/optimization-for-data-science-iuh-2025/public/js/multilang.js'></script>
    <script src='/optimization-for-data-science-iuh-2025/public/js/search.js'></script>
  </body>
</html>
