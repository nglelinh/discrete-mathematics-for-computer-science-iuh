<!DOCTYPE html>
<html lang="en">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      05-02-01 Linear Least-Squares Problems &middot; Optimization in Data Science
    
  </title>

  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/poole.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/syntax.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/lanyon.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/github-markdown.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/multilang.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/search.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/content-boxes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap">
  
  <!-- Lunr.js for search functionality -->
  <script src="https://unpkg.com/lunr/lunr.js"></script>

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="http://localhost:4000/optimization-for-data-science-iuh-2025/public/logo.png">
  <link rel="shortcut icon" href="http://localhost:4000/optimization-for-data-science-iuh-2025/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/optimization-for-data-science-iuh-2025/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Optimization in Data Science</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/">Home</a>

    

    
    
    
    <!-- Hiển thị các chương có sẵn cho ngôn ngữ hiện tại -->
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter00/">
              00. Basic Mathematical Concepts
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter01/">
              01. Introduction
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter02/">
              02. Convex Sets
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter03/">
              03. Convex Functions
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter04/">
              04. Convex Optimization Basis
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter05/">
              05. Canonical Problems
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter06/">
              06. Gradient Descent
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter07/">
              07. Subgradient
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter08/">
              08. Subgradient Method
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter09/">
              09. Proximal Gradient Descent and Acceleration
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter10/">
              10. Duality in Linear Programs
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter11/">
              11. Duality in General Programs
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter12/">
              12. KKT Conditions
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter13/">
              13. Duality uses and correspondences
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter14/">
              14. Newton's Method
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter16/">
              16. Duality Revisited
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter18/">
              18. Quasi-Newton Methods
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/en/chapter19/">
              19. Proximal Netwon Method
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
    
    <!-- Nếu không có nội dung cho ngôn ngữ hiện tại, hiển thị thông báo -->
    
    
    <!-- Hiển thị thông tin về tình trạng dịch thuật -->
    
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/optimization-for-data-science-iuh-2025/" title="Home">Optimization in Data Science</a>
            <small></small>
          </h3>
          <!-- Header Actions: Language Toggle and GitHub Link -->
          <div class="header-actions">
            <div class="language-toggle">
              <a href="/optimization-for-data-science-iuh-2025/contents/vi/chapter05/05_02_01_Least_Square/" class="language-switch" title="Switch to Vietnamese">Switch to Vietnamese</a>
            </div>
            <a class="github-logo__wrapper" target="_blank" href="https://github.com/nglelinh/optimization-for-data-science-iuh-2025" title="Github">
             <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
            </a>
          </div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">
    05-02-01 Linear Least-Squares Problems
    
      
        <span class="lesson-badge required large">Required</span>
      
    
  </h1>
  <h2 id="linear-least-squares-problems">Linear Least-Squares Problems</h2>

<p>A linear least-squares problem is an optimization problem without constraints where we minimize the sum of squared errors:</p>

\[\text{minimize}_{x} \quad f_0(x) = \|Ax - b\|_2^2 = \sum_{i=1}^{k} (a_i^T x - b_i)^2\]

<p><strong>where:</strong></p>
<ul>
  <li>\(A \in \mathbb{R}^{k \times n}\) is a matrix with \(k \geq n\)</li>
  <li>\(a_i^T\) are the rows of \(A\)</li>
  <li>\(x \in \mathbb{R}^n\) is the variable we want to find</li>
  <li>\(b \in \mathbb{R}^k\) is the target vector</li>
</ul>

<p><strong>Goal:</strong> Find \(x\) to minimize the sum of squared residuals.</p>

<h3 id="example-linear-regression-for-single-variable">Example: Linear Regression for single variable</h3>

<p>Finding the best-fit line \(y = mx + c\) through data points. We minimize the sum of squared vertical distances from points to the line.</p>

<p><strong>Goal:</strong> Find \(m, c\).</p>

<div id="linear-regression-demo" style="border: 2px solid #ddd; padding: 20px; margin: 20px 0; border-radius: 10px; background-color: #f9f9f9;">
    <h4 style="text-align: center; color: #333;">Interactive Linear Regression Demonstration</h4>
    
    <div style="display: flex; flex-wrap: wrap; gap: 20px; align-items: flex-start;">
        <!-- Canvas for visualization -->
        <div style="flex: 1; min-width: 400px;">
            <canvas id="regressionCanvas" width="400" height="300" style="border: 1px solid #ccc; background: white; cursor: crosshair;"></canvas>
            <p style="font-size: 12px; color: #666; margin-top: 5px;">
                <strong>Instructions:</strong> Click on the canvas to add data points. The red line shows the best-fit line.
            </p>
        </div>
        
        <!-- Controls and information -->
        <div style="flex: 1; min-width: 250px;">
            <div style="background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <h5 style="margin-top: 0; color: #444;">Regression Parameters</h5>
                <div id="regression-params" style="font-family: monospace; font-size: 14px; line-height: 1.6;">
                    <div><strong>Slope (m):</strong> <span id="slope-value">0.000</span></div>
                    <div><strong>Intercept (c):</strong> <span id="intercept-value">0.000</span></div>
                    <div><strong>R² Score:</strong> <span id="r2-value">N/A</span></div>
                    <div><strong>MSE:</strong> <span id="mse-value">N/A</span></div>
                </div>
                
                <div style="margin-top: 15px;">
                    <h5 style="color: #444;">Equation</h5>
                    <div id="equation" style="font-family: monospace; font-size: 16px; background: #f0f0f0; padding: 8px; border-radius: 4px;">
                        y = 0.000x + 0.000
                    </div>
                </div>
                
                <div style="margin-top: 15px;">
                    <button onclick="clearPoints()" style="background: #ff6b6b; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer; margin-right: 10px;">Clear Points</button>
                    <button onclick="addRandomPoints()" style="background: #4ecdc4; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">Add Random Points</button>
                </div>
            </div>
            
            <div style="background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); margin-top: 15px;">
                <h5 style="margin-top: 0; color: #444;">Mathematical Formulation</h5>
                <div style="font-size: 13px; line-height: 1.5;">
                    <p><strong>Objective:</strong> Minimize sum of squared residuals</p>
                    <div style="background: #f8f8f8; padding: 8px; border-radius: 4px; font-family: monospace;">
                        S(m,c) = Σ(yᵢ - mxᵢ - c)²
                    </div>
                    <p style="margin-top: 10px;"><strong>Solution:</strong></p>
                    <div style="background: #f8f8f8; padding: 8px; border-radius: 4px; font-family: monospace; font-size: 11px;">
                        m = Σ(xᵢ-x̄)(yᵢ-ȳ) / Σ(xᵢ-x̄)²<br />
                        c = ȳ - mx̄
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
// Linear Regression Interactive Demo
class LinearRegressionDemo {
    constructor() {
        this.canvas = document.getElementById('regressionCanvas');
        this.ctx = this.canvas.getContext('2d');
        this.points = [];
        this.slope = 0;
        this.intercept = 0;
        
        // Set up canvas
        this.canvas.addEventListener('click', (e) => this.addPoint(e));
        
        // Initialize with some sample points
        this.addRandomPoints();
        this.draw();
    }
    
    addPoint(event) {
        const rect = this.canvas.getBoundingClientRect();
        const x = event.clientX - rect.left;
        const y = event.clientY - rect.top;
        
        // Convert canvas coordinates to data coordinates
        const dataX = (x / this.canvas.width) * 10;
        const dataY = ((this.canvas.height - y) / this.canvas.height) * 10;
        
        this.points.push({x: dataX, y: dataY});
        this.calculateRegression();
        this.draw();
        this.updateDisplay();
    }
    
    addRandomPoints() {
        // Add some sample points with a trend
        const baseSlope = 0.8;
        const baseIntercept = 2;
        
        for (let i = 0; i < 8; i++) {
            const x = Math.random() * 8 + 1;
            const y = baseSlope * x + baseIntercept + (Math.random() - 0.5) * 2;
            this.points.push({x: x, y: Math.max(0, Math.min(10, y))});
        }
        
        this.calculateRegression();
        this.draw();
        this.updateDisplay();
    }
    
    clearPoints() {
        this.points = [];
        this.slope = 0;
        this.intercept = 0;
        this.draw();
        this.updateDisplay();
    }
    
    calculateRegression() {
        if (this.points.length < 2) {
            this.slope = 0;
            this.intercept = 0;
            return;
        }
        
        const n = this.points.length;
        const sumX = this.points.reduce((sum, p) => sum + p.x, 0);
        const sumY = this.points.reduce((sum, p) => sum + p.y, 0);
        const sumXY = this.points.reduce((sum, p) => sum + p.x * p.y, 0);
        const sumXX = this.points.reduce((sum, p) => sum + p.x * p.x, 0);
        
        const meanX = sumX / n;
        const meanY = sumY / n;
        
        const numerator = sumXY - n * meanX * meanY;
        const denominator = sumXX - n * meanX * meanX;
        
        if (Math.abs(denominator) < 1e-10) {
            this.slope = 0;
            this.intercept = meanY;
        } else {
            this.slope = numerator / denominator;
            this.intercept = meanY - this.slope * meanX;
        }
    }
    
    calculateR2() {
        if (this.points.length < 2) return 0;
        
        const meanY = this.points.reduce((sum, p) => sum + p.y, 0) / this.points.length;
        let ssRes = 0;
        let ssTot = 0;
        
        for (const point of this.points) {
            const predicted = this.slope * point.x + this.intercept;
            ssRes += Math.pow(point.y - predicted, 2);
            ssTot += Math.pow(point.y - meanY, 2);
        }
        
        return ssTot === 0 ? 1 : 1 - (ssRes / ssTot);
    }
    
    calculateMSE() {
        if (this.points.length === 0) return 0;
        
        let mse = 0;
        for (const point of this.points) {
            const predicted = this.slope * point.x + this.intercept;
            mse += Math.pow(point.y - predicted, 2);
        }
        
        return mse / this.points.length;
    }
    
    draw() {
        // Clear canvas
        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        
        // Draw grid
        this.drawGrid();
        
        // Draw regression line
        if (this.points.length >= 2) {
            this.drawRegressionLine();
        }
        
        // Draw points and residuals
        this.drawPoints();
        
        // Draw axes labels
        this.drawLabels();
    }
    
    drawGrid() {
        this.ctx.strokeStyle = '#f0f0f0';
        this.ctx.lineWidth = 1;
        
        // Vertical lines
        for (let i = 0; i <= 10; i++) {
            const x = (i / 10) * this.canvas.width;
            this.ctx.beginPath();
            this.ctx.moveTo(x, 0);
            this.ctx.lineTo(x, this.canvas.height);
            this.ctx.stroke();
        }
        
        // Horizontal lines
        for (let i = 0; i <= 10; i++) {
            const y = (i / 10) * this.canvas.height;
            this.ctx.beginPath();
            this.ctx.moveTo(0, y);
            this.ctx.lineTo(this.canvas.width, y);
            this.ctx.stroke();
        }
    }
    
    drawRegressionLine() {
        this.ctx.strokeStyle = '#ff4757';
        this.ctx.lineWidth = 2;
        
        const x1 = 0;
        const y1 = this.intercept;
        const x2 = 10;
        const y2 = this.slope * x2 + this.intercept;
        
        const canvasX1 = (x1 / 10) * this.canvas.width;
        const canvasY1 = this.canvas.height - (y1 / 10) * this.canvas.height;
        const canvasX2 = (x2 / 10) * this.canvas.width;
        const canvasY2 = this.canvas.height - (y2 / 10) * this.canvas.height;
        
        this.ctx.beginPath();
        this.ctx.moveTo(canvasX1, canvasY1);
        this.ctx.lineTo(canvasX2, canvasY2);
        this.ctx.stroke();
    }
    
    drawPoints() {
        for (const point of this.points) {
            const canvasX = (point.x / 10) * this.canvas.width;
            const canvasY = this.canvas.height - (point.y / 10) * this.canvas.height;
            
            // Draw residual line (vertical distance to regression line)
            if (this.points.length >= 2) {
                const predictedY = this.slope * point.x + this.intercept;
                const predictedCanvasY = this.canvas.height - (predictedY / 10) * this.canvas.height;
                
                this.ctx.strokeStyle = '#ff6b6b';
                this.ctx.lineWidth = 1;
                this.ctx.setLineDash([2, 2]);
                this.ctx.beginPath();
                this.ctx.moveTo(canvasX, canvasY);
                this.ctx.lineTo(canvasX, predictedCanvasY);
                this.ctx.stroke();
                this.ctx.setLineDash([]);
            }
            
            // Draw point
            this.ctx.fillStyle = '#2f3542';
            this.ctx.beginPath();
            this.ctx.arc(canvasX, canvasY, 4, 0, 2 * Math.PI);
            this.ctx.fill();
        }
    }
    
    drawLabels() {
        this.ctx.fillStyle = '#666';
        this.ctx.font = '12px Arial';
        
        // X-axis labels
        for (let i = 0; i <= 10; i += 2) {
            const x = (i / 10) * this.canvas.width;
            this.ctx.fillText(i.toString(), x - 5, this.canvas.height - 5);
        }
        
        // Y-axis labels
        for (let i = 0; i <= 10; i += 2) {
            const y = this.canvas.height - (i / 10) * this.canvas.height;
            this.ctx.fillText(i.toString(), 5, y + 3);
        }
    }
    
    updateDisplay() {
        document.getElementById('slope-value').textContent = this.slope.toFixed(3);
        document.getElementById('intercept-value').textContent = this.intercept.toFixed(3);
        document.getElementById('equation').textContent = `y = ${this.slope.toFixed(3)}x + ${this.intercept.toFixed(3)}`;
        
        if (this.points.length >= 2) {
            document.getElementById('r2-value').textContent = this.calculateR2().toFixed(3);
            document.getElementById('mse-value').textContent = this.calculateMSE().toFixed(3);
        } else {
            document.getElementById('r2-value').textContent = 'N/A';
            document.getElementById('mse-value').textContent = 'N/A';
        }
    }
}

// Global functions for buttons
function clearPoints() {
    if (window.regressionDemo) {
        window.regressionDemo.clearPoints();
    }
}

function addRandomPoints() {
    if (window.regressionDemo) {
        window.regressionDemo.clearPoints();
        window.regressionDemo.addRandomPoints();
    }
}

// Initialize when DOM is loaded
document.addEventListener('DOMContentLoaded', function() {
    if (document.getElementById('regressionCanvas')) {
        window.regressionDemo = new LinearRegressionDemo();
    }
});

// Initialize immediately if DOM is already loaded
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', function() {
        if (document.getElementById('regressionCanvas')) {
            window.regressionDemo = new LinearRegressionDemo();
        }
    });
} else {
    if (document.getElementById('regressionCanvas')) {
        window.regressionDemo = new LinearRegressionDemo();
    }
}
</script>

<p><strong>Problem:</strong> Given \(n\) data points \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\), find the line \(y = mx + c\) that minimizes the sum of squared vertical distances from the points to the line.</p>

<p><strong>Objective Function:</strong> We want to minimize
\(S(m,c) = \sum_{i=1}^{n} (y_i - mx_i - c)^2\)</p>

<p><strong>Solution:</strong> To find the minimum, we take partial derivatives and set them equal to zero.</p>

<p>Taking the partial derivative with respect to \(c\):</p>
<blockquote>
\[\frac{\partial S}{\partial c} = \sum_{i=1}^{n} 2(y_i - mx_i - c)(-1) = -2\sum_{i=1}^{n} (y_i - mx_i - c) = 0\]
</blockquote>

<p>This gives us:
\(\sum_{i=1}^{n} y_i = m\sum_{i=1}^{n} x_i + nc\)</p>

<p>Therefore:</p>

<blockquote>
\[c = \frac{1}{n}\sum_{i=1}^{n} y_i - m\frac{1}{n}\sum_{i=1}^{n} x_i = \bar{y} - m\bar{x}\]
</blockquote>

<p>where \(\bar{x}\) and \(\bar{y}\) are the means of \(x\) and \(y\) values.</p>

<p>Taking the partial derivative with respect to \(m\):</p>

<blockquote>
\[\frac{\partial S}{\partial m} = \sum_{i=1}^{n} 2(y_i - mx_i - c)(-x_i) = -2\sum_{i=1}^{n} x_i(y_i - mx_i - c) = 0\]
</blockquote>

<p>Substituting \(c = \bar{y} - m\bar{x}\):
\(\sum_{i=1}^{n} x_i(y_i - mx_i - \bar{y} + m\bar{x}) = 0\)</p>

<p>Rearranging:
\(\sum_{i=1}^{n} x_iy_i - m\sum_{i=1}^{n} x_i^2 - \bar{y}\sum_{i=1}^{n} x_i + m\bar{x}\sum_{i=1}^{n} x_i = 0\)</p>

<p>Since \(\sum_{i=1}^{n} x_i = n\bar{x}\) and \(\sum_{i=1}^{n} x_iy_i - \bar{y}\sum_{i=1}^{n} x_i = \sum_{i=1}^{n} x_i(y_i - \bar{y})\):</p>

<blockquote>
\[\sum_{i=1}^{n} x_i(y_i - \bar{y}) = m\left(\sum_{i=1}^{n} x_i^2 - n\bar{x}^2\right)\]
</blockquote>

<p>Note that \(\sum_{i=1}^{n} x_i^2 - n\bar{x}^2 = \sum_{i=1}^{n} (x_i - \bar{x})^2\)</p>

<p>Therefore:</p>

<blockquote>
\[m = \frac{\sum_{i=1}^{n} x_i(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}\]
</blockquote>

<p><strong>Final Result:</strong> The best-fit line has parameters:</p>

<blockquote>
\[\boxed{m = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \quad \text{and} \quad c = \bar{y} - m\bar{x}}\]
</blockquote>

<p>This is the classical least-squares solution for linear regression, also known as the <strong>Normal Equations</strong>.</p>

<hr />

<h2 id="the-optimal-solution-of-linear-regression-with-multiple-variables">The Optimal Solution of Linear Regression with multiple variables</h2>

<p><strong>Problem Statement:</strong></p>

<p>In Linear Regression, we aim to find a vector of coefficients \(\mathbf{w}\) that best fits a linear model to a given dataset. We have \(n\) data points, each with \(p\) features.</p>

<p>Let \(X\) be the design matrix of size \(n \times p\), where each row represents a data point and each column represents a feature.</p>

<p>Let \(\mathbf{y}\) be the vector of target values of size \(n \times 1\).</p>

<p>Our linear model predicts the target values \(\hat{\mathbf{y}}\) as:</p>
<blockquote>
  <p>\(\hat{\mathbf{y}} = X\mathbf{w}\)
where \(\mathbf{w}\) is the vector of unknown coefficients of size \(p \times 1\).</p>
</blockquote>

<p><strong>Objective Function (Cost Function):</strong></p>

<p>The goal is to minimize the sum of squared errors (residuals) between the actual target values \(\mathbf{y}\) and the predicted values \(\hat{\mathbf{y}}\). This is known as the Ordinary Least Squares (OLS) objective function:
\(J(\mathbf{w}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \| \mathbf{y} - X\mathbf{w} \|^2\)</p>

<blockquote>
  <p><strong>Note:</strong> \(J(\mathbf{w})\) is the <strong>cost function</strong> (also called loss function or objective function) that measures how well our linear model fits the data. It represents the total squared error between our predictions and the actual values. The smaller \(J(\mathbf{w})\), the better our model fits the data. Our goal is to find the optimal weights \(\mathbf{w}\) that minimize this function.</p>
</blockquote>

<p>We can express this in matrix form by expanding the squared Euclidean norm:
\(J(\mathbf{w}) = (\mathbf{y} - X\mathbf{w})^{\text{T}}(\mathbf{y} - X\mathbf{w})\)</p>

<p>Expanding this expression:
\(J(\mathbf{w}) = \mathbf{y}^{\text{T}}\mathbf{y} - \mathbf{y}^{\text{T}}X\mathbf{w} - (X\mathbf{w})^{\text{T}}\mathbf{y} + (X\mathbf{w})^{\text{T}}X\mathbf{w}\)</p>

<p>Using the property that \((AB)^{\text{T}} = B^{\text{T}}A^{\text{T}}\), we have \((X\mathbf{w})^{\text{T}} = \mathbf{w}^{\text{T}}X^{\text{T}}\).</p>

<p>Also, since \(\mathbf{y}^{\text{T}}X\mathbf{w}\) is a scalar, its transpose is itself: \((\mathbf{y}^{\text{T}}X\mathbf{w})^{\text{T}} = \mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y}\).</p>

<p>Thus, the two middle terms are identical:
\(J(\mathbf{w}) = \mathbf{y}^{\text{T}}\mathbf{y} - 2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y} + \mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w}\)</p>

<p><strong>Minimization \(J(\mathbf{w})\) using Calculus:</strong></p>

<p>To find the optimal \(\mathbf{w}\) that minimizes \(J(\mathbf{w})\), we take the derivative of \(J(\mathbf{w})\) with respect to \(\mathbf{w}\) and set it to zero. We use the following matrix calculus rules:</p>

<ol>
  <li>
\[\frac{\partial (\mathbf{a}^{\text{T}}\mathbf{x})}{\partial \mathbf{x}} = \mathbf{a}\]
  </li>
  <li>\(\frac{\partial (\mathbf{x}^{\text{T}}A\mathbf{x})}{\partial \mathbf{x}} = (A + A^{\text{T}})\mathbf{x}\) (If \(A\) is symmetric, this simplifies to \(2A\mathbf{x}\))</li>
</ol>

<p>Applying these rules to \(J(\mathbf{w})\):
\(\frac{\partial J(\mathbf{w})}{\partial \mathbf{w}} = \frac{\partial (\mathbf{y}^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} - \frac{\partial (2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} + \frac{\partial (\mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w})}{\partial \mathbf{w}}\)</p>

<p>Let’s evaluate each term:</p>
<ul>
  <li>\(\frac{\partial (\mathbf{y}^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} = 0\) (since \(\mathbf{y}^{\text{T}}\mathbf{y}\) is a scalar constant with respect to \(\mathbf{w}\))</li>
  <li>\(\frac{\partial (2\mathbf{w}^{\text{T}}X^{\text{T}}\mathbf{y})}{\partial \mathbf{w}} = 2X^{\text{T}}\mathbf{y}\) (using rule 1, with \(\mathbf{a} = X^{\text{T}}\mathbf{y}\))</li>
  <li>
    <p>For the third term, let \(A = X^{\text{T}}X\). Note that \(A\) is a symmetric matrix because \((X^{\text{T}}X)^{\text{T}} = X^{\text{T}}(X^{\text{T}})^{\text{T}} = X^{\text{T}}X\).</p>

    <p>So, \(\frac{\partial (\mathbf{w}^{\text{T}}X^{\text{T}}X\mathbf{w})}{\partial \mathbf{w}} = 2X^{\text{T}}X\mathbf{w}\) (using rule 2 for a symmetric matrix \(A\))</p>
  </li>
</ul>

<p>Combining these, the derivative is:</p>

<blockquote>
\[\frac{\partial J(\mathbf{w})}{\partial \mathbf{w}} = 0 - 2X^{\text{T}}\mathbf{y} + 2X^{\text{T}}X\mathbf{w} = 2X^{\text{T}}X\mathbf{w} - 2X^{\text{T}}\mathbf{y}\]
</blockquote>

<p><strong>Finding the Optimal Solution:</strong></p>

<p>To find the minimum of \(J(\mathbf{w})\), we set the derivative equal to zero:</p>

\[\frac{\partial J(\mathbf{w})}{\partial \mathbf{w}} = 2X^{\text{T}}X\mathbf{w} - 2X^{\text{T}}\mathbf{y} = 0\]

<p>Dividing by 2 and rearranging:
\(X^{\text{T}}X\mathbf{w} = X^{\text{T}}\mathbf{y}\)</p>

<p>This is known as the <strong>Normal Equation</strong>. If \(X^{\text{T}}X\) is invertible (which happens when \(X\) has full column rank), we can solve for \(\mathbf{w}\):</p>

<blockquote>
\[\boxed{\mathbf{w}^* = (X^{\text{T}}X)^{-1}X^{\text{T}}\mathbf{y}}\]
</blockquote>

<p>This is the <strong>closed-form solution</strong> for linear least-squares regression, also known as the <strong>Normal Equation solution</strong>.</p>

<p><strong>Performance:</strong></p>
<ul>
  <li>Time complexity: roughly \(n^2k\) operations</li>
  <li>A standard computer solves problems with hundreds of variables and thousands of terms in seconds</li>
  <li>Sparse matrices (many zero entries) can be solved much faster</li>
</ul>

<p><strong>Example:</strong> A sparse matrix for image processing might have only 5 non-zero entries per row in a 10,000 × 10,000 matrix.</p>

</div>

<!-- Back to Chapter Home Link -->

  
  
  <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff;">
    <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter05/" style="text-decoration: none; color: #007bff; font-weight: bold;">
      ← Back to Chapter Home
    </a>
  </div>













<div class="related">
  <ul class="related-posts">
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_06_Conic_Programming_(CP)/">
              05-06 Conic Programming (CP)
            </a>
          </h3>
        </li>
      
    
      
    
      
    
      
    
    
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <li>
        <h2>Next Post</h2>
        <h3>
          <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_02_02_Geometric_Programming/">
            05-02-02 Geometric Programming
          </a>
        </h3>
      </li>
    
  
    
      <li>
        <h2>Next Post</h2>
        <h3>
          <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter05/05_01_01_LP_Simple_Algorithm/">
            05-01-01 Linear Programming - Simplex Algorithm
          </a>
        </h3>
      </li>
    
  
  </ul>
</div>



<script src="https://utteranc.es/client.js"
        repo="convex-optimization-for-all/convex-optimization-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/optimization-for-data-science-iuh-2025/public/js/script.js'></script>
    <script src='/optimization-for-data-science-iuh-2025/public/js/multilang.js'></script>
    <script src='/optimization-for-data-science-iuh-2025/public/js/search.js'></script>
  </body>
</html>
