<!DOCTYPE html>
<html lang="vi">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <style>
    .MathJax {
      padding: 2em 0.3em;
      overflow-x: auto;
      overflow-y: hidden;
    }
  </style>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <title>
    
      14-05 PhÃ¢n tÃ­ch há»™i tá»¥ &middot; Optimization in Data Science
    
  </title>

  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/poole.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/syntax.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/lanyon.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/github-markdown.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/multilang.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/search.css">
  <link rel="stylesheet" href="/optimization-for-data-science-iuh-2025/public/css/content-boxes.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap">
  
  <!-- Lunr.js for search functionality -->
  <script src="https://unpkg.com/lunr/lunr.js"></script>

  <link rel="apple-touch-icon-precomposed" sizes="122x144" href="http://localhost:4000/optimization-for-data-science-iuh-2025/public/logo.png">
  <link rel="shortcut icon" href="http://localhost:4000/optimization-for-data-science-iuh-2025/public/convex-logo-144x144.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/optimization-for-data-science-iuh-2025/atom.xml">

  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Tá»‘i Æ°u hÃ³a trong Khoa há»c Dá»¯ liá»‡u</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/">Trang chá»§</a>

    

    
    
    
    <!-- Hiá»ƒn thá»‹ cÃ¡c chÆ°Æ¡ng cÃ³ sáºµn cho ngÃ´n ngá»¯ hiá»‡n táº¡i -->
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter00/">
              00. CÃ¡c KhÃ¡i Niá»‡m ToÃ¡n Há»c CÆ¡ Báº£n
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter01/">
              01. Giá»›i thiá»‡u
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter02/">
              02. Táº­p Lá»“i
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter03/">
              03. HÃ m Lá»“i
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter04/">
              04. CÆ¡ báº£n vá» Tá»‘i Æ°u hÃ³a Lá»“i
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter05/">
              05. CÃ¡c BÃ i ToÃ¡n Chuáº©n
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter06/">
              06. Gradient Descent
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter07/">
              07. Subgradient
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter08/">
              08. Subgradient Method
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter09/">
              09. Proximal Gradient Descent and Acceleration
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter10/">
              10. Duality in Linear Programs
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter11/">
              11. Duality in General Programs
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter12/">
              12. KKT Conditions
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter13/">
              13. Duality uses and correspondences
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter14/">
              14. Newton's Method
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter16/">
              16. Duality Revisited
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter18/">
              18. Quasi-Newton Methods
              
            </a>
          
        
      
    
      
        
          
          
            <a class="sidebar-nav-item" href="http://localhost:4000/optimization-for-data-science-iuh-2025/contents/vi/chapter19/">
              19. Proximal Netwon Method
              
            </a>
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
      
        
          
          
        
      
    
    
    <!-- Náº¿u khÃ´ng cÃ³ ná»™i dung cho ngÃ´n ngá»¯ hiá»‡n táº¡i, hiá»ƒn thá»‹ thÃ´ng bÃ¡o -->
    
    
    <!-- Hiá»ƒn thá»‹ thÃ´ng tin vá» tÃ¬nh tráº¡ng dá»‹ch thuáº­t -->
    
      <div class="sidebar-nav-item" style="font-size: 0.8em; color: #999; margin-top: 10px;">
        ğŸ“ 26 chÆ°Æ¡ng Ä‘Ã£ dá»‹ch / 26 chÆ°Æ¡ng tá»•ng
      </div>
    
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2025. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap github-md-body">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/optimization-for-data-science-iuh-2025/" title="Trang chá»§">Tá»‘i Æ°u hÃ³a trong Khoa há»c Dá»¯ liá»‡u</a>
            <small></small>
          </h3>
          <!-- Header Actions: Language Toggle and GitHub Link -->
          <div class="header-actions">
            <div class="language-toggle">
              <a href="/optimization-for-data-science-iuh-2025/contents/en/chapter14/14_05_convergence_analysis/" class="language-switch" title="Chuyá»ƒn sang tiáº¿ng Anh">Chuyá»ƒn sang tiáº¿ng Anh</a>
            </div>
            <a class="github-logo__wrapper" target="_blank" href="https://github.com/nglelinh/optimization-for-data-science-iuh-2025" title="Github">
             <svg class="github-logo" xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48"><linearGradient id="rL2wppHyxHVbobwndsT6Ca" x1="4" x2="44" y1="23.508" y2="23.508" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#4c4c4c"/><stop offset="1" stop-color="#343434"/></linearGradient><path fill="url(#rL2wppHyxHVbobwndsT6Ca)" d="M24,4C12.954,4,4,12.954,4,24c0,8.887,5.801,16.411,13.82,19.016h12.36	C38.199,40.411,44,32.887,44,24C44,12.954,35.046,4,24,4z"/><path d="M30.01,41.996L30,36.198c0-0.939-0.22-1.856-0.642-2.687c5.641-1.133,8.386-4.468,8.386-10.177	c0-2.255-0.665-4.246-1.976-5.92c0.1-0.317,0.174-0.645,0.22-0.981c0.188-1.369-0.023-2.264-0.193-2.984l-0.027-0.116	c-0.186-0.796-0.409-1.364-0.418-1.388l-0.111-0.282l-0.111-0.282l-0.302-0.032l-0.303-0.032c0,0-0.199-0.021-0.501-0.021	c-0.419,0-1.04,0.042-1.627,0.241l-0.196,0.066c-0.74,0.249-1.439,0.485-2.417,1.069c-0.286,0.171-0.599,0.366-0.934,0.584	C27.334,12.881,25.705,12.69,24,12.69c-1.722,0-3.365,0.192-4.889,0.571c-0.339-0.22-0.654-0.417-0.942-0.589	c-0.978-0.584-1.677-0.819-2.417-1.069l-0.196-0.066c-0.585-0.199-1.207-0.241-1.626-0.241c-0.302,0-0.501,0.021-0.501,0.021	l-0.302,0.032l-0.3,0.031l-0.112,0.281l-0.113,0.283c-0.01,0.026-0.233,0.594-0.419,1.391l-0.027,0.115	c-0.17,0.719-0.381,1.615-0.193,2.983c0.048,0.346,0.125,0.685,0.23,1.011c-1.285,1.666-1.936,3.646-1.936,5.89	c0,5.695,2.748,9.028,8.397,10.17c-0.194,0.388-0.345,0.798-0.452,1.224c-0.197,0.067-0.378,0.112-0.538,0.137	c-0.238,0.036-0.487,0.054-0.739,0.054c-0.686,0-1.225-0.134-1.435-0.259c-0.313-0.186-0.872-0.727-1.414-1.518	c-0.463-0.675-1.185-1.558-1.992-1.927c-0.698-0.319-1.437-0.502-2.029-0.502c-0.138,0-0.265,0.01-0.376,0.028	c-0.517,0.082-0.949,0.366-1.184,0.78c-0.203,0.357-0.235,0.773-0.088,1.141c0.219,0.548,0.851,0.985,1.343,1.255	c0.242,0.133,0.765,0.619,1.07,1.109c0.229,0.368,0.335,0.63,0.482,0.992c0.087,0.215,0.183,0.449,0.313,0.732	c0.47,1.022,1.937,1.924,2.103,2.023c0.806,0.483,2.161,0.638,3.157,0.683l0.123,0.003c0,0,0.001,0,0.001,0	c0.24,0,0.57-0.023,1.004-0.071v2.613c0.002,0.529-0.537,0.649-1.25,0.638l0.547,0.184C19.395,43.572,21.645,44,24,44	c2.355,0,4.605-0.428,6.703-1.176l0.703-0.262C30.695,42.538,30.016,42.422,30.01,41.996z" opacity=".05"/><path d="M30.781,42.797c-0.406,0.047-1.281-0.109-1.281-0.795v-5.804c0-1.094-0.328-2.151-0.936-3.052	c5.915-0.957,8.679-4.093,8.679-9.812c0-2.237-0.686-4.194-2.039-5.822c0.137-0.365,0.233-0.75,0.288-1.147	c0.175-1.276-0.016-2.086-0.184-2.801l-0.027-0.116c-0.178-0.761-0.388-1.297-0.397-1.319l-0.111-0.282l-0.303-0.032	c0,0-0.178-0.019-0.449-0.019c-0.381,0-0.944,0.037-1.466,0.215l-0.196,0.066c-0.714,0.241-1.389,0.468-2.321,1.024	c-0.332,0.198-0.702,0.431-1.101,0.694C27.404,13.394,25.745,13.19,24,13.19c-1.762,0-3.435,0.205-4.979,0.61	c-0.403-0.265-0.775-0.499-1.109-0.699c-0.932-0.556-1.607-0.784-2.321-1.024l-0.196-0.066c-0.521-0.177-1.085-0.215-1.466-0.215	c-0.271,0-0.449,0.019-0.449,0.019l-0.302,0.032l-0.113,0.283c-0.009,0.022-0.219,0.558-0.397,1.319l-0.027,0.116	c-0.169,0.715-0.36,1.524-0.184,2.8c0.056,0.407,0.156,0.801,0.298,1.174c-1.327,1.62-1.999,3.567-1.999,5.795	c0,5.703,2.766,8.838,8.686,9.806c-0.395,0.59-0.671,1.255-0.813,1.964c-0.33,0.13-0.629,0.216-0.891,0.256	c-0.263,0.04-0.537,0.06-0.814,0.06c-0.69,0-1.353-0.129-1.69-0.329c-0.44-0.261-1.057-0.914-1.572-1.665	c-0.35-0.51-1.047-1.417-1.788-1.755c-0.635-0.29-1.298-0.457-1.821-0.457c-0.11,0-0.21,0.008-0.298,0.022	c-0.366,0.058-0.668,0.252-0.828,0.534c-0.128,0.224-0.149,0.483-0.059,0.708c0.179,0.448,0.842,0.85,1.119,1.002	c0.335,0.184,0.919,0.744,1.254,1.284c0.251,0.404,0.37,0.697,0.521,1.067c0.085,0.209,0.178,0.437,0.304,0.712	c0.331,0.719,1.353,1.472,1.905,1.803c0.754,0.452,2.154,0.578,2.922,0.612l0.111,0.002c0.299,0,0.8-0.045,1.495-0.135v3.177	c0,0.779-0.991,0.81-1.234,0.81c-0.031,0,0.503,0.184,0.503,0.184C19.731,43.64,21.822,44,24,44c2.178,0,4.269-0.36,6.231-1.003	C30.231,42.997,30.812,42.793,30.781,42.797z" opacity=".07"/><path fill="#fff" d="M36.744,23.334c0-2.31-0.782-4.226-2.117-5.728c0.145-0.325,0.296-0.761,0.371-1.309	c0.172-1.25-0.031-2-0.203-2.734s-0.375-1.25-0.375-1.25s-0.922-0.094-1.703,0.172s-1.453,0.469-2.422,1.047	c-0.453,0.27-0.909,0.566-1.27,0.806C27.482,13.91,25.785,13.69,24,13.69c-1.801,0-3.513,0.221-5.067,0.652	c-0.362-0.241-0.821-0.539-1.277-0.811c-0.969-0.578-1.641-0.781-2.422-1.047s-1.703-0.172-1.703-0.172s-0.203,0.516-0.375,1.25	s-0.375,1.484-0.203,2.734c0.077,0.562,0.233,1.006,0.382,1.333c-1.31,1.493-2.078,3.397-2.078,5.704	c0,5.983,3.232,8.714,9.121,9.435c-0.687,0.726-1.148,1.656-1.303,2.691c-0.387,0.17-0.833,0.33-1.262,0.394	c-1.104,0.167-2.271,0-2.833-0.333s-1.229-1.083-1.729-1.813c-0.422-0.616-1.031-1.331-1.583-1.583	c-0.729-0.333-1.438-0.458-1.833-0.396c-0.396,0.063-0.583,0.354-0.5,0.563c0.083,0.208,0.479,0.521,0.896,0.75	c0.417,0.229,1.063,0.854,1.438,1.458c0.418,0.674,0.5,1.063,0.854,1.833c0.249,0.542,1.101,1.219,1.708,1.583	c0.521,0.313,1.562,0.491,2.688,0.542c0.389,0.018,1.308-0.096,2.083-0.206v3.75c0,0.639-0.585,1.125-1.191,1.013	C19.756,43.668,21.833,44,24,44c2.166,0,4.243-0.332,6.19-0.984C29.585,43.127,29,42.641,29,42.002v-5.804	c0-1.329-0.527-2.53-1.373-3.425C33.473,32.071,36.744,29.405,36.744,23.334z M11.239,32.727c-0.154-0.079-0.237-0.225-0.185-0.328	c0.052-0.103,0.22-0.122,0.374-0.043c0.154,0.079,0.237,0.225,0.185,0.328S11.393,32.806,11.239,32.727z M12.451,33.482	c-0.081,0.088-0.255,0.06-0.389-0.062s-0.177-0.293-0.096-0.381c0.081-0.088,0.255-0.06,0.389,0.062S12.532,33.394,12.451,33.482z M13.205,34.732c-0.102,0.072-0.275,0.005-0.386-0.15s-0.118-0.34-0.016-0.412s0.275-0.005,0.386,0.15	C13.299,34.475,13.307,34.66,13.205,34.732z M14.288,35.673c-0.069,0.112-0.265,0.117-0.437,0.012s-0.256-0.281-0.187-0.393	c0.069-0.112,0.265-0.117,0.437-0.012S14.357,35.561,14.288,35.673z M15.312,36.594c-0.213-0.026-0.371-0.159-0.353-0.297	c0.017-0.138,0.204-0.228,0.416-0.202c0.213,0.026,0.371,0.159,0.353,0.297C15.711,36.529,15.525,36.62,15.312,36.594z M16.963,36.833c-0.227-0.013-0.404-0.143-0.395-0.289c0.009-0.146,0.2-0.255,0.427-0.242c0.227,0.013,0.404,0.143,0.395,0.289	C17.381,36.738,17.19,36.846,16.963,36.833z M18.521,36.677c-0.242,0-0.438-0.126-0.438-0.281s0.196-0.281,0.438-0.281	c0.242,0,0.438,0.126,0.438,0.281S18.762,36.677,18.521,36.677z"/></svg>
            </a>
          </div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">
    14-05 PhÃ¢n tÃ­ch há»™i tá»¥
    
  </h1>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "center"
    });
</script>

<p>Cho Ä‘áº¿n nay, chÃºng ta Ä‘Ã£ xem xÃ©t phÆ°Æ¡ng phÃ¡p Newton thuáº§n tÃºy, chá»‰ cÃ³ tÃ­nh cháº¥t há»™i tá»¥ cá»¥c bá»™, vÃ  phÆ°Æ¡ng phÃ¡p Newton cÃ³ giáº£m cháº¥n (phÆ°Æ¡ng phÃ¡p Newton vá»›i tÃ¬m kiáº¿m Ä‘Æ°á»ng backtracking), Ã¡p dá»¥ng tÃ¬m kiáº¿m Ä‘Æ°á»ng backtracking Ä‘á»ƒ Ä‘áº£m báº£o há»™i tá»¥ toÃ n cá»¥c khi lá»“i.</p>

<p>Trong chÆ°Æ¡ng nÃ y, chÃºng ta phÃ¢n tÃ­ch tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a phÆ°Æ¡ng phÃ¡p Newton cÃ³ giáº£m cháº¥n. Äá»‘i vá»›i phÆ°Æ¡ng phÃ¡p Newton cÃ³ giáº£m cháº¥n, chÃºng ta xem xÃ©t cÃ¡c cáº­n há»™i tá»¥ Ä‘Æ°á»£c chia thÃ nh hai giai Ä‘oáº¡n: giai Ä‘oáº¡n mÃ  backtracking Ä‘Æ°á»£c Ã¡p dá»¥ng (giai Ä‘oáº¡n giáº£m cháº¥n: tiáº¿n bá»™ cháº­m) vÃ  giai Ä‘oáº¡n há»™i tá»¥ cá»¥c bá»™ mÃ  backtracking khÃ´ng cÃ²n cáº§n thiáº¿t (giai Ä‘oáº¡n thuáº§n tÃºy: há»™i tá»¥ báº­c hai).</p>

<h2 id="conditions-of-f-for-convergence-analysis">Conditions of \(f\) for convergence analysis</h2>
<p>Assume that \(f\) is convex, twice differentiable, has \(dom(f)=\mathbb{R}^{n}\), and satisfies the following three conditions:</p>

<ol>
  <li>\(\nabla f\) is Lipschitz continuous with parameter L.
    <blockquote>
\[\begin{align}
\|\nabla f(x) - \nabla f(y)\|_{2} \leq L\|x-y\|_{2} \quad \forall x,y.
\end{align}\]
    </blockquote>
  </li>
  <li>\(f\) is strongly convex with parameter m. (Relationship between upper bound \(L\) and Lipschitz continuous: <a href="https://xingyuzhou.org/blog/notes/strong-convexity">source</a>, <a href="/contents/vi/chapter06/06_03_05_look_at_the_conditions_and_practicalities/">this book: 06-03-05</a>)
    <blockquote>
\[\begin{align}
mI\preceq\nabla^{2}f(x)\preceq LI.
\end{align}\]
    </blockquote>
  </li>
  <li>\(\nabla^{2} f\) is Lipschitz continuous with parameter M.
    <blockquote>
\[\begin{align}
\|\nabla^{2}f(x)-\nabla^{2}f(y)\|_{2} \leq M\|x-y\|_{2} \quad \forall x,y.
\end{align}\]
    </blockquote>
  </li>
</ol>

<h2 id="convergence-analysis">Convergence analysis</h2>
<p>If the above three conditions are satisfied, for \(\eta, \gamma\) satisfying \(0&lt;\eta \leq m^{2}/M\) and \(\gamma&gt;0\), the convergence for each phase can be obtained as follows:</p>

<blockquote>
  <p>Phase I : â€œDampedâ€ phase, \(\|\nabla f(x^{(k)})\|_{2} \geq \eta\),</p>

\[\begin{align}
f(x^{(k+1)})-f(x^{(k)}) \leq -\gamma
\end{align}\]

  <p>Phase 2 : â€œPureâ€ phase, \(\|\nabla f(x^{(k)})\|_{2}&lt;\eta\), backtracking selects \(t = 1\)</p>

\[\begin{align}
\frac{M}{2m^{2}}\|\nabla f(x^{(k+1)})\|_{2} \leq \bigg( \frac{M}{2m^{2}}\|\nabla f(x^{(k)})\|_{2} \bigg)^{2}.
\end{align}\]
</blockquote>

<p>Note that once the Pure phase is reached when \(\|\nabla f(x^{(k)})\|_{2}&lt;\eta\) is satisfied at the \(k\)-th iteration for the first time, this condition is always satisfied for subsequent iterations.</p>

<h2 id="convergence-analysis--written-in-optimal-value-term">Convergence analysis : written in optimal value term</h2>
<p>Now we want to compare the convergence of each phase in terms of the difference from the optimal value.</p>

<p>For Phase 1, if we perform k iterations starting from \(x^{(0)}\), we can organize the equation for each step and represent it as follows:</p>

<blockquote>
\[\begin{align}
\require{cancel}
&amp; &amp;\cancel{f(x^{(1)})}-f(x^{(0)}) \leq -\gamma \\\\
&amp; &amp;\cancel{f(x^{(2)})}-\cancel{f(x^{(1)})} \leq -\gamma \\\\
&amp; &amp;\vdots \\\\
&amp;+ &amp;f(x^{(k)})-\cancel{f(x^{(k-1)})} \leq -\gamma \\\\
&amp;= &amp;f(x^{(k)})-f(x^{(0)})\leq -k\gamma.
\end{align}\]
</blockquote>

<p>Subtracting \(f^{\star}\) from both sides, we can obtain the following result. Let \(k_{0}\) be the first \(k\) that satisfies \(\|\nabla f(x^{(k+1)})\|&lt;\eta\).</p>
<blockquote>
\[\begin{align}
f(x^{(k)})-f^{\star} \geq (f(x^{(0)})-f^{\star})-\gamma k \qquad \text{if }k \geq k_{0}
\end{align}\]
</blockquote>

<p>For Phase 2, assume that iteration starts from \(k_{0}\) and proceeds for \(k-k_{0}\) steps. Also, using \(\|\nabla f(x^{(k)})\|_2&lt;\eta \leq m^{2}/M\) from earlier and strong convexity, we can organize the equation as follows:</p>
<blockquote>
\[\begin{align}
&amp; &amp;\frac{M}{2m^{2}}\|\nabla f^{(k_{0}+1)}\|_{2} \leq \big( \frac{M}{2m^{2}}\|\nabla f^{(k_{0})}\|_{2} \big) ^{2}.\\\\
&amp;\Leftrightarrow &amp;\frac{M}{2m^{2}}\|\nabla f^{(k_{0}+(k-k_{0}))}\|_{2} \leq \bigg( \big( \frac{M}{2m^{2}}\|\nabla f^{(k_{0}+1)}\|_{2} \big) ^{2} \bigg)^{k-k_{0}} \leq (\frac{1}{2})^{2^{(k-k_{0})}}.\\\\
&amp;\Leftrightarrow &amp;f(y)\geq f(x)+\nabla f(x)^{T}(y-x)+\frac{m}{2}\|y-x\|^{2}_{2}\geq f(x)-\frac{1}{2m}\|\nabla f(x)\|^{2}_{2}, \text{ for all }y,\\\\
&amp;\Leftrightarrow &amp;f(x^{(k)})-f^{\star} \leq \frac{1}{2m}\|\nabla f(x^{k})\|_{2}^{2}\leq \frac{2m^{3}}{M^{2}}(\frac{1}{2})^{2^{k-k_{0}+1}}.
\end{align}\]
</blockquote>

<p>Therefore, we can organize the convergence according to steps with the \(k_{0}\)-th iteration as the branch point as follows:</p>

<blockquote>
  <p>Theorem: Newtonâ€™s method using backtracking line search has two-stage convergence bounds.
\(\begin{align}
&amp;f(x^{(k)})-f^{\star} \leq \begin{cases} (f(x^{(0)})-f^{\star})-\gamma k \qquad &amp;\text{if }k\leq k_{0}\\
\frac{2m^{3}}{M^{2}}(\frac{1}{2})^{2^{k-k_{0}+1}} \qquad &amp;\text{if }k&gt;k_{0}.
\end{cases}
\end{align}\)</p>
</blockquote>

<blockquote>
  <p>Here, \(\gamma = \frac{\alpha \beta^{2}\eta^{2}m}{L^{2}}\), \(\eta = \min\{1, 3(1-2\alpha)\}\frac{m^{2}}{M}\), and \(k_{0}\) is the step where \(\|\nabla f(x^{k_0+1}))\|_{2}&lt;\eta\) starts to be satisfied.</p>
</blockquote>

<h2 id="proof-1-damped-phase">Proof 1. Damped phase</h2>
<p>First, we derive the damped phase that satisfies \(\|\nabla f(x)\|_{2} \geq \eta\). We derive the convergence of the damped phase through the lower bound of the step size determined by the backtracking line search process. The Newton decrement relationship is frequently used in the proof process.</p>

<blockquote>
  <p>We start from the following equation where we set \(y=x+t\Delta x_{nt}\) in the Taylor approximation of \(f\) and apply the upper bound of the Lipschitz condition.</p>

\[\begin{align}
f(x+t\Delta x_{nt}) \leq f(x)+t\nabla f(x)^{T}\Delta x_{nt} + \frac{L \|\Delta x_{nt} \|^{2}_{2} }{2}t^{2},
\end{align}\]

  <p>Newton decrement, ì¦ë¶„ê³¼ hessian matrixì™€ì˜ ê´€ê³„ì™€ Strong convexityì˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì „ê°œí•  ìˆ˜ ìˆë‹¤.</p>

\[\begin{align}
&amp;\text{ Since, }\lambda(x)^{2}=\Delta x_{nt}^{T} \nabla^{2} f(x) \geq m\|\Delta x_{nt}\|^{2}_{2},\\\\
&amp;f(x)+t\nabla f(x)^{T}\Delta x_{nt} + \frac{L \|\Delta x_{nt} \|^{2}_{2} }{2}t^{2} \leq f(x)-t\lambda(x)^{2} + \frac{L}{2m}t^{2}\lambda(x)^{2},
\end{align}\]

  <p>ì´ ë•Œ, backtracking line searchì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ë¥¼ ë§Œì¡±í•´ì•¼ í•œë‹¤.</p>

\[\begin{align}
f(x+t\Delta x_{nt}) \leq f(x)-(1-\frac{L}{2m}t)t \lambda(x)^{2}, \qquad \text{ where, }0&lt;1-\frac{L}{2m}t \leq \frac{1}{2}
\end{align}\]

  <p>ìœ„ë¥¼ ë§Œì¡±í•˜ëŠ” tì˜ ìµœì†Œê°’ì„ \(\hat{t}\)ë¼ í•  ë•Œ, \(\hat{t} = \frac{m}{L}\)ì´ ë˜ê³ , ì´ë¥¼ ì› ì‹ì— ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[\begin{align}
f(x+\hat{t}\Delta x_{nt})\leq f(x)-\frac{m}{2L}\lambda(x)^{2} \leq f(x) -\alpha \hat{t} \lambda(x)^{2},
\end{align}\]

  <p>backtracking line searchì—ì„œ \(0&lt;\beta\leq 1\)ì´ë¯€ë¡œ, \(t\geq \beta \frac{m}{L}\)ë¥¼ ë§Œì¡±í•˜ê³ , ì´ë¥¼ ì •ë¦¬í•˜ì—¬ ìµœì¢… ê²°ê³¼ë¥¼ ìœ ë„í•  ìˆ˜ ìˆë‹¤.</p>

\[\begin{align}
f(x^{+})-f(x) &amp;\leq -\alpha t \lambda(x)^{2}\\
&amp;\leq -\alpha\beta \frac{m}{L}\lambda(x)^{2}\\
&amp;\leq -\alpha\beta \frac{m}{L^{2}}\|\nabla f(x)\|^{2}_{2}\\
&amp;\leq -\alpha\beta \eta^{2}\frac{m}{L^{2}},\\
&amp;\gamma = \alpha\beta \eta^{2}\frac{m}{L^{2}}.
\end{align}\]
</blockquote>

<h2 id="proof-2-pure-phase">Proof 2. Pure phase</h2>
<p>ì´ì œ \(\|\nabla f(x)\|_{2} &lt; \eta\)ì¼ ë•Œë¥¼ ê°€ì •í•˜ê³ , Damped phase(quadratically convergent phase)ë¥¼ ì‚´í´ë³¸ë‹¤. ì¦ëª…ì€ ë‘ê°€ì§€ ê³¼ì •ìœ¼ë¡œ ë‚˜ë‰œë‹¤. backtracking line searchì˜ t ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒì„ ë³´ì´ê³ , ìˆ˜ë ´ì†ë„ê°€ quadraticí•¨ì„ ë³´ì´ê²Œ ëœë‹¤.</p>

<blockquote>
  <p>Backtracking line searchë¡œ ë¶€í„° ë‹¤ìŒê³¼ ê°™ì€ ì‹ì´ ìœ ë„ëœë‹¤.</p>

\[\begin{align}
\eta \leq 3(1-2\alpha)\frac{m^{2}}{M}.
\end{align}\]

  <p>ë˜í•œ, Lipschitz conditionì— ë”°ë¼ \(t \geq 0\)ì— ëŒ€í•˜ì—¬, ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤.</p>

\[\begin{align}
\|\nabla^{2}f(x+t\Delta x_{nt})-\nabla^{2}f(x)\|_{2} \leq tM \|\Delta x_{nt} \|_{2},\\
| \Delta x_{nt}^{T} \big( \nabla^{2}f(x+t\Delta x_{nt})-\nabla^{2}f(x) \big) \Delta x_{nt}| \leq tM \|\Delta x_{nt} \|_{2}^{3}.
\end{align}\]

  <p>\(\tilde{f} = f(x+t\Delta x_{nt}\))ë¼ ë‘ë©´, \(\tilde{f}''(t) = \Delta x_{nt}^{T} \nabla^{2}f(x+t\Delta x_{nt})\Delta x_{nt}\)ì´ê³ , ì´ë¥¼ ëŒ€ì…í•œë‹¤.</p>

\[\begin{align}
\tilde{f}''(t) \leq \tilde{f}''(0)+tM\|\Delta x_{nt}\|^{3}_{2} \leq tM\|\Delta x_{nt} \|^{3}_{2}
\end{align}\]

  <p>\(\tilde{f}''(0) = \lambda(x)^{2}\)ì´ê³ , \(\lambda(x)^{2} \geq m\|\nabla x_{nt}\|_{2}^{2}\) ì„ì„ ì´ìš©í•˜ê³ , ë¶€ë“±ì‹ì„ í•©ì¹œë‹¤. \(\tilde{f}'(0) = -\lambda(x)^{2}\)ì´ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.</p>

\[\begin{align}
\tilde{f}''(t) &amp;\leq \tilde{f}''(0) + tM \| \Delta x_{nt} \| ^{3}_{2} \leq \lambda(x)^{2} + t\frac{M}{m^{3/2}}\lambda(x)^{3}, \\
\tilde{f}'(t) &amp;\leq \tilde{f}'(0)+t\lambda(x)^{2} +t^{2}\frac{M}{2m^{3/2}}\lambda(x)^{3},\\
&amp;= -\lambda(x)^{2}+t\lambda(x)^{2} + t^{2}\frac{L}{2m^{3/2}}\lambda(x)^{3}.
\end{align}\]

  <p>ì´ì œ ì–‘ë³€ì„ ì ë¶„í•œë‹¤.</p>

\[\begin{align}
\tilde{f}(t) \leq \tilde{f}(0) - t\lambda(x)^{2} + t^{2} \frac{1}{2}\lambda(x)^{2} + t^{3}\frac{M}{6m^{3/2}}\lambda(x)^{3}.
\end{align}\]

  <p>t = 1ë¡œ ë‘ë©´, ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.</p>

\[\begin{align}
f(x+\Delta x_{nt}) \leq f(x) -\frac{1}{2}\lambda(x)^{2} + \frac{M}{6m^{3/2}}\lambda(x)^{3}.
\end{align}\]

  <p>ì´ì œ \(\|\nabla f(x)\|_{2}\leq \eta \leq 3(1-2\alpha)\frac{m^{2}}{M}\)ì´ë¼ ê°€ì •í•˜ë©´, strong convexity ì¡°ê±´ì— ì˜í•´ \(\lambda(x) \leq 3(1-2\alpha)m^{3/2}/L\)ì´ë‹¤. ì´ë¥¼ ìœ„ì— ë¶€ë“±ì‹ì— ëŒ€ì…í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ìœ ë„í•  ìˆ˜ ìˆë‹¤.</p>

\[\begin{align}
f(x+\Delta x_{nt}) &amp;\leq f(x) - \lambda(x)^{2}( \frac{1}{2}- \frac{M\lambda(x)}{6m^{3/2}} ) \\
&amp;\leq f(x) -\alpha \lambda(x)^{2} \\
&amp;= f(x) + \alpha \nabla f(x)^{T} \Delta x_{nt},
\end{align}\]

  <p>ì´ ê²°ê³¼ëŠ” \(t=1\)ì¼ë•Œ backtracking line searchë¥¼ ìˆ˜í–‰í•˜ë”ë¼ë„ í•­ìƒ ì¡°ê±´ì„ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì—, \(t\)ë¥¼ ê°ì†Œì‹œí‚¤ì§€ ì•ŠìŒì„ ì˜ë¯¸í•œë‹¤.</p>
</blockquote>

<p>ì´ì œ ìš°ë¦¬ëŠ” ìˆ˜ë ´ì†ë„ê°€ quadraticí•˜ê²Œ ì¤„ì–´ë“¬ì„ ì¦ëª…í•´ë³¸ë‹¤.</p>
<blockquote>
  <p>\(x_{nt} = -(\nabla^{2}f(x))^{-1}\nabla f(x)\)ì„ì„ ì´ìš©í•œ ë’¤, ì ë¶„ì˜ ì„±ì§ˆ ì¤‘ í•˜ë‚˜ì¸ \(f(t, u) - f(t, v) = \int^{u}_{v}{\frac{\partial f}{\partial x}(t, x) dx}\)ë¥¼ ì´ìš©í•˜ì—¬ ì •ë¦¬í•˜ê³ , Hessianì˜ Lipschitz ì¡°ê±´ì„ ì ë¶„ì‹ì— ì ìš©í•˜ê³  ì •ë¦¬í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ strong convexity ì¡°ê±´ì„ ì ìš©í•˜ë©´ ì¦ëª…ì´ ì™„ë£Œëœë‹¤. ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

\[\begin{align}
\| \nabla f(x^{+}) \| _{2} &amp;= \| \nabla f(x+\Delta x_{nt}) - \nabla f(x) - \nabla^{2}f(x)\Delta x_{nt} \|_{2}\\\\
&amp;=\| \int^{1}_{0}{ \big( \nabla^{2}f(x+t\Delta x_{nt})-\nabla^{2} f(x) \big) \Delta x_{nt} dt } \|_{2}\\\\
&amp; \leq \frac{M}{2}\|\Delta x_{nt} \|^{2}_{2}\\\\
&amp; = \frac{M}{2}\|\nabla^{2}f(x)^{-1}\nabla f(x)\|^{2}_{2}\\\\
&amp; \leq \frac{M}{2m^{2}}\|\nabla f(x)\|^{2}_{2}.
\end{align}\]
</blockquote>

<p>ê²°ë¡ ì„ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´, \(\eta = \min \{1, 3(1-2\alpha)\}\frac{m^{2}}{M}\) ì¼ ë•Œ, \(\|\nabla f(x^{(k)}) \|_{2}&lt;\eta\)ë¥¼ ë§Œì¡±í•˜ëŠ” ì¡°ê±´ì—ì„œëŠ” Newtonâ€™s methodëŠ” backtracing line searchì—ì„œì˜ ì—…ë°ì´íŠ¸ê°€ ë”ì´ìƒ í•„ìš”í•˜ì§€ ì•Šê³ , quadraticí•˜ê²Œ converge í•œë‹¤.</p>

<h2 id="estimating-total-complexity">Estimating total complexity</h2>
<p>ì´ì œ, ìš°ë¦¬ëŠ” ì „ì²´ ê³¼ì •ì—ì„œì˜ complexity, ë‹¬ë¦¬ ë§í•´ ì´ˆê¸° ê°’ìœ¼ë¡œë¶€í„° ìµœì ê°’ê¹Œì§€ ë„ë‹¬í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” iteration íšŸìˆ˜ì— ëŒ€í•œ boundë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.
ìš°ì„ , ìœ„ì˜ damped Newton phaseì—ì„œ \(f\)ëŠ” ë§¤ iterationë§ˆë‹¤ \(\gamma\)ë¥¼ ë„˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ ê°’ì´ ê°ì†Œí•˜ë¯€ë¡œ, damped Newton stepì˜ ì „ì²´ step ìˆ˜ëŠ” ë‹¤ìŒì˜ ì‹ì˜ ê²°ê³¼ê°’ì„ ë„˜ì§€ ëª»í•œë‹¤.</p>
<blockquote>
\[\begin{align}
\frac{f(x^{(0)})-p^{\star}}{\gamma}.
\end{align}\]
</blockquote>

<p>pure Newton phaseì—ì„œì˜ iteration íšŸìˆ˜ì˜ bound ë˜í•œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ìœ„ì˜ ì‹ì„ \(f(x)-p^{\star}\leq \epsilon\), \(\epsilon_{0} = \frac{2m^{3}}{M^{2}}\)ë¡œ ë‘ê³ , iteration íšŸìˆ˜ë¡œ ì‹ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.</p>
<blockquote>
\[\begin{align}
&amp; &amp;\epsilon = \epsilon_{0} (\frac{1}{2})^{2^{k-k_{0}+1}}\\\\
&amp;\Leftrightarrow &amp;\frac{\epsilon_{0}}{\epsilon} = 2^{2^{k-k_{0}+1}}\\\\
&amp;\Leftrightarrow &amp;k-k_{0}+1 = log_{2}log_{2}(\frac{\epsilon_{0}}{\epsilon})
\end{align}\]
</blockquote>

<p>ë”°ë¼ì„œ pure Newton phaseì—ì„œ iteration íšŸìˆ˜ëŠ” \(\log \log(\frac{\epsilon_{0}}{\epsilon})\)ë¡œ bound ëœë‹¤.</p>

<p>ì´ ë‘ ê²°ê³¼ë¥¼ ë”í•˜ë©´, Newton methodë¥¼ í†µí•˜ì—¬ ì›í•˜ëŠ” ì •ë°€ë„ì˜ í•´ë¥¼ ì–»ëŠ”ë° í•„ìš”í•œ iteration íšŸìˆ˜ì˜ upper boundë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.</p>
<blockquote>
\[\begin{align}
\frac{f(x^{(0)})-p^{\star}}{\gamma} + \log \log (\frac{\epsilon_{0}}{\epsilon}).
\end{align}\]
</blockquote>

<p>ë¬¸ì œë¥¼ í•´ê²°í• ë•Œ ìš”êµ¬ë˜ëŠ” ì •ë°€ë„ \(\epsilon\)ì˜ ë³€í™”ì— ë¹„í•´ ìš°ë³€ì˜ ë‘ë²ˆì§¸ í•­ì€ ë§¤ìš° ì‘ì€ ë³€í™”ë¥¼ ë³´ì´ë¯€ë¡œ, ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ì´ë¥¼ ìƒìˆ˜ë¡œ ë‘ê³  ì¶”ì •ì„ í•˜ê²Œ ëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ 6ë²ˆì˜ iterationì€ \(\epsilon \approx 5\cdot 10^{-20}\epsilon_{0}\)ì˜ ì •ë°€ë„ë¥¼ ë³´ì¸ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.</p>

<p>ì¼ë°˜ì ìœ¼ë¡œ ë§í•´ì„œ, ëª©ì í•¨ìˆ˜ \(f\)ë¥¼ ìµœì†Œí™”í•˜ëŠ”ë° ìˆì–´ì„œ í•„ìš”í•œ iteration íšŸìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<blockquote>
\[\begin{align}
\frac{f(x^{(0)})-p^{\star}}{\gamma} + 6.
\end{align}\]
</blockquote>

</div>

<!-- Back to Chapter Home Link -->

  
  
  <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #007bff;">
    <a href="/optimization-for-data-science-iuh-2025/contents/vi/chapter14/" style="text-decoration: none; color: #007bff; font-weight: bold;">
      â† Quay láº¡i Ä‘áº§u chÆ°Æ¡ng
    </a>
  </div>













<div class="related">
  <ul class="related-posts">
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <li>
          <h2>Previous Post</h2>
          <h3>
            <a href="/optimization-for-data-science-iuh-2025/contents/vi/chapter14/14_04_backtracking_line_search/">
              14-04 TÃ¬m kiáº¿m Ä‘Æ°á»ng backtracking
            </a>
          </h3>
        </li>
      
    
      
    
    
    
  
    
      <li>
        <h2>Next Post</h2>
        <h3>
          <a href="/optimization-for-data-science-iuh-2025/contents/vi/chapter14/14_06_self_concordance/">
            14-06 Tá»± hÃ i hÃ²a (Self concordance)
          </a>
        </h3>
      </li>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
  </ul>
</div>



<script src="https://utteranc.es/client.js"
        repo="convex-optimization-for-all/convex-optimization-for-all.github.io"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/optimization-for-data-science-iuh-2025/public/js/script.js'></script>
    <script src='/optimization-for-data-science-iuh-2025/public/js/multilang.js'></script>
    <script src='/optimization-for-data-science-iuh-2025/public/js/search.js'></script>
  </body>
</html>
